{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor, KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import autokeras as ak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/TPath_300_finish_data.pkl')\n",
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish_D2_I7000_F9000/TPath_300_finish_data_D2.pkl')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e3c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(data.columns[2:5], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8783d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "\n",
    "print(df1)\n",
    "df_reset = df1.reset_index(drop=True)\n",
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e72a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = df_reset[\"Temperatur\"]\n",
    "# Korrektur: Verwenden Sie den Spaltennamen direkt, ohne Indexierung der columns-Eigenschaft\n",
    "df1 = df_reset.drop(\"Temperatur\", axis=1)\n",
    "X = df1\n",
    "y = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7fa289a50d87423"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3303b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # train_df enthält 80% der Daten, test_df enthält 20% der Daten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c705edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_features2 = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "X_test_scaled = scaler_features.transform(X_test)  # Nutze unterschiedliche Skalierungsparameter\n",
    "\n",
    "# Initialisiere einen SEPARATEN MinMaxScaler für das Ziel, wenn nötig\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Skaliere y_train und y_test. Beachte, dass y_train.reshape(-1, 1) verwendet wird, da MinMaxScaler \n",
    "# erwartet, dass die Eingaben als 2D-Arrays kommen, und Ziele normalerweise als 1D-Arrays vorliegen.\n",
    "y_train_scaled = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbefe631e495b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eccbb7534cd064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lernplan erstellen\n",
    "#initial_learning_rate = 0.1\n",
    "#lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#    initial_learning_rate, decay_steps=100, decay_rate=0.9, staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d664797f8dcbd88c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "    # Eingabeschicht\n",
    "    Dense(32, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.01)),\n",
    "    #Dropout(0.01),\n",
    "    # Versteckte Schicht\n",
    "    #Dense(8, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.01)),\n",
    "    #Dropout(0.01),\n",
    "    # Ausgabeschicht für Regression (1 Einheit ohne Aktivierungsfunktion)\n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = 'adam' #Adam(learningrate =  0.001)\n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min', restore_best_weights=True, min_delta = 0.0001)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 50\n",
    "epochs = 100\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mae = history.history['mae']\n",
    "val_mae = history.history['val_mae']\n",
    "\n",
    "epochs = range(1, len(mae) + 1)\n",
    "\n",
    "# MAE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mae, 'r', label='Training MAE')\n",
    "plt.plot(epochs, val_mae, 'b', label='Validation MAE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Führen Sie die Rücktransformation der skalierten Werte durch\n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[100]} Actual: {original_actual_values[100]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "    \n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "    \n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca80c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 1: Definiere dein Modell als eine Funktion, die Hyperparameter akzeptiert\n",
    "def build_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=64, activation=activation, input_dim=X_train_scaled.shape[1]))\n",
    "    model.add(Dense(units=32, activation=activation))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "# Schritt 2: Erstelle eine Instanz von KerasRegressor ohne batch_size und epochs im param_grid\n",
    "model = KerasRegressor(model=build_model, verbose=1)\n",
    "\n",
    "# Schritt 3: Definiere den Parameter-Raum für die Grid-Suche OHNE batch_size und epochs\n",
    "param_grid = {\n",
    "    'model__optimizer': ['adam', 'sgd'],\n",
    "    'model__activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "# Optional: Definiere Fit-Parameter (wie batch_size und epochs) separat\n",
    "fit_params = {\n",
    "    'batch_size': 32,\n",
    "    'epochs': 10\n",
    "}\n",
    "\n",
    "# Schritt 4: Führe GridSearchCV aus mit fit_params, wenn nötig\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train_scaled, y_train_scaled, **fit_params)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for mean_score, params in zip(grid_result.cv_results_['mean_test_score'], grid_result.cv_results_['params']):\n",
    "    print(\"%f with: %r\" % (mean_score, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(optimizer='adam', activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=64, activation=activation, input_dim=X_train_scaled.shape[1]))\n",
    "    model.add(Dense(units=32, activation=activation))\n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(model=build_model, verbose=1)\n",
    "\n",
    "param_grid = {\n",
    "    'model__optimizer': ['adam', 'sgd'],\n",
    "    'model__activation': ['relu', 'tanh'],\n",
    "    'fit__batch_size': [10, 20, 32],  # Testen verschiedener batch_size Werte\n",
    "    'fit__epochs': [5, 10]  # Optional: Testen verschiedener epoch Werte\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for mean_score, params in zip(grid_result.cv_results_['mean_test_score'], grid_result.cv_results_['params']):\n",
    "    print(\"%f with: %r\" % (mean_score, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492cf0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

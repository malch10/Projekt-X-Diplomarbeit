{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94b0518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:58:56.929115200Z",
     "start_time": "2024-03-01T12:58:56.919597100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4ff61b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:58:58.096862100Z",
     "start_time": "2024-03-01T12:58:58.066137700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n0          0.00000      -0.00200        500   7000   9000      669.05\n1          0.00000      -0.00196        500   7000   9000      696.80\n2          0.00000      -0.00192        500   7000   9000      724.42\n3          0.00000      -0.00188        500   7000   9000      751.84\n4          0.00000      -0.00184        500   7000   9000      779.83\n...            ...           ...        ...    ...    ...         ...\n6358       0.00248       0.00184        500   7000   9000      651.36\n6359       0.00248       0.00188        500   7000   9000      612.09\n6360       0.00248       0.00192        500   7000   9000      584.59\n6361       0.00248       0.00196        500   7000   9000      578.64\n6362       0.00248       0.00200        500   7000   9000      572.78\n\n[6363 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00000</td>\n      <td>-0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00000</td>\n      <td>-0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>696.80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00000</td>\n      <td>-0.00192</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>724.42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00000</td>\n      <td>-0.00188</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>751.84</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00000</td>\n      <td>-0.00184</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>779.83</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6358</th>\n      <td>0.00248</td>\n      <td>0.00184</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>651.36</td>\n    </tr>\n    <tr>\n      <th>6359</th>\n      <td>0.00248</td>\n      <td>0.00188</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>612.09</td>\n    </tr>\n    <tr>\n      <th>6360</th>\n      <td>0.00248</td>\n      <td>0.00192</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>584.59</td>\n    </tr>\n    <tr>\n      <th>6361</th>\n      <td>0.00248</td>\n      <td>0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>578.64</td>\n    </tr>\n    <tr>\n      <th>6362</th>\n      <td>0.00248</td>\n      <td>0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>572.78</td>\n    </tr>\n  </tbody>\n</table>\n<p>6363 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/TPath_300_finish_data.pkl')\n",
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish_D3_I7000_F9000/TPath_500_finish_data_D3.pkl')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "966e3c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:59:04.110678Z",
     "start_time": "2024-03-01T12:59:04.077485300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Temperatur\n0          0.00000      -0.00200      669.05\n1          0.00000      -0.00196      696.80\n2          0.00000      -0.00192      724.42\n3          0.00000      -0.00188      751.84\n4          0.00000      -0.00184      779.83\n...            ...           ...         ...\n6358       0.00248       0.00184      651.36\n6359       0.00248       0.00188      612.09\n6360       0.00248       0.00192      584.59\n6361       0.00248       0.00196      578.64\n6362       0.00248       0.00200      572.78\n\n[6363 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00000</td>\n      <td>-0.00200</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00000</td>\n      <td>-0.00196</td>\n      <td>696.80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00000</td>\n      <td>-0.00192</td>\n      <td>724.42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00000</td>\n      <td>-0.00188</td>\n      <td>751.84</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00000</td>\n      <td>-0.00184</td>\n      <td>779.83</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6358</th>\n      <td>0.00248</td>\n      <td>0.00184</td>\n      <td>651.36</td>\n    </tr>\n    <tr>\n      <th>6359</th>\n      <td>0.00248</td>\n      <td>0.00188</td>\n      <td>612.09</td>\n    </tr>\n    <tr>\n      <th>6360</th>\n      <td>0.00248</td>\n      <td>0.00192</td>\n      <td>584.59</td>\n    </tr>\n    <tr>\n      <th>6361</th>\n      <td>0.00248</td>\n      <td>0.00196</td>\n      <td>578.64</td>\n    </tr>\n    <tr>\n      <th>6362</th>\n      <td>0.00248</td>\n      <td>0.00200</td>\n      <td>572.78</td>\n    </tr>\n  </tbody>\n</table>\n<p>6363 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop(data.columns[2:5], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8783d1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:59:08.807062Z",
     "start_time": "2024-03-01T12:59:08.776582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X-Koordinate  Y-Koordinate  Temperatur\n",
      "6243       0.00244       0.00128      978.37\n",
      "2949       0.00116      -0.00120     1193.00\n",
      "393        0.00012       0.00160      816.01\n",
      "3844       0.00152      -0.00176      827.43\n",
      "2154       0.00084      -0.00068     1419.40\n",
      "...            ...           ...         ...\n",
      "3772       0.00148      -0.00060     1381.60\n",
      "5191       0.00204      -0.00040     1320.60\n",
      "5226       0.00204       0.00100     1122.50\n",
      "5390       0.00212      -0.00052     1288.40\n",
      "860        0.00032       0.00008     1509.50\n",
      "\n",
      "[6363 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Temperatur\n0          0.00244       0.00128      978.37\n1          0.00116      -0.00120     1193.00\n2          0.00012       0.00160      816.01\n3          0.00152      -0.00176      827.43\n4          0.00084      -0.00068     1419.40\n...            ...           ...         ...\n6358       0.00148      -0.00060     1381.60\n6359       0.00204      -0.00040     1320.60\n6360       0.00204       0.00100     1122.50\n6361       0.00212      -0.00052     1288.40\n6362       0.00032       0.00008     1509.50\n\n[6363 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00244</td>\n      <td>0.00128</td>\n      <td>978.37</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00116</td>\n      <td>-0.00120</td>\n      <td>1193.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00012</td>\n      <td>0.00160</td>\n      <td>816.01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00152</td>\n      <td>-0.00176</td>\n      <td>827.43</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00084</td>\n      <td>-0.00068</td>\n      <td>1419.40</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6358</th>\n      <td>0.00148</td>\n      <td>-0.00060</td>\n      <td>1381.60</td>\n    </tr>\n    <tr>\n      <th>6359</th>\n      <td>0.00204</td>\n      <td>-0.00040</td>\n      <td>1320.60</td>\n    </tr>\n    <tr>\n      <th>6360</th>\n      <td>0.00204</td>\n      <td>0.00100</td>\n      <td>1122.50</td>\n    </tr>\n    <tr>\n      <th>6361</th>\n      <td>0.00212</td>\n      <td>-0.00052</td>\n      <td>1288.40</td>\n    </tr>\n    <tr>\n      <th>6362</th>\n      <td>0.00032</td>\n      <td>0.00008</td>\n      <td>1509.50</td>\n    </tr>\n  </tbody>\n</table>\n<p>6363 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "\n",
    "print(df1)\n",
    "df_reset = df1.reset_index(drop=True)\n",
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4e72a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:59:10.922330900Z",
     "start_time": "2024-03-01T12:59:10.913813Z"
    }
   },
   "outputs": [],
   "source": [
    "label = df_reset[\"Temperatur\"]\n",
    "# Korrektur: Verwenden Sie den Spaltennamen direkt, ohne Indexierung der columns-Eigenschaft\n",
    "df1 = df_reset.drop(\"Temperatur\", axis=1)\n",
    "X = df1\n",
    "y = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7fa289a50d87423"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e694a236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:59:13.781925200Z",
     "start_time": "2024-03-01T12:59:13.747628100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate\n0          0.00244       0.00128\n1          0.00116      -0.00120\n2          0.00012       0.00160\n3          0.00152      -0.00176\n4          0.00084      -0.00068\n...            ...           ...\n6358       0.00148      -0.00060\n6359       0.00204      -0.00040\n6360       0.00204       0.00100\n6361       0.00212      -0.00052\n6362       0.00032       0.00008\n\n[6363 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00244</td>\n      <td>0.00128</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00116</td>\n      <td>-0.00120</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00012</td>\n      <td>0.00160</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00152</td>\n      <td>-0.00176</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00084</td>\n      <td>-0.00068</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6358</th>\n      <td>0.00148</td>\n      <td>-0.00060</td>\n    </tr>\n    <tr>\n      <th>6359</th>\n      <td>0.00204</td>\n      <td>-0.00040</td>\n    </tr>\n    <tr>\n      <th>6360</th>\n      <td>0.00204</td>\n      <td>0.00100</td>\n    </tr>\n    <tr>\n      <th>6361</th>\n      <td>0.00212</td>\n      <td>-0.00052</td>\n    </tr>\n    <tr>\n      <th>6362</th>\n      <td>0.00032</td>\n      <td>0.00008</td>\n    </tr>\n  </tbody>\n</table>\n<p>6363 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f3303b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:59:15.879900300Z",
     "start_time": "2024-03-01T12:59:15.872618900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0        978.37\n1       1193.00\n2        816.01\n3        827.43\n4       1419.40\n         ...   \n6358    1381.60\n6359    1320.60\n6360    1122.50\n6361    1288.40\n6362    1509.50\nName: Temperatur, Length: 6363, dtype: float64"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3ad8da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:59:22.980767300Z",
     "start_time": "2024-03-01T12:59:22.972922500Z"
    }
   },
   "outputs": [],
   "source": [
    " # train_df enthält 80% der Daten, test_df enthält 20% der Daten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c705edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:59:25.238045Z",
     "start_time": "2024-03-01T12:59:25.203359200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_features2 = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "X_test_scaled = scaler_features.transform(X_test)  # Nutze gleiche Skalierungsparameter ohne das X_Test Informationen einfließen\n",
    "\n",
    "# Initialisiere einen SEPARATEN MinMaxScaler für das Ziel, wenn nötig\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Skaliere y_train und y_test. Beachte, dass y_train.reshape(-1, 1) verwendet wird, da MinMaxScaler \n",
    "# erwartet, dass die Eingaben als 2D-Arrays kommen, und Ziele normalerweise als 1D-Arrays vorliegen.\n",
    "y_train_scaled = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbefe631e495b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T12:59:26.016597500Z",
     "start_time": "2024-03-01T12:59:26.008905700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.96774194, 0.58      ],\n       [0.12903226, 0.07      ],\n       [0.03225806, 0.07      ],\n       ...,\n       [0.01612903, 0.25      ],\n       [0.67741935, 0.82      ],\n       [0.5483871 , 0.65      ]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9999999999999999"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_scaled.max()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T12:59:38.120920800Z",
     "start_time": "2024-03-01T12:59:38.113249500Z"
    }
   },
   "id": "ce04ce43aac2242f"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "163/163 [==============================] - 2s 4ms/step - loss: 0.4788 - mae: 0.2787 - val_loss: 0.3930 - val_mae: 0.1956\n",
      "Epoch 2/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.3524 - mae: 0.1452 - val_loss: 0.3096 - val_mae: 0.0723\n",
      "Epoch 3/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.2916 - mae: 0.0520 - val_loss: 0.2750 - val_mae: 0.0311\n",
      "Epoch 4/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.2664 - mae: 0.0405 - val_loss: 0.2592 - val_mae: 0.0596\n",
      "Epoch 5/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.2464 - mae: 0.0244 - val_loss: 0.2378 - val_mae: 0.0114\n",
      "Epoch 6/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.2324 - mae: 0.0227 - val_loss: 0.2266 - val_mae: 0.0290\n",
      "Epoch 7/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.2214 - mae: 0.0222 - val_loss: 0.2324 - val_mae: 0.0902\n",
      "Epoch 8/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.2135 - mae: 0.0308 - val_loss: 0.2106 - val_mae: 0.0397\n",
      "Epoch 9/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.2058 - mae: 0.0268 - val_loss: 0.2023 - val_mae: 0.0277\n",
      "Epoch 10/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1991 - mae: 0.0201 - val_loss: 0.2024 - val_mae: 0.0657\n",
      "Epoch 11/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1938 - mae: 0.0171 - val_loss: 0.1908 - val_mae: 0.0063\n",
      "Epoch 12/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1889 - mae: 0.0118 - val_loss: 0.1869 - val_mae: 0.0148\n",
      "Epoch 13/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1850 - mae: 0.0130 - val_loss: 0.1830 - val_mae: 0.0119\n",
      "Epoch 14/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1829 - mae: 0.0277 - val_loss: 0.1817 - val_mae: 0.0388\n",
      "Epoch 15/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1783 - mae: 0.0182 - val_loss: 0.1767 - val_mae: 0.0199\n",
      "Epoch 16/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1772 - mae: 0.0308 - val_loss: 0.1730 - val_mae: 0.0084\n",
      "Epoch 17/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1718 - mae: 0.0124 - val_loss: 0.1701 - val_mae: 0.0074\n",
      "Epoch 18/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1690 - mae: 0.0132 - val_loss: 0.1676 - val_mae: 0.0137\n",
      "Epoch 19/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1666 - mae: 0.0181 - val_loss: 0.1649 - val_mae: 0.0152\n",
      "Epoch 20/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1635 - mae: 0.0134 - val_loss: 0.1621 - val_mae: 0.0132\n",
      "Epoch 21/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1615 - mae: 0.0212 - val_loss: 0.1595 - val_mae: 0.0130\n",
      "Epoch 22/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1589 - mae: 0.0183 - val_loss: 0.1590 - val_mae: 0.0360\n",
      "Epoch 23/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1572 - mae: 0.0277 - val_loss: 0.1570 - val_mae: 0.0357\n",
      "Epoch 24/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1534 - mae: 0.0124 - val_loss: 0.1519 - val_mae: 0.0071\n",
      "Epoch 25/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1509 - mae: 0.0108 - val_loss: 0.1495 - val_mae: 0.0059\n",
      "Epoch 26/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1484 - mae: 0.0085 - val_loss: 0.1472 - val_mae: 0.0082\n",
      "Epoch 27/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1461 - mae: 0.0101 - val_loss: 0.1447 - val_mae: 0.0040\n",
      "Epoch 28/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1437 - mae: 0.0103 - val_loss: 0.1434 - val_mae: 0.0245\n",
      "Epoch 29/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1420 - mae: 0.0188 - val_loss: 0.1405 - val_mae: 0.0176\n",
      "Epoch 30/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1393 - mae: 0.0139 - val_loss: 0.1394 - val_mae: 0.0323\n",
      "Epoch 31/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1377 - mae: 0.0181 - val_loss: 0.1357 - val_mae: 0.0101\n",
      "Epoch 32/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1346 - mae: 0.0085 - val_loss: 0.1335 - val_mae: 0.0116\n",
      "Epoch 33/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.0228 - val_loss: 0.1323 - val_mae: 0.0245\n",
      "Epoch 34/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1304 - mae: 0.0127 - val_loss: 0.1298 - val_mae: 0.0235\n",
      "Epoch 35/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1284 - mae: 0.0127 - val_loss: 0.1274 - val_mae: 0.0157\n",
      "Epoch 36/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1262 - mae: 0.0111 - val_loss: 0.1250 - val_mae: 0.0056\n",
      "Epoch 37/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1240 - mae: 0.0072 - val_loss: 0.1229 - val_mae: 0.0041\n",
      "Epoch 38/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1234 - mae: 0.0209 - val_loss: 0.1291 - val_mae: 0.0621\n",
      "Epoch 39/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1211 - mae: 0.0209 - val_loss: 0.1189 - val_mae: 0.0056\n",
      "Epoch 40/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1180 - mae: 0.0072 - val_loss: 0.1173 - val_mae: 0.0129\n",
      "Epoch 41/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1165 - mae: 0.0145 - val_loss: 0.1153 - val_mae: 0.0096\n",
      "Epoch 42/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1144 - mae: 0.0112 - val_loss: 0.1133 - val_mae: 0.0045\n",
      "Epoch 43/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1126 - mae: 0.0112 - val_loss: 0.1120 - val_mae: 0.0196\n",
      "Epoch 44/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.1105 - mae: 0.0076 - val_loss: 0.1095 - val_mae: 0.0045\n",
      "Epoch 45/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1093 - mae: 0.0168 - val_loss: 0.1081 - val_mae: 0.0164\n",
      "Epoch 46/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1072 - mae: 0.0131 - val_loss: 0.1059 - val_mae: 0.0061\n",
      "Epoch 47/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1051 - mae: 0.0063 - val_loss: 0.1041 - val_mae: 0.0048\n",
      "Epoch 48/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1033 - mae: 0.0071 - val_loss: 0.1031 - val_mae: 0.0226\n",
      "Epoch 49/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1016 - mae: 0.0089 - val_loss: 0.1006 - val_mae: 0.0044\n",
      "Epoch 50/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0997 - mae: 0.0063 - val_loss: 0.0989 - val_mae: 0.0095\n",
      "Epoch 51/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1020 - mae: 0.0344 - val_loss: 0.0973 - val_mae: 0.0103\n",
      "Epoch 52/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0966 - mae: 0.0090 - val_loss: 0.0958 - val_mae: 0.0049\n",
      "Epoch 53/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0953 - mae: 0.0090 - val_loss: 0.0948 - val_mae: 0.0164\n",
      "Epoch 54/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0939 - mae: 0.0100 - val_loss: 0.0930 - val_mae: 0.0059\n",
      "Epoch 55/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0923 - mae: 0.0065 - val_loss: 0.0917 - val_mae: 0.0104\n",
      "Epoch 56/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0911 - mae: 0.0128 - val_loss: 0.0902 - val_mae: 0.0062\n",
      "Epoch 57/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0896 - mae: 0.0084 - val_loss: 0.0888 - val_mae: 0.0091\n",
      "Epoch 58/500\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0883 - mae: 0.0103 - val_loss: 0.0883 - val_mae: 0.0222\n",
      "Epoch 59/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0869 - mae: 0.0114 - val_loss: 0.0859 - val_mae: 0.0048\n",
      "Epoch 60/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0854 - mae: 0.0080 - val_loss: 0.0846 - val_mae: 0.0058\n",
      "Epoch 61/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0839 - mae: 0.0056 - val_loss: 0.0832 - val_mae: 0.0044\n",
      "Epoch 62/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0825 - mae: 0.0068 - val_loss: 0.0818 - val_mae: 0.0058\n",
      "Epoch 63/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0815 - mae: 0.0153 - val_loss: 0.0814 - val_mae: 0.0284\n",
      "Epoch 64/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0800 - mae: 0.0109 - val_loss: 0.0794 - val_mae: 0.0145\n",
      "Epoch 65/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0787 - mae: 0.0115 - val_loss: 0.0778 - val_mae: 0.0054\n",
      "Epoch 66/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0774 - mae: 0.0108 - val_loss: 0.0767 - val_mae: 0.0096\n",
      "Epoch 67/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0760 - mae: 0.0080 - val_loss: 0.0753 - val_mae: 0.0056\n",
      "Epoch 68/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0747 - mae: 0.0069 - val_loss: 0.0740 - val_mae: 0.0050\n",
      "Epoch 69/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0734 - mae: 0.0068 - val_loss: 0.0728 - val_mae: 0.0074\n",
      "Epoch 70/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0724 - mae: 0.0124 - val_loss: 0.0715 - val_mae: 0.0073\n",
      "Epoch 71/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0710 - mae: 0.0105 - val_loss: 0.0703 - val_mae: 0.0069\n",
      "Epoch 72/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0699 - mae: 0.0107 - val_loss: 0.0691 - val_mae: 0.0051\n",
      "Epoch 73/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0685 - mae: 0.0050 - val_loss: 0.0679 - val_mae: 0.0032\n",
      "Epoch 74/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0673 - mae: 0.0062 - val_loss: 0.0668 - val_mae: 0.0081\n",
      "Epoch 75/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0661 - mae: 0.0066 - val_loss: 0.0656 - val_mae: 0.0101\n",
      "Epoch 76/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0651 - mae: 0.0108 - val_loss: 0.0643 - val_mae: 0.0045\n",
      "Epoch 77/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0638 - mae: 0.0060 - val_loss: 0.0633 - val_mae: 0.0092\n",
      "Epoch 78/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0626 - mae: 0.0066 - val_loss: 0.0621 - val_mae: 0.0061\n",
      "Epoch 79/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0615 - mae: 0.0082 - val_loss: 0.0609 - val_mae: 0.0058\n",
      "Epoch 80/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0604 - mae: 0.0082 - val_loss: 0.0597 - val_mae: 0.0048\n",
      "Epoch 81/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0595 - mae: 0.0110 - val_loss: 0.0648 - val_mae: 0.0624\n",
      "Epoch 82/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0586 - mae: 0.0133 - val_loss: 0.0577 - val_mae: 0.0047\n",
      "Epoch 83/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0573 - mae: 0.0068 - val_loss: 0.0567 - val_mae: 0.0042\n",
      "Epoch 84/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0563 - mae: 0.0060 - val_loss: 0.0560 - val_mae: 0.0117\n",
      "Epoch 85/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0555 - mae: 0.0107 - val_loss: 0.0561 - val_mae: 0.0285\n",
      "Epoch 86/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0546 - mae: 0.0093 - val_loss: 0.0540 - val_mae: 0.0038\n",
      "Epoch 87/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0535 - mae: 0.0052 - val_loss: 0.0531 - val_mae: 0.0080\n",
      "Epoch 88/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0526 - mae: 0.0057 - val_loss: 0.0521 - val_mae: 0.0045\n",
      "Epoch 89/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0517 - mae: 0.0064 - val_loss: 0.0512 - val_mae: 0.0045\n",
      "Epoch 90/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0511 - mae: 0.0118 - val_loss: 0.0505 - val_mae: 0.0112\n",
      "Epoch 91/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0501 - mae: 0.0097 - val_loss: 0.0495 - val_mae: 0.0045\n",
      "Epoch 92/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0491 - mae: 0.0057 - val_loss: 0.0488 - val_mae: 0.0115\n",
      "Epoch 93/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0484 - mae: 0.0093 - val_loss: 0.0479 - val_mae: 0.0092\n",
      "Epoch 94/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0475 - mae: 0.0067 - val_loss: 0.0472 - val_mae: 0.0125\n",
      "Epoch 95/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0467 - mae: 0.0073 - val_loss: 0.0462 - val_mae: 0.0034\n",
      "Epoch 96/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0458 - mae: 0.0055 - val_loss: 0.0462 - val_mae: 0.0229\n",
      "Epoch 97/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0458 - mae: 0.0157 - val_loss: 0.0447 - val_mae: 0.0041\n",
      "Epoch 98/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0443 - mae: 0.0050 - val_loss: 0.0440 - val_mae: 0.0046\n",
      "Epoch 99/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0436 - mae: 0.0044 - val_loss: 0.0433 - val_mae: 0.0040\n",
      "Epoch 100/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0429 - mae: 0.0044 - val_loss: 0.0426 - val_mae: 0.0074\n",
      "Epoch 101/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0422 - mae: 0.0058 - val_loss: 0.0418 - val_mae: 0.0037\n",
      "Epoch 102/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0418 - mae: 0.0133 - val_loss: 0.0411 - val_mae: 0.0056\n",
      "Epoch 103/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0408 - mae: 0.0072 - val_loss: 0.0404 - val_mae: 0.0047\n",
      "Epoch 104/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0401 - mae: 0.0060 - val_loss: 0.0398 - val_mae: 0.0042\n",
      "Epoch 105/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0394 - mae: 0.0046 - val_loss: 0.0391 - val_mae: 0.0042\n",
      "Epoch 106/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0388 - mae: 0.0051 - val_loss: 0.0384 - val_mae: 0.0048\n",
      "Epoch 107/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0381 - mae: 0.0049 - val_loss: 0.0377 - val_mae: 0.0052\n",
      "Epoch 108/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0374 - mae: 0.0049 - val_loss: 0.0370 - val_mae: 0.0041\n",
      "Epoch 109/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0367 - mae: 0.0068 - val_loss: 0.0363 - val_mae: 0.0052\n",
      "Epoch 110/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0360 - mae: 0.0066 - val_loss: 0.0357 - val_mae: 0.0076\n",
      "Epoch 111/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0354 - mae: 0.0079 - val_loss: 0.0349 - val_mae: 0.0038\n",
      "Epoch 112/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0347 - mae: 0.0073 - val_loss: 0.0343 - val_mae: 0.0053\n",
      "Epoch 113/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0340 - mae: 0.0067 - val_loss: 0.0336 - val_mae: 0.0054\n",
      "Epoch 114/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0334 - mae: 0.0072 - val_loss: 0.0330 - val_mae: 0.0036\n",
      "Epoch 115/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0327 - mae: 0.0069 - val_loss: 0.0324 - val_mae: 0.0058\n",
      "Epoch 116/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0321 - mae: 0.0062 - val_loss: 0.0318 - val_mae: 0.0059\n",
      "Epoch 117/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0315 - mae: 0.0054 - val_loss: 0.0311 - val_mae: 0.0044\n",
      "Epoch 118/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0311 - mae: 0.0105 - val_loss: 0.0313 - val_mae: 0.0222\n",
      "Epoch 119/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0305 - mae: 0.0110 - val_loss: 0.0301 - val_mae: 0.0048\n",
      "Epoch 120/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0298 - mae: 0.0042 - val_loss: 0.0296 - val_mae: 0.0068\n",
      "Epoch 121/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0293 - mae: 0.0051 - val_loss: 0.0290 - val_mae: 0.0033\n",
      "Epoch 122/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0288 - mae: 0.0043 - val_loss: 0.0286 - val_mae: 0.0068\n",
      "Epoch 123/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0283 - mae: 0.0044 - val_loss: 0.0280 - val_mae: 0.0054\n",
      "Epoch 124/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0278 - mae: 0.0067 - val_loss: 0.0275 - val_mae: 0.0031\n",
      "Epoch 125/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0272 - mae: 0.0042 - val_loss: 0.0270 - val_mae: 0.0040\n",
      "Epoch 126/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0268 - mae: 0.0072 - val_loss: 0.0264 - val_mae: 0.0029\n",
      "Epoch 127/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0262 - mae: 0.0062 - val_loss: 0.0261 - val_mae: 0.0114\n",
      "Epoch 128/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0257 - mae: 0.0057 - val_loss: 0.0254 - val_mae: 0.0032\n",
      "Epoch 129/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0252 - mae: 0.0054 - val_loss: 0.0251 - val_mae: 0.0095\n",
      "Epoch 130/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0248 - mae: 0.0065 - val_loss: 0.0259 - val_mae: 0.0325\n",
      "Epoch 131/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0243 - mae: 0.0064 - val_loss: 0.0240 - val_mae: 0.0064\n",
      "Epoch 132/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0238 - mae: 0.0049 - val_loss: 0.0235 - val_mae: 0.0044\n",
      "Epoch 133/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0233 - mae: 0.0050 - val_loss: 0.0231 - val_mae: 0.0057\n",
      "Epoch 134/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0229 - mae: 0.0065 - val_loss: 0.0238 - val_mae: 0.0258\n",
      "Epoch 135/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0225 - mae: 0.0082 - val_loss: 0.0230 - val_mae: 0.0234\n",
      "Epoch 136/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0220 - mae: 0.0060 - val_loss: 0.0217 - val_mae: 0.0031\n",
      "Epoch 137/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0216 - mae: 0.0064 - val_loss: 0.0214 - val_mae: 0.0035\n",
      "Epoch 138/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0212 - mae: 0.0052 - val_loss: 0.0210 - val_mae: 0.0030\n",
      "Epoch 139/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0208 - mae: 0.0051 - val_loss: 0.0206 - val_mae: 0.0051\n",
      "Epoch 140/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0204 - mae: 0.0056 - val_loss: 0.0202 - val_mae: 0.0076\n",
      "Epoch 141/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0200 - mae: 0.0070 - val_loss: 0.0199 - val_mae: 0.0112\n",
      "Epoch 142/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0197 - mae: 0.0083 - val_loss: 0.0195 - val_mae: 0.0104\n",
      "Epoch 143/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0192 - mae: 0.0046 - val_loss: 0.0190 - val_mae: 0.0034\n",
      "Epoch 144/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0189 - mae: 0.0049 - val_loss: 0.0187 - val_mae: 0.0078\n",
      "Epoch 145/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0185 - mae: 0.0053 - val_loss: 0.0183 - val_mae: 0.0033\n",
      "Epoch 146/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0182 - mae: 0.0062 - val_loss: 0.0181 - val_mae: 0.0081\n",
      "Epoch 147/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0178 - mae: 0.0046 - val_loss: 0.0177 - val_mae: 0.0061\n",
      "Epoch 148/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0175 - mae: 0.0059 - val_loss: 0.0174 - val_mae: 0.0084\n",
      "Epoch 149/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0171 - mae: 0.0049 - val_loss: 0.0170 - val_mae: 0.0049\n",
      "Epoch 150/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0169 - mae: 0.0083 - val_loss: 0.0167 - val_mae: 0.0105\n",
      "Epoch 151/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0051 - val_loss: 0.0164 - val_mae: 0.0069\n",
      "Epoch 152/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0058 - val_loss: 0.0160 - val_mae: 0.0053\n",
      "Epoch 153/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0050 - val_loss: 0.0157 - val_mae: 0.0042\n",
      "Epoch 154/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0156 - mae: 0.0075 - val_loss: 0.0155 - val_mae: 0.0102\n",
      "Epoch 155/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0153 - mae: 0.0049 - val_loss: 0.0152 - val_mae: 0.0063\n",
      "Epoch 156/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0150 - mae: 0.0060 - val_loss: 0.0149 - val_mae: 0.0041\n",
      "Epoch 157/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0147 - mae: 0.0047 - val_loss: 0.0146 - val_mae: 0.0045\n",
      "Epoch 158/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0145 - mae: 0.0047 - val_loss: 0.0143 - val_mae: 0.0055\n",
      "Epoch 159/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0142 - mae: 0.0073 - val_loss: 0.0142 - val_mae: 0.0107\n",
      "Epoch 160/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0140 - mae: 0.0066 - val_loss: 0.0138 - val_mae: 0.0027\n",
      "Epoch 161/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0137 - mae: 0.0051 - val_loss: 0.0136 - val_mae: 0.0066\n",
      "Epoch 162/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0135 - mae: 0.0066 - val_loss: 0.0133 - val_mae: 0.0037\n",
      "Epoch 163/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0043 - val_loss: 0.0130 - val_mae: 0.0032\n",
      "Epoch 164/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0129 - mae: 0.0044 - val_loss: 0.0128 - val_mae: 0.0030\n",
      "Epoch 165/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0127 - mae: 0.0064 - val_loss: 0.0128 - val_mae: 0.0110\n",
      "Epoch 166/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0046 - val_loss: 0.0123 - val_mae: 0.0036\n",
      "Epoch 167/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0122 - mae: 0.0044 - val_loss: 0.0121 - val_mae: 0.0042\n",
      "Epoch 168/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0085 - val_loss: 0.0119 - val_mae: 0.0034\n",
      "Epoch 169/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0119 - mae: 0.0092 - val_loss: 0.0117 - val_mae: 0.0054\n",
      "Epoch 170/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0116 - mae: 0.0038 - val_loss: 0.0115 - val_mae: 0.0035\n",
      "Epoch 171/500\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0114 - mae: 0.0046 - val_loss: 0.0113 - val_mae: 0.0032\n",
      "Epoch 172/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0113 - mae: 0.0066 - val_loss: 0.0111 - val_mae: 0.0033\n",
      "Epoch 173/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0111 - mae: 0.0054 - val_loss: 0.0110 - val_mae: 0.0038\n",
      "Epoch 174/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0109 - mae: 0.0034 - val_loss: 0.0108 - val_mae: 0.0031\n",
      "Epoch 175/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0107 - mae: 0.0050 - val_loss: 0.0106 - val_mae: 0.0042\n",
      "Epoch 176/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0105 - mae: 0.0044 - val_loss: 0.0104 - val_mae: 0.0038\n",
      "Epoch 177/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0103 - mae: 0.0040 - val_loss: 0.0102 - val_mae: 0.0045\n",
      "Epoch 178/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0048 - val_loss: 0.0100 - val_mae: 0.0033\n",
      "Epoch 179/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0100 - mae: 0.0049 - val_loss: 0.0099 - val_mae: 0.0059\n",
      "Epoch 180/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0098 - mae: 0.0045 - val_loss: 0.0097 - val_mae: 0.0080\n",
      "Epoch 181/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0053 - val_loss: 0.0095 - val_mae: 0.0043\n",
      "Epoch 182/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0058 - val_loss: 0.0093 - val_mae: 0.0040\n",
      "Epoch 183/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0048 - val_loss: 0.0091 - val_mae: 0.0049\n",
      "Epoch 184/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0056 - val_loss: 0.0091 - val_mae: 0.0110\n",
      "Epoch 185/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0064 - val_loss: 0.0088 - val_mae: 0.0039\n",
      "Epoch 186/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0078 - val_loss: 0.0087 - val_mae: 0.0034\n",
      "Epoch 187/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0041 - val_loss: 0.0085 - val_mae: 0.0039\n",
      "Epoch 188/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0085 - mae: 0.0055 - val_loss: 0.0084 - val_mae: 0.0067\n",
      "Epoch 189/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0041 - val_loss: 0.0082 - val_mae: 0.0037\n",
      "Epoch 190/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0049 - val_loss: 0.0081 - val_mae: 0.0050\n",
      "Epoch 191/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0081 - mae: 0.0052 - val_loss: 0.0080 - val_mae: 0.0036\n",
      "Epoch 192/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0042 - val_loss: 0.0078 - val_mae: 0.0052\n",
      "Epoch 193/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0050 - val_loss: 0.0077 - val_mae: 0.0033\n",
      "Epoch 194/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0077 - mae: 0.0055 - val_loss: 0.0076 - val_mae: 0.0042\n",
      "Epoch 195/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0075 - mae: 0.0049 - val_loss: 0.0074 - val_mae: 0.0047\n",
      "Epoch 196/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0074 - mae: 0.0047 - val_loss: 0.0073 - val_mae: 0.0038\n",
      "Epoch 197/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0073 - mae: 0.0045 - val_loss: 0.0072 - val_mae: 0.0040\n",
      "Epoch 198/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0071 - mae: 0.0053 - val_loss: 0.0071 - val_mae: 0.0044\n",
      "Epoch 199/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0070 - mae: 0.0054 - val_loss: 0.0069 - val_mae: 0.0039\n",
      "Epoch 200/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0069 - mae: 0.0056 - val_loss: 0.0068 - val_mae: 0.0063\n",
      "Epoch 201/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0068 - mae: 0.0057 - val_loss: 0.0067 - val_mae: 0.0035\n",
      "Epoch 202/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0067 - mae: 0.0054 - val_loss: 0.0066 - val_mae: 0.0048\n",
      "Epoch 203/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0066 - mae: 0.0051 - val_loss: 0.0065 - val_mae: 0.0069\n",
      "Epoch 204/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0064 - mae: 0.0042 - val_loss: 0.0064 - val_mae: 0.0029\n",
      "Epoch 205/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0063 - mae: 0.0040 - val_loss: 0.0063 - val_mae: 0.0041\n",
      "Epoch 206/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0063 - mae: 0.0053 - val_loss: 0.0062 - val_mae: 0.0035\n",
      "Epoch 207/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0062 - mae: 0.0061 - val_loss: 0.0061 - val_mae: 0.0071\n",
      "Epoch 208/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0061 - mae: 0.0051 - val_loss: 0.0060 - val_mae: 0.0085\n",
      "Epoch 209/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0060 - mae: 0.0051 - val_loss: 0.0059 - val_mae: 0.0056\n",
      "Epoch 210/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0059 - mae: 0.0049 - val_loss: 0.0058 - val_mae: 0.0051\n",
      "Epoch 211/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0058 - mae: 0.0064 - val_loss: 0.0057 - val_mae: 0.0042\n",
      "Epoch 212/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0057 - mae: 0.0052 - val_loss: 0.0056 - val_mae: 0.0038\n",
      "Epoch 213/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0048 - val_loss: 0.0056 - val_mae: 0.0065\n",
      "Epoch 214/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0055 - mae: 0.0050 - val_loss: 0.0055 - val_mae: 0.0066\n",
      "Epoch 215/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0082 - val_loss: 0.0054 - val_mae: 0.0037\n",
      "Epoch 216/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0054 - mae: 0.0041 - val_loss: 0.0053 - val_mae: 0.0036\n",
      "Epoch 217/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0053 - mae: 0.0043 - val_loss: 0.0054 - val_mae: 0.0120\n",
      "Epoch 218/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0053 - mae: 0.0062 - val_loss: 0.0052 - val_mae: 0.0040\n",
      "Epoch 219/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0052 - mae: 0.0042 - val_loss: 0.0051 - val_mae: 0.0037\n",
      "Epoch 220/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0051 - mae: 0.0042 - val_loss: 0.0051 - val_mae: 0.0050\n",
      "Epoch 221/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0050 - mae: 0.0047 - val_loss: 0.0050 - val_mae: 0.0071\n",
      "Epoch 222/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0050 - mae: 0.0051 - val_loss: 0.0049 - val_mae: 0.0051\n",
      "Epoch 223/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0049 - mae: 0.0040 - val_loss: 0.0049 - val_mae: 0.0063\n",
      "Epoch 224/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0048 - mae: 0.0052 - val_loss: 0.0048 - val_mae: 0.0039\n",
      "Epoch 225/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0048 - mae: 0.0049 - val_loss: 0.0047 - val_mae: 0.0033\n",
      "Epoch 226/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0047 - mae: 0.0069 - val_loss: 0.0047 - val_mae: 0.0056\n",
      "Epoch 227/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0046 - mae: 0.0055 - val_loss: 0.0046 - val_mae: 0.0035\n",
      "Epoch 228/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0046 - mae: 0.0049 - val_loss: 0.0045 - val_mae: 0.0045\n",
      "Epoch 229/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0045 - mae: 0.0048 - val_loss: 0.0046 - val_mae: 0.0085\n",
      "Epoch 230/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0045 - mae: 0.0059 - val_loss: 0.0044 - val_mae: 0.0044\n",
      "Epoch 231/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0044 - mae: 0.0055 - val_loss: 0.0044 - val_mae: 0.0044\n",
      "Epoch 232/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0043 - mae: 0.0051 - val_loss: 0.0044 - val_mae: 0.0078\n",
      "Epoch 233/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0043 - mae: 0.0051 - val_loss: 0.0043 - val_mae: 0.0065\n",
      "Epoch 234/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0053 - val_loss: 0.0042 - val_mae: 0.0032\n",
      "Epoch 235/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0057 - val_loss: 0.0042 - val_mae: 0.0042\n",
      "Epoch 236/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0041 - mae: 0.0058 - val_loss: 0.0041 - val_mae: 0.0067\n",
      "Epoch 237/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0041 - mae: 0.0055 - val_loss: 0.0040 - val_mae: 0.0042\n",
      "Epoch 238/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0040 - mae: 0.0042 - val_loss: 0.0040 - val_mae: 0.0058\n",
      "Epoch 239/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0040 - mae: 0.0046 - val_loss: 0.0039 - val_mae: 0.0030\n",
      "Epoch 240/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0039 - mae: 0.0048 - val_loss: 0.0039 - val_mae: 0.0057\n",
      "Epoch 241/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0039 - mae: 0.0057 - val_loss: 0.0039 - val_mae: 0.0038\n",
      "Epoch 242/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0044 - val_loss: 0.0038 - val_mae: 0.0033\n",
      "Epoch 243/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0048 - val_loss: 0.0038 - val_mae: 0.0054\n",
      "Epoch 244/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0050 - val_loss: 0.0037 - val_mae: 0.0041\n",
      "Epoch 245/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0037 - mae: 0.0058 - val_loss: 0.0037 - val_mae: 0.0038\n",
      "Epoch 246/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0037 - mae: 0.0045 - val_loss: 0.0037 - val_mae: 0.0066\n",
      "Epoch 247/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0037 - mae: 0.0063 - val_loss: 0.0036 - val_mae: 0.0033\n",
      "Epoch 248/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0036 - mae: 0.0062 - val_loss: 0.0036 - val_mae: 0.0032\n",
      "Epoch 249/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0036 - mae: 0.0042 - val_loss: 0.0035 - val_mae: 0.0035\n",
      "Epoch 250/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0035 - mae: 0.0048 - val_loss: 0.0035 - val_mae: 0.0032\n",
      "Epoch 251/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0035 - mae: 0.0043 - val_loss: 0.0035 - val_mae: 0.0071\n",
      "Epoch 252/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0035 - mae: 0.0062 - val_loss: 0.0034 - val_mae: 0.0039\n",
      "Epoch 253/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0034 - mae: 0.0061 - val_loss: 0.0034 - val_mae: 0.0033\n",
      "Epoch 254/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0034 - mae: 0.0046 - val_loss: 0.0034 - val_mae: 0.0075\n",
      "Epoch 255/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0033 - mae: 0.0048 - val_loss: 0.0033 - val_mae: 0.0051\n",
      "Epoch 256/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0033 - mae: 0.0042 - val_loss: 0.0033 - val_mae: 0.0030\n",
      "Epoch 257/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0033 - mae: 0.0056 - val_loss: 0.0033 - val_mae: 0.0071\n",
      "Epoch 258/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0033 - mae: 0.0057 - val_loss: 0.0032 - val_mae: 0.0029\n",
      "Epoch 259/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0032 - mae: 0.0048 - val_loss: 0.0032 - val_mae: 0.0031\n",
      "Epoch 260/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0032 - mae: 0.0046 - val_loss: 0.0032 - val_mae: 0.0046\n",
      "Epoch 261/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0032 - mae: 0.0062 - val_loss: 0.0031 - val_mae: 0.0049\n",
      "Epoch 262/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0031 - mae: 0.0042 - val_loss: 0.0031 - val_mae: 0.0054\n",
      "Epoch 263/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0031 - mae: 0.0053 - val_loss: 0.0031 - val_mae: 0.0039\n",
      "Epoch 264/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0031 - mae: 0.0049 - val_loss: 0.0030 - val_mae: 0.0033\n",
      "Epoch 265/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0030 - mae: 0.0051 - val_loss: 0.0030 - val_mae: 0.0034\n",
      "Epoch 266/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0030 - mae: 0.0050 - val_loss: 0.0030 - val_mae: 0.0059\n",
      "Epoch 267/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0030 - mae: 0.0040 - val_loss: 0.0029 - val_mae: 0.0038\n",
      "Epoch 268/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0029 - mae: 0.0044 - val_loss: 0.0029 - val_mae: 0.0041\n",
      "Epoch 269/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0029 - mae: 0.0053 - val_loss: 0.0029 - val_mae: 0.0067\n",
      "Epoch 270/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0029 - mae: 0.0044 - val_loss: 0.0029 - val_mae: 0.0052\n",
      "Epoch 271/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0029 - mae: 0.0044 - val_loss: 0.0028 - val_mae: 0.0031\n",
      "Epoch 272/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0052 - val_loss: 0.0028 - val_mae: 0.0047\n",
      "Epoch 273/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0046 - val_loss: 0.0028 - val_mae: 0.0033\n",
      "Epoch 274/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0049 - val_loss: 0.0028 - val_mae: 0.0054\n",
      "Epoch 275/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0044 - val_loss: 0.0027 - val_mae: 0.0049\n",
      "Epoch 276/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0058 - val_loss: 0.0027 - val_mae: 0.0034\n",
      "Epoch 277/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0073 - val_loss: 0.0027 - val_mae: 0.0028\n",
      "Epoch 278/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0027 - mae: 0.0039 - val_loss: 0.0027 - val_mae: 0.0035\n",
      "Epoch 279/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0027 - mae: 0.0040 - val_loss: 0.0027 - val_mae: 0.0051\n",
      "Epoch 280/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0027 - mae: 0.0049 - val_loss: 0.0026 - val_mae: 0.0030\n",
      "Epoch 281/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0046 - val_loss: 0.0026 - val_mae: 0.0030\n",
      "Epoch 282/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0058 - val_loss: 0.0026 - val_mae: 0.0046\n",
      "Epoch 283/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0048 - val_loss: 0.0026 - val_mae: 0.0032\n",
      "Epoch 284/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0048 - val_loss: 0.0026 - val_mae: 0.0073\n",
      "Epoch 285/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0047 - val_loss: 0.0026 - val_mae: 0.0067\n",
      "Epoch 286/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0061 - val_loss: 0.0025 - val_mae: 0.0056\n",
      "Epoch 287/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0051 - val_loss: 0.0025 - val_mae: 0.0039\n",
      "Epoch 288/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0045 - val_loss: 0.0025 - val_mae: 0.0075\n",
      "Epoch 289/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0045 - val_loss: 0.0024 - val_mae: 0.0026\n",
      "Epoch 290/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0057 - val_loss: 0.0024 - val_mae: 0.0050\n",
      "Epoch 291/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0056 - val_loss: 0.0024 - val_mae: 0.0033\n",
      "Epoch 292/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0024 - mae: 0.0051 - val_loss: 0.0024 - val_mae: 0.0032\n",
      "Epoch 293/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0024 - mae: 0.0048 - val_loss: 0.0024 - val_mae: 0.0031\n",
      "Epoch 294/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0024 - mae: 0.0041 - val_loss: 0.0024 - val_mae: 0.0045\n",
      "Epoch 295/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0024 - mae: 0.0051 - val_loss: 0.0023 - val_mae: 0.0027\n",
      "Epoch 296/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0024 - mae: 0.0053 - val_loss: 0.0023 - val_mae: 0.0037\n",
      "Epoch 297/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0024 - mae: 0.0053 - val_loss: 0.0023 - val_mae: 0.0042\n",
      "Epoch 298/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0041 - val_loss: 0.0023 - val_mae: 0.0041\n",
      "Epoch 299/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0042 - val_loss: 0.0023 - val_mae: 0.0030\n",
      "Epoch 300/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0052 - val_loss: 0.0023 - val_mae: 0.0045\n",
      "Epoch 301/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0050 - val_loss: 0.0023 - val_mae: 0.0038\n",
      "Epoch 302/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0051 - val_loss: 0.0023 - val_mae: 0.0044\n",
      "Epoch 303/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0045 - val_loss: 0.0022 - val_mae: 0.0035\n",
      "Epoch 304/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0052 - val_loss: 0.0023 - val_mae: 0.0074\n",
      "Epoch 305/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0043 - val_loss: 0.0022 - val_mae: 0.0051\n",
      "Epoch 306/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0051 - val_loss: 0.0022 - val_mae: 0.0036\n",
      "Epoch 307/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0046 - val_loss: 0.0022 - val_mae: 0.0077\n",
      "Epoch 308/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0056 - val_loss: 0.0022 - val_mae: 0.0087\n",
      "Epoch 309/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0041 - val_loss: 0.0022 - val_mae: 0.0029\n",
      "Epoch 310/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0051 - val_loss: 0.0022 - val_mae: 0.0076\n",
      "Epoch 311/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0049 - val_loss: 0.0022 - val_mae: 0.0074\n",
      "Epoch 312/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0069 - val_loss: 0.0021 - val_mae: 0.0056\n",
      "Epoch 313/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0042 - val_loss: 0.0021 - val_mae: 0.0033\n",
      "Epoch 314/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0038 - val_loss: 0.0021 - val_mae: 0.0051\n",
      "Epoch 315/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0046 - val_loss: 0.0021 - val_mae: 0.0060\n",
      "Epoch 316/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0044 - val_loss: 0.0021 - val_mae: 0.0070\n",
      "Epoch 317/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0045 - val_loss: 0.0021 - val_mae: 0.0040\n",
      "Epoch 318/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0053 - val_loss: 0.0021 - val_mae: 0.0038\n",
      "Epoch 319/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0049 - val_loss: 0.0020 - val_mae: 0.0040\n",
      "Epoch 320/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0043 - val_loss: 0.0020 - val_mae: 0.0029\n",
      "Epoch 321/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0043 - val_loss: 0.0021 - val_mae: 0.0054\n",
      "Epoch 322/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0061 - val_loss: 0.0020 - val_mae: 0.0037\n",
      "Epoch 323/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0020 - val_mae: 0.0034\n",
      "Epoch 324/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0050 - val_loss: 0.0020 - val_mae: 0.0051\n",
      "Epoch 325/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0052 - val_loss: 0.0020 - val_mae: 0.0028\n",
      "Epoch 326/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0047 - val_loss: 0.0020 - val_mae: 0.0037\n",
      "Epoch 327/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0055 - val_loss: 0.0020 - val_mae: 0.0032\n",
      "Epoch 328/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0044 - val_loss: 0.0020 - val_mae: 0.0036\n",
      "Epoch 329/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0050 - val_loss: 0.0019 - val_mae: 0.0035\n",
      "Epoch 330/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0040 - val_loss: 0.0019 - val_mae: 0.0033\n",
      "Epoch 331/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0041\n",
      "Epoch 332/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0049 - val_loss: 0.0020 - val_mae: 0.0059\n",
      "Epoch 333/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0050 - val_loss: 0.0019 - val_mae: 0.0057\n",
      "Epoch 334/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0050 - val_loss: 0.0019 - val_mae: 0.0041\n",
      "Epoch 335/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0054 - val_loss: 0.0019 - val_mae: 0.0055\n",
      "Epoch 336/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0045\n",
      "Epoch 337/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0046\n",
      "Epoch 338/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0038\n",
      "Epoch 339/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0050 - val_loss: 0.0019 - val_mae: 0.0044\n",
      "Epoch 340/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0053 - val_loss: 0.0019 - val_mae: 0.0048\n",
      "Epoch 341/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0031\n",
      "Epoch 342/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0046 - val_loss: 0.0018 - val_mae: 0.0031\n",
      "Epoch 343/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0046 - val_loss: 0.0018 - val_mae: 0.0036\n",
      "Epoch 344/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0035\n",
      "Epoch 345/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0041 - val_loss: 0.0019 - val_mae: 0.0066\n",
      "Epoch 346/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0049 - val_loss: 0.0019 - val_mae: 0.0065\n",
      "Epoch 347/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0057 - val_loss: 0.0018 - val_mae: 0.0045\n",
      "Epoch 348/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0044 - val_loss: 0.0018 - val_mae: 0.0035\n",
      "Epoch 349/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0052 - val_loss: 0.0018 - val_mae: 0.0033\n",
      "Epoch 350/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0048 - val_loss: 0.0018 - val_mae: 0.0065\n",
      "Epoch 351/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0046 - val_loss: 0.0018 - val_mae: 0.0045\n",
      "Epoch 352/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0051 - val_loss: 0.0018 - val_mae: 0.0037\n",
      "Epoch 353/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0049 - val_loss: 0.0018 - val_mae: 0.0086\n",
      "Epoch 354/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0045 - val_loss: 0.0018 - val_mae: 0.0030\n",
      "Epoch 355/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0049 - val_loss: 0.0018 - val_mae: 0.0053\n",
      "Epoch 356/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0066 - val_loss: 0.0017 - val_mae: 0.0037\n",
      "Epoch 357/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0045 - val_loss: 0.0018 - val_mae: 0.0044\n",
      "Epoch 358/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0049 - val_loss: 0.0018 - val_mae: 0.0056\n",
      "Epoch 359/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0045 - val_loss: 0.0017 - val_mae: 0.0040\n",
      "Epoch 360/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0033\n",
      "Epoch 361/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0055 - val_loss: 0.0017 - val_mae: 0.0047\n",
      "Epoch 362/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0042 - val_loss: 0.0017 - val_mae: 0.0034\n",
      "Epoch 363/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0040\n",
      "Epoch 364/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0054 - val_loss: 0.0017 - val_mae: 0.0030\n",
      "Epoch 365/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0050 - val_loss: 0.0017 - val_mae: 0.0048\n",
      "Epoch 366/500\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0041 - val_loss: 0.0017 - val_mae: 0.0047\n",
      "Epoch 367/500\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0044 - val_loss: 0.0017 - val_mae: 0.0039\n",
      "Epoch 368/500\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0030\n",
      "Epoch 369/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0060 - val_loss: 0.0017 - val_mae: 0.0030\n",
      "Epoch 370/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0038\n",
      "Epoch 371/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0042 - val_loss: 0.0018 - val_mae: 0.0108\n",
      "Epoch 372/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0057 - val_loss: 0.0017 - val_mae: 0.0033\n",
      "Epoch 373/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0040 - val_loss: 0.0016 - val_mae: 0.0029\n",
      "Epoch 374/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0037\n",
      "Epoch 375/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0047 - val_loss: 0.0017 - val_mae: 0.0052\n",
      "Epoch 376/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0050 - val_loss: 0.0016 - val_mae: 0.0038\n",
      "Epoch 377/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0042 - val_loss: 0.0016 - val_mae: 0.0044\n",
      "Epoch 378/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0030\n",
      "Epoch 379/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0052 - val_loss: 0.0016 - val_mae: 0.0037\n",
      "Epoch 380/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0068 - val_loss: 0.0016 - val_mae: 0.0032\n",
      "Epoch 381/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0044\n",
      "Epoch 382/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0043 - val_loss: 0.0016 - val_mae: 0.0052\n",
      "Epoch 383/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0042 - val_loss: 0.0016 - val_mae: 0.0049\n",
      "Epoch 384/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0051 - val_loss: 0.0016 - val_mae: 0.0049\n",
      "Epoch 385/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0045 - val_loss: 0.0016 - val_mae: 0.0034\n",
      "Epoch 386/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0045 - val_loss: 0.0016 - val_mae: 0.0040\n",
      "Epoch 387/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0052 - val_loss: 0.0016 - val_mae: 0.0031\n",
      "Epoch 388/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0049 - val_loss: 0.0016 - val_mae: 0.0046\n",
      "Epoch 389/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0051 - val_loss: 0.0017 - val_mae: 0.0103\n",
      "Epoch 390/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0058 - val_loss: 0.0016 - val_mae: 0.0053\n",
      "Epoch 391/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0055 - val_loss: 0.0016 - val_mae: 0.0051\n",
      "Epoch 392/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0055 - val_loss: 0.0016 - val_mae: 0.0048\n",
      "Epoch 393/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0060 - val_loss: 0.0016 - val_mae: 0.0040\n",
      "Epoch 394/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0044 - val_loss: 0.0016 - val_mae: 0.0035\n",
      "Epoch 395/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0049 - val_loss: 0.0015 - val_mae: 0.0032\n",
      "Epoch 396/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0066\n",
      "Epoch 397/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0044 - val_loss: 0.0016 - val_mae: 0.0054\n",
      "Epoch 398/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0050 - val_loss: 0.0018 - val_mae: 0.0148\n",
      "Epoch 399/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0057 - val_loss: 0.0017 - val_mae: 0.0100\n",
      "Epoch 400/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0052 - val_loss: 0.0015 - val_mae: 0.0046\n",
      "Epoch 401/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0068 - val_loss: 0.0016 - val_mae: 0.0061\n",
      "Epoch 402/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0052 - val_loss: 0.0015 - val_mae: 0.0052\n",
      "Epoch 403/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0044 - val_loss: 0.0015 - val_mae: 0.0034\n",
      "Epoch 404/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0042 - val_loss: 0.0015 - val_mae: 0.0029\n",
      "Epoch 405/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0041 - val_loss: 0.0015 - val_mae: 0.0048\n",
      "Epoch 406/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0066 - val_loss: 0.0016 - val_mae: 0.0073\n",
      "Epoch 407/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0049 - val_loss: 0.0015 - val_mae: 0.0044\n",
      "Epoch 408/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0044 - val_loss: 0.0015 - val_mae: 0.0058\n",
      "Epoch 409/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0050 - val_loss: 0.0015 - val_mae: 0.0050\n",
      "Epoch 410/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0052 - val_loss: 0.0015 - val_mae: 0.0051\n",
      "Epoch 411/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0048 - val_loss: 0.0015 - val_mae: 0.0040\n",
      "Epoch 412/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0052 - val_loss: 0.0015 - val_mae: 0.0050\n",
      "Epoch 413/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0050 - val_loss: 0.0015 - val_mae: 0.0059\n",
      "Epoch 414/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0053 - val_loss: 0.0015 - val_mae: 0.0040\n",
      "Epoch 415/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0057 - val_loss: 0.0015 - val_mae: 0.0046\n",
      "Epoch 416/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0050 - val_loss: 0.0015 - val_mae: 0.0051\n",
      "Epoch 417/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0048 - val_loss: 0.0015 - val_mae: 0.0036\n",
      "Epoch 418/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0047 - val_loss: 0.0015 - val_mae: 0.0032\n",
      "Epoch 419/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0047 - val_loss: 0.0015 - val_mae: 0.0060\n",
      "Epoch 420/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0031\n",
      "Epoch 421/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0058 - val_loss: 0.0015 - val_mae: 0.0072\n",
      "Epoch 422/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0047 - val_loss: 0.0015 - val_mae: 0.0032\n",
      "Epoch 423/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0050 - val_loss: 0.0015 - val_mae: 0.0050\n",
      "Epoch 424/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0048 - val_loss: 0.0015 - val_mae: 0.0040\n",
      "Epoch 425/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0055 - val_loss: 0.0015 - val_mae: 0.0073\n",
      "Epoch 426/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0051 - val_loss: 0.0015 - val_mae: 0.0054\n",
      "Epoch 427/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0048 - val_loss: 0.0015 - val_mae: 0.0055\n",
      "Epoch 428/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0048 - val_loss: 0.0014 - val_mae: 0.0037\n",
      "Epoch 429/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0049 - val_loss: 0.0014 - val_mae: 0.0038\n",
      "Epoch 430/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0063 - val_loss: 0.0015 - val_mae: 0.0057\n",
      "Epoch 431/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0048\n",
      "Epoch 432/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0051 - val_loss: 0.0014 - val_mae: 0.0046\n",
      "Epoch 433/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0057 - val_loss: 0.0015 - val_mae: 0.0050\n",
      "Epoch 434/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0047 - val_loss: 0.0014 - val_mae: 0.0046\n",
      "Epoch 435/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 0.0014 - val_mae: 0.0037\n",
      "Epoch 436/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0047 - val_loss: 0.0014 - val_mae: 0.0032\n",
      "Epoch 437/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0049 - val_loss: 0.0015 - val_mae: 0.0075\n",
      "Epoch 438/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0053 - val_loss: 0.0015 - val_mae: 0.0065\n",
      "Epoch 439/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0058 - val_loss: 0.0014 - val_mae: 0.0056\n",
      "Epoch 440/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0051 - val_loss: 0.0015 - val_mae: 0.0102\n",
      "Epoch 441/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0050 - val_loss: 0.0014 - val_mae: 0.0047\n",
      "Epoch 442/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0049 - val_loss: 0.0014 - val_mae: 0.0040\n",
      "Epoch 443/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0047 - val_loss: 0.0014 - val_mae: 0.0034\n",
      "Epoch 444/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0034\n",
      "Epoch 445/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0049 - val_loss: 0.0014 - val_mae: 0.0038\n",
      "Epoch 446/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 0.0014 - val_mae: 0.0067\n",
      "Epoch 447/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0055 - val_loss: 0.0014 - val_mae: 0.0035\n",
      "Epoch 448/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0050 - val_loss: 0.0014 - val_mae: 0.0038\n",
      "Epoch 449/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0056 - val_loss: 0.0014 - val_mae: 0.0049\n",
      "Epoch 450/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0037\n",
      "Epoch 451/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0040\n",
      "Epoch 452/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0051 - val_loss: 0.0014 - val_mae: 0.0045\n",
      "Epoch 453/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0054 - val_loss: 0.0014 - val_mae: 0.0039\n",
      "Epoch 454/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0053 - val_loss: 0.0014 - val_mae: 0.0041\n",
      "Epoch 455/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0083\n",
      "Epoch 456/500\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0049 - val_loss: 0.0014 - val_mae: 0.0043\n",
      "Epoch 457/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 0.0014 - val_mae: 0.0035\n",
      "Epoch 458/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0052\n",
      "Epoch 459/500\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0044\n",
      "Epoch 460/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0066 - val_loss: 0.0014 - val_mae: 0.0034\n",
      "Epoch 461/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0040 - val_loss: 0.0014 - val_mae: 0.0037\n",
      "Epoch 462/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 0.0015 - val_mae: 0.0125\n",
      "Epoch 463/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0040\n",
      "Epoch 464/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0034\n",
      "Epoch 465/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 0.0014 - val_mae: 0.0033\n",
      "Epoch 466/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0056 - val_loss: 0.0014 - val_mae: 0.0059\n",
      "Epoch 467/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 0.0014 - val_mae: 0.0077\n",
      "Epoch 468/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0050 - val_loss: 0.0014 - val_mae: 0.0062\n",
      "Epoch 469/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0051 - val_loss: 0.0014 - val_mae: 0.0038\n",
      "Epoch 470/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0050 - val_loss: 0.0013 - val_mae: 0.0038\n",
      "Epoch 471/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0052 - val_loss: 0.0013 - val_mae: 0.0041\n",
      "Epoch 472/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0055 - val_loss: 0.0013 - val_mae: 0.0036\n",
      "Epoch 473/500\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 0.0013 - val_mae: 0.0030\n",
      "Epoch 474/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0052 - val_loss: 0.0013 - val_mae: 0.0039\n",
      "Epoch 475/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0062\n",
      "Epoch 476/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 0.0014 - val_mae: 0.0066\n",
      "Epoch 477/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0043 - val_loss: 0.0014 - val_mae: 0.0050\n",
      "Epoch 478/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0057 - val_loss: 0.0013 - val_mae: 0.0040\n",
      "Epoch 479/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0054 - val_loss: 0.0013 - val_mae: 0.0036\n",
      "Epoch 480/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0051 - val_loss: 0.0014 - val_mae: 0.0057\n",
      "Epoch 481/500\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0051 - val_loss: 0.0014 - val_mae: 0.0056\n",
      "Epoch 482/500\n",
      "143/163 [=========================>....] - ETA: 0s - loss: 0.0013 - mae: 0.0051Restoring model weights from the end of the best epoch: 477.\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0049 - val_loss: 0.0013 - val_mae: 0.0040\n",
      "Epoch 482: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "\n",
    "    Dense(248, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(296, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(280, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(216, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(184, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(40, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(312, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(200, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(104, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 25\n",
    "epochs = 500\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T13:36:34.241480Z",
     "start_time": "2024-03-01T13:33:07.216347400Z"
    }
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 0s - loss: 0.0014 - mae: 0.0052 - 70ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.001351905521005392, 0.00520929554477334]"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test_scaled, y_test_scaled, verbose=2)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T13:36:54.435792400Z",
     "start_time": "2024-03-01T13:36:54.324309800Z"
    }
   },
   "id": "9050ab66ab36f169"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bsp. Predicted: [1459.7566] Actual: [1463.6] \n",
      "Durchschnittliche Abweichung (MAE): [4.92834433]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Führen Sie die Rücktransformation der skalierten Werte durch\n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[100]} Actual: {original_actual_values[100]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "\n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "\n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T13:36:55.086206200Z",
     "start_time": "2024-03-01T13:36:54.900323300Z"
    }
   },
   "id": "10cd79ca028075dd"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Werte die kleiner sind: 512\n"
     ]
    },
    {
     "data": {
      "text/plain": "             Echt  Vorhergesagt  X-Koordinate  Y-Koordinate   Differenz\n286   1236.860596        1360.7      0.000000          0.27 -123.839404\n1230  1236.907104        1360.7      0.016129          0.27 -123.792896\n96    1236.953491        1360.5      0.032258          0.27 -123.546509\n913   1220.262207        1343.8      0.000000          0.26 -123.537793\n647   1253.458740        1376.6      0.000000          0.28 -123.141260\n...           ...           ...           ...           ...         ...\n1210  1434.598389        1237.9      1.000000          0.50  196.698389\n227   1430.330078        1229.4      1.000000          0.53  200.930078\n989   1402.274048        1199.1      0.983871          0.60  203.174048\n172   1376.663208        1172.0      1.000000          0.62  204.663208\n549   1425.615845        1220.4      1.000000          0.55  205.215845\n\n[1273 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Echt</th>\n      <th>Vorhergesagt</th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Differenz</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>286</th>\n      <td>1236.860596</td>\n      <td>1360.7</td>\n      <td>0.000000</td>\n      <td>0.27</td>\n      <td>-123.839404</td>\n    </tr>\n    <tr>\n      <th>1230</th>\n      <td>1236.907104</td>\n      <td>1360.7</td>\n      <td>0.016129</td>\n      <td>0.27</td>\n      <td>-123.792896</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>1236.953491</td>\n      <td>1360.5</td>\n      <td>0.032258</td>\n      <td>0.27</td>\n      <td>-123.546509</td>\n    </tr>\n    <tr>\n      <th>913</th>\n      <td>1220.262207</td>\n      <td>1343.8</td>\n      <td>0.000000</td>\n      <td>0.26</td>\n      <td>-123.537793</td>\n    </tr>\n    <tr>\n      <th>647</th>\n      <td>1253.458740</td>\n      <td>1376.6</td>\n      <td>0.000000</td>\n      <td>0.28</td>\n      <td>-123.141260</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1210</th>\n      <td>1434.598389</td>\n      <td>1237.9</td>\n      <td>1.000000</td>\n      <td>0.50</td>\n      <td>196.698389</td>\n    </tr>\n    <tr>\n      <th>227</th>\n      <td>1430.330078</td>\n      <td>1229.4</td>\n      <td>1.000000</td>\n      <td>0.53</td>\n      <td>200.930078</td>\n    </tr>\n    <tr>\n      <th>989</th>\n      <td>1402.274048</td>\n      <td>1199.1</td>\n      <td>0.983871</td>\n      <td>0.60</td>\n      <td>203.174048</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>1376.663208</td>\n      <td>1172.0</td>\n      <td>1.000000</td>\n      <td>0.62</td>\n      <td>204.663208</td>\n    </tr>\n    <tr>\n      <th>549</th>\n      <td>1425.615845</td>\n      <td>1220.4</td>\n      <td>1.000000</td>\n      <td>0.55</td>\n      <td>205.215845</td>\n    </tr>\n  </tbody>\n</table>\n<p>1273 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'Echt': [val[0] for val in list1], 'Vorhergesagt': [val[0] for val in list2]})\n",
    "df_result['X-Koordinate'] = X_test_scaled[:, 0]\n",
    "df_result['Y-Koordinate'] = X_test_scaled[:, 1]\n",
    "\n",
    "df_result['Differenz'] = df_result['Echt'] - df_result['Vorhergesagt']\n",
    "df_result['Differenz'].sort_values()\n",
    "sorted_df = df_result.sort_values(by= 'Differenz')\n",
    "Anzahl_Punkte = (sorted_df['Differenz'] < -20).sum()\n",
    "print(\"Anzahl der Werte die kleiner sind:\", Anzahl_Punkte)\n",
    "\n",
    "sorted_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T12:58:13.981564800Z",
     "start_time": "2024-03-01T12:58:13.937760600Z"
    }
   },
   "id": "47d4a39665ed1159"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae = history.history['loss']\n",
    "val_mae = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(mae) + 1)\n",
    "\n",
    "# MAE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mae, 'r', label='Training MSE')\n",
    "plt.plot(epochs, val_mae, 'b', label='Validation MSE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T12:57:00.802772500Z",
     "start_time": "2024-03-01T12:57:00.653387300Z"
    }
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f6f672a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T23:32:30.399633900Z",
     "start_time": "2024-02-28T23:32:30.254682800Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# def build_model(learning_rate=0.001, activation='relu', regularization=0.0001, dropout_rate=0.0):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(200, activation=activation, input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(448, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(352, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(320, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(256, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(416, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(128, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(96, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))    \n",
    "# \n",
    "#     model.add(Dense(32, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "#     return model\n",
    "# \n",
    "# # Verwenden Sie eine Funktion, um das Modell zu instanziieren, für scikit-learn Wrapper\n",
    "# model = KerasRegressor(model=build_model, verbose=2)\n",
    "# \n",
    "# # Anpassung der Parameter im param_grid\n",
    "# param_grid = {\n",
    "#     'model__learning_rate': [0.01, 0.001, 0.0001],\n",
    "#     'model__regularization': [0.001, 0.0001],\n",
    "#     'fit__batch_size': [25, 50, 75, 100],\n",
    "#     'fit__epochs': [50],\n",
    "#     'model__dropout_rate' : [0.0, 0.1, 0.2]\n",
    "# }\n",
    "# \n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "# # Hinweis: Stellen Sie sicher, dass Ihre Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\n",
    "# grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "# # Beste Parameter und Score ausgeben\n",
    "# print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "# print(\"Beste Genauigkeit:\", grid_search.best_score_)\n",
    "# \n",
    "# with open(\"Gridsearch_D3.txt\", \"w\") as f:\n",
    "#     f.write(f\"Beste Parameter: {grid_search.best_params_}\\n\")\n",
    "#     f.write(f\"Beste Genauigkeit: {grid_search.best_score_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T23:32:30.417634100Z",
     "start_time": "2024-02-28T23:32:30.260114500Z"
    }
   },
   "id": "578403f6e218787a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b9aef262bcb2c3"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "492cf0ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T00:02:58.897856300Z",
     "start_time": "2024-02-28T23:32:30.266634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beste Hyperparameter für Lauf 1: {'input_units': 200, 'n_layers': 6, 'units_0': 328, 'units_1': 88, 'units_2': 88, 'units_3': 280, 'units_4': 24, 'units_5': 120, 'units_6': 120, 'units_7': 152, 'units_8': 72, 'units_9': 280}\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n",
      "Beste Hyperparameter für Lauf 2: {'input_units': 264, 'n_layers': 10, 'units_0': 88, 'units_1': 72, 'units_2': 280, 'units_3': 120, 'units_4': 136, 'units_5': 40, 'units_6': 168, 'units_7': 328, 'units_8': 8, 'units_9': 200}\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.39\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.40\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.41\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.42\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.43\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.44\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.45\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.46\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.47\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.48\n",
      "Beste Hyperparameter für Lauf 3: {'input_units': 232, 'n_layers': 8, 'units_0': 120, 'units_1': 296, 'units_2': 152, 'units_3': 216, 'units_4': 184, 'units_5': 136, 'units_6': 104, 'units_7': 120, 'units_8': 280, 'units_9': 72}\n"
     ]
    }
   ],
   "source": [
    "# # Funktion zum Erstellen des Modells\n",
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(hp.Int('input_units', min_value=8, max_value=328, step=16), input_shape=(2,), activation='relu'))\n",
    "#     for i in range(hp.Int('n_layers', 1, 10)):\n",
    "#         model.add(Dense(hp.Int(f'units_{i}', min_value=8, max_value=328, step=16), activation='relu'))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#     return model\n",
    "# \n",
    "# # Durchführung der Random Search dreimal\n",
    "# for run in range(1, 4):\n",
    "#     # Anpassen des Verzeichnisses und des Projektnamens für jeden Durchlauf\n",
    "#     directory = 'random_search'\n",
    "#     project_name = f'random_search_D3_{run}'\n",
    "#     \n",
    "#     tuner = RandomSearch(\n",
    "#         build_model,\n",
    "#         objective='val_loss',\n",
    "#         max_trials=100,\n",
    "#         executions_per_trial=1,\n",
    "#         directory=directory,\n",
    "#         project_name=project_name\n",
    "#     )\n",
    "#     \n",
    "#     # Durchführung des Random Search\n",
    "#     tuner.search(X_train_scaled, y_train_scaled, epochs=50, verbose =0, batch_size=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "#     \n",
    "#     # Abrufen und Speichern des besten Modells\n",
    "#     best_model = tuner.get_best_models(num_models=1)[0]\n",
    "#     model_path = os.path.join(directory, project_name, 'best_model.h5') \n",
    "#     best_model.save(model_path)\n",
    "#     \n",
    "# \n",
    "#     # Optional: Abrufen und Ausgeben der besten Hyperparameter\n",
    "#     best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "#     \n",
    "#     # Konvertieren der Hyperparameter in ein DataFrame\n",
    "#     df_hyperparameters = pd.DataFrame([best_hyperparameters.values])\n",
    "#     # Speichern des DataFrame als CSV\n",
    "#     df_hyperparameters.to_csv(f'random_search_D3_{run}.csv', index=False)\n",
    "#     \n",
    "#     print(f\"Beste Hyperparameter für Lauf {run}: {best_hyperparameters.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T00:02:58.902331700Z",
     "start_time": "2024-02-29T00:02:58.900316600Z"
    }
   },
   "id": "412f38f9b1e03d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

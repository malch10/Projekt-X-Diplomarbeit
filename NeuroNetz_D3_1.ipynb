{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b0518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:02:53.963902100Z",
     "start_time": "2024-04-15T15:02:48.418334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datenvorverarbeitung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc74b31b8226caeb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ff61b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-15T15:02:53.987923200Z",
     "start_time": "2024-04-15T15:02:53.963902100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n0          0.00000      -0.00200        500   7000   9000      669.05\n1          0.00000      -0.00196        500   7000   9000      696.80\n2          0.00000      -0.00192        500   7000   9000      724.42\n3          0.00000      -0.00188        500   7000   9000      751.84\n4          0.00000      -0.00184        500   7000   9000      779.83\n...            ...           ...        ...    ...    ...         ...\n6358       0.00248       0.00184        500   7000   9000      651.36\n6359       0.00248       0.00188        500   7000   9000      612.09\n6360       0.00248       0.00192        500   7000   9000      584.59\n6361       0.00248       0.00196        500   7000   9000      578.64\n6362       0.00248       0.00200        500   7000   9000      572.78\n\n[6363 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00000</td>\n      <td>-0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00000</td>\n      <td>-0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>696.80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00000</td>\n      <td>-0.00192</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>724.42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00000</td>\n      <td>-0.00188</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>751.84</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00000</td>\n      <td>-0.00184</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>779.83</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6358</th>\n      <td>0.00248</td>\n      <td>0.00184</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>651.36</td>\n    </tr>\n    <tr>\n      <th>6359</th>\n      <td>0.00248</td>\n      <td>0.00188</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>612.09</td>\n    </tr>\n    <tr>\n      <th>6360</th>\n      <td>0.00248</td>\n      <td>0.00192</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>584.59</td>\n    </tr>\n    <tr>\n      <th>6361</th>\n      <td>0.00248</td>\n      <td>0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>578.64</td>\n    </tr>\n    <tr>\n      <th>6362</th>\n      <td>0.00248</td>\n      <td>0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>572.78</td>\n    </tr>\n  </tbody>\n</table>\n<p>6363 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/TPath_300_finish_data.pkl')\n",
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/Finish_D3_I7000_F9000/TPath_500_finish_data_D3.pkl')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "966e3c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:47:06.577294300Z",
     "start_time": "2024-04-06T21:47:06.469074500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Temperatur\n0          0.00000      -0.00200      669.05\n1          0.00000      -0.00196      696.80\n2          0.00000      -0.00192      724.42\n3          0.00000      -0.00188      751.84\n4          0.00000      -0.00184      779.83\n...            ...           ...         ...\n6358       0.00248       0.00184      651.36\n6359       0.00248       0.00188      612.09\n6360       0.00248       0.00192      584.59\n6361       0.00248       0.00196      578.64\n6362       0.00248       0.00200      572.78\n\n[6363 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00000</td>\n      <td>-0.00200</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00000</td>\n      <td>-0.00196</td>\n      <td>696.80</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00000</td>\n      <td>-0.00192</td>\n      <td>724.42</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00000</td>\n      <td>-0.00188</td>\n      <td>751.84</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00000</td>\n      <td>-0.00184</td>\n      <td>779.83</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6358</th>\n      <td>0.00248</td>\n      <td>0.00184</td>\n      <td>651.36</td>\n    </tr>\n    <tr>\n      <th>6359</th>\n      <td>0.00248</td>\n      <td>0.00188</td>\n      <td>612.09</td>\n    </tr>\n    <tr>\n      <th>6360</th>\n      <td>0.00248</td>\n      <td>0.00192</td>\n      <td>584.59</td>\n    </tr>\n    <tr>\n      <th>6361</th>\n      <td>0.00248</td>\n      <td>0.00196</td>\n      <td>578.64</td>\n    </tr>\n    <tr>\n      <th>6362</th>\n      <td>0.00248</td>\n      <td>0.00200</td>\n      <td>572.78</td>\n    </tr>\n  </tbody>\n</table>\n<p>6363 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Droppen unnötiger Spalten\n",
    "df = data.drop(data.columns[2:5], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8783d1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:47:06.864166900Z",
     "start_time": "2024-04-06T21:47:06.705706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X-Koordinate  Y-Koordinate  Temperatur\n",
      "6243       0.00244       0.00128      978.37\n",
      "2949       0.00116      -0.00120     1193.00\n",
      "393        0.00012       0.00160      816.01\n",
      "3844       0.00152      -0.00176      827.43\n",
      "2154       0.00084      -0.00068     1419.40\n",
      "...            ...           ...         ...\n",
      "3772       0.00148      -0.00060     1381.60\n",
      "5191       0.00204      -0.00040     1320.60\n",
      "5226       0.00204       0.00100     1122.50\n",
      "5390       0.00212      -0.00052     1288.40\n",
      "860        0.00032       0.00008     1509.50\n",
      "\n",
      "[6363 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Temperatur\n0          0.00244       0.00128      978.37\n1          0.00116      -0.00120     1193.00\n2          0.00012       0.00160      816.01\n3          0.00152      -0.00176      827.43\n4          0.00084      -0.00068     1419.40\n...            ...           ...         ...\n6358       0.00148      -0.00060     1381.60\n6359       0.00204      -0.00040     1320.60\n6360       0.00204       0.00100     1122.50\n6361       0.00212      -0.00052     1288.40\n6362       0.00032       0.00008     1509.50\n\n[6363 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00244</td>\n      <td>0.00128</td>\n      <td>978.37</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00116</td>\n      <td>-0.00120</td>\n      <td>1193.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00012</td>\n      <td>0.00160</td>\n      <td>816.01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00152</td>\n      <td>-0.00176</td>\n      <td>827.43</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00084</td>\n      <td>-0.00068</td>\n      <td>1419.40</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6358</th>\n      <td>0.00148</td>\n      <td>-0.00060</td>\n      <td>1381.60</td>\n    </tr>\n    <tr>\n      <th>6359</th>\n      <td>0.00204</td>\n      <td>-0.00040</td>\n      <td>1320.60</td>\n    </tr>\n    <tr>\n      <th>6360</th>\n      <td>0.00204</td>\n      <td>0.00100</td>\n      <td>1122.50</td>\n    </tr>\n    <tr>\n      <th>6361</th>\n      <td>0.00212</td>\n      <td>-0.00052</td>\n      <td>1288.40</td>\n    </tr>\n    <tr>\n      <th>6362</th>\n      <td>0.00032</td>\n      <td>0.00008</td>\n      <td>1509.50</td>\n    </tr>\n  </tbody>\n</table>\n<p>6363 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomisieren der Anordnung \n",
    "df1 = df.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "print(df1)\n",
    "df_reset = df1.reset_index(drop=True)\n",
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4e72a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:47:06.981178700Z",
     "start_time": "2024-04-06T21:47:06.863659Z"
    }
   },
   "outputs": [],
   "source": [
    "# Festlegen der Gesamtdaten\n",
    "label = df_reset[\"Temperatur\"]\n",
    "# Korrektur: Verwenden Sie den Spaltennamen direkt, ohne Indexierung der columns-Eigenschaft\n",
    "df1 = df_reset.drop(\"Temperatur\", axis=1)\n",
    "X = df1\n",
    "y = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7fa289a50d87423"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e694a236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:47:07.662003400Z",
     "start_time": "2024-04-06T21:47:07.589103600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate\n0          0.00244       0.00128\n1          0.00116      -0.00120\n2          0.00012       0.00160\n3          0.00152      -0.00176\n4          0.00084      -0.00068\n...            ...           ...\n6358       0.00148      -0.00060\n6359       0.00204      -0.00040\n6360       0.00204       0.00100\n6361       0.00212      -0.00052\n6362       0.00032       0.00008\n\n[6363 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00244</td>\n      <td>0.00128</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00116</td>\n      <td>-0.00120</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00012</td>\n      <td>0.00160</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00152</td>\n      <td>-0.00176</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00084</td>\n      <td>-0.00068</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6358</th>\n      <td>0.00148</td>\n      <td>-0.00060</td>\n    </tr>\n    <tr>\n      <th>6359</th>\n      <td>0.00204</td>\n      <td>-0.00040</td>\n    </tr>\n    <tr>\n      <th>6360</th>\n      <td>0.00204</td>\n      <td>0.00100</td>\n    </tr>\n    <tr>\n      <th>6361</th>\n      <td>0.00212</td>\n      <td>-0.00052</td>\n    </tr>\n    <tr>\n      <th>6362</th>\n      <td>0.00032</td>\n      <td>0.00008</td>\n    </tr>\n  </tbody>\n</table>\n<p>6363 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f3303b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:47:07.889401100Z",
     "start_time": "2024-04-06T21:47:07.848275600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0        978.37\n1       1193.00\n2        816.01\n3        827.43\n4       1419.40\n         ...   \n6358    1381.60\n6359    1320.60\n6360    1122.50\n6361    1288.40\n6362    1509.50\nName: Temperatur, Length: 6363, dtype: float64"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3ad8da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:47:08.093016500Z",
     "start_time": "2024-04-06T21:47:07.966803500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Einteilung in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c705edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:47:08.833202700Z",
     "start_time": "2024-04-06T21:47:08.783837700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_features2 = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "X_test_scaled = scaler_features.transform(X_test)  # Nutze gleiche Skalierungsparameter ohne das X_Test Informationen einfließen\n",
    "\n",
    "# Initialisiere einen SEPARATEN MinMaxScaler für das Ziel\n",
    "scaler_target = MinMaxScaler()\n",
    "y_train_scaled = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbefe631e495b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:47:09.615048800Z",
     "start_time": "2024-04-06T21:47:09.549026300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.96774194, 0.58      ],\n       [0.12903226, 0.07      ],\n       [0.03225806, 0.07      ],\n       ...,\n       [0.01612903, 0.25      ],\n       [0.67741935, 0.82      ],\n       [0.5483871 , 0.65      ]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9999999999999999"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_scaled.max()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T21:47:10.021027600Z",
     "start_time": "2024-04-06T21:47:09.963440200Z"
    }
   },
   "id": "ce04ce43aac2242f"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "163/163 [==============================] - 2s 4ms/step - loss: 0.2221 - mae: 0.3242 - val_loss: 0.1305 - val_mae: 0.2449\n",
      "Epoch 2/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.1047 - mae: 0.2044 - val_loss: 0.0525 - val_mae: 0.1101\n",
      "Epoch 3/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0445 - mae: 0.0814 - val_loss: 0.0346 - val_mae: 0.0504\n",
      "Epoch 4/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0376 - mae: 0.0602 - val_loss: 0.0306 - val_mae: 0.0374\n",
      "Epoch 5/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0297 - mae: 0.0332 - val_loss: 0.0271 - val_mae: 0.0166\n",
      "Epoch 6/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0264 - mae: 0.0150 - val_loss: 0.0270 - val_mae: 0.0303\n",
      "Epoch 7/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0254 - mae: 0.0124 - val_loss: 0.0249 - val_mae: 0.0131\n",
      "Epoch 8/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0338 - mae: 0.0519 - val_loss: 0.0252 - val_mae: 0.0260\n",
      "Epoch 9/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0253 - mae: 0.0269 - val_loss: 0.0246 - val_mae: 0.0213\n",
      "Epoch 10/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0240 - mae: 0.0176 - val_loss: 0.0237 - val_mae: 0.0143\n",
      "Epoch 11/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0235 - mae: 0.0142 - val_loss: 0.0231 - val_mae: 0.0122\n",
      "Epoch 12/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0234 - mae: 0.0169 - val_loss: 0.0231 - val_mae: 0.0158\n",
      "Epoch 13/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0233 - mae: 0.0187 - val_loss: 0.0225 - val_mae: 0.0076\n",
      "Epoch 14/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0226 - mae: 0.0125 - val_loss: 0.0238 - val_mae: 0.0336\n",
      "Epoch 15/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0239 - mae: 0.0264 - val_loss: 0.0229 - val_mae: 0.0220\n",
      "Epoch 16/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0268 - mae: 0.0454 - val_loss: 0.0239 - val_mae: 0.0324\n",
      "Epoch 17/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0232 - mae: 0.0222 - val_loss: 0.0218 - val_mae: 0.0078\n",
      "Epoch 18/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0222 - mae: 0.0150 - val_loss: 0.0217 - val_mae: 0.0071\n",
      "Epoch 19/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0237 - mae: 0.0270 - val_loss: 0.0216 - val_mae: 0.0083\n",
      "Epoch 20/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0224 - mae: 0.0197 - val_loss: 0.0234 - val_mae: 0.0338\n",
      "Epoch 21/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0220 - mae: 0.0194 - val_loss: 0.0214 - val_mae: 0.0121\n",
      "Epoch 22/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0213 - mae: 0.0088 - val_loss: 0.0211 - val_mae: 0.0055\n",
      "Epoch 23/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0212 - mae: 0.0105 - val_loss: 0.0210 - val_mae: 0.0064\n",
      "Epoch 24/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0219 - mae: 0.0165 - val_loss: 0.0289 - val_mae: 0.0645\n",
      "Epoch 25/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0236 - mae: 0.0364 - val_loss: 0.0210 - val_mae: 0.0131\n",
      "Epoch 26/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0211 - mae: 0.0153 - val_loss: 0.0207 - val_mae: 0.0076\n",
      "Epoch 27/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0209 - mae: 0.0127 - val_loss: 0.0208 - val_mae: 0.0124\n",
      "Epoch 28/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0258 - mae: 0.0465 - val_loss: 0.0244 - val_mae: 0.0499\n",
      "Epoch 29/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0215 - mae: 0.0206 - val_loss: 0.0205 - val_mae: 0.0107\n",
      "Epoch 30/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0203 - mae: 0.0076 - val_loss: 0.0202 - val_mae: 0.0064\n",
      "Epoch 31/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0202 - mae: 0.0069 - val_loss: 0.0201 - val_mae: 0.0072\n",
      "Epoch 32/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0200 - mae: 0.0063 - val_loss: 0.0199 - val_mae: 0.0048\n",
      "Epoch 33/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0202 - mae: 0.0115 - val_loss: 0.0199 - val_mae: 0.0073\n",
      "Epoch 34/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0211 - mae: 0.0223 - val_loss: 0.0200 - val_mae: 0.0139\n",
      "Epoch 35/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0199 - mae: 0.0112 - val_loss: 0.0196 - val_mae: 0.0067\n",
      "Epoch 36/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0197 - mae: 0.0083 - val_loss: 0.0197 - val_mae: 0.0144\n",
      "Epoch 37/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0200 - mae: 0.0160 - val_loss: 0.0194 - val_mae: 0.0062\n",
      "Epoch 38/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0193 - mae: 0.0055 - val_loss: 0.0194 - val_mae: 0.0090\n",
      "Epoch 39/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0227 - mae: 0.0323 - val_loss: 0.0210 - val_mae: 0.0343\n",
      "Epoch 40/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0199 - mae: 0.0194 - val_loss: 0.0191 - val_mae: 0.0082\n",
      "Epoch 41/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0190 - mae: 0.0072 - val_loss: 0.0189 - val_mae: 0.0068\n",
      "Epoch 42/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0190 - mae: 0.0096 - val_loss: 0.0188 - val_mae: 0.0068\n",
      "Epoch 43/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0188 - mae: 0.0076 - val_loss: 0.0187 - val_mae: 0.0052\n",
      "Epoch 44/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0186 - mae: 0.0064 - val_loss: 0.0185 - val_mae: 0.0041\n",
      "Epoch 45/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0189 - mae: 0.0147 - val_loss: 0.0195 - val_mae: 0.0218\n",
      "Epoch 46/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0219 - mae: 0.0362 - val_loss: 0.0187 - val_mae: 0.0173\n",
      "Epoch 47/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0183 - mae: 0.0085 - val_loss: 0.0182 - val_mae: 0.0052\n",
      "Epoch 48/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0182 - mae: 0.0063 - val_loss: 0.0181 - val_mae: 0.0069\n",
      "Epoch 49/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0180 - mae: 0.0048 - val_loss: 0.0180 - val_mae: 0.0056\n",
      "Epoch 50/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0179 - mae: 0.0053 - val_loss: 0.0181 - val_mae: 0.0147\n",
      "Epoch 51/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0188 - mae: 0.0233 - val_loss: 0.0180 - val_mae: 0.0126\n",
      "Epoch 52/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0179 - mae: 0.0123 - val_loss: 0.0176 - val_mae: 0.0056\n",
      "Epoch 53/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0186 - mae: 0.0191 - val_loss: 0.0176 - val_mae: 0.0089\n",
      "Epoch 54/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0175 - mae: 0.0073 - val_loss: 0.0174 - val_mae: 0.0075\n",
      "Epoch 55/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0174 - mae: 0.0088 - val_loss: 0.0172 - val_mae: 0.0062\n",
      "Epoch 56/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0172 - mae: 0.0054 - val_loss: 0.0171 - val_mae: 0.0046\n",
      "Epoch 57/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0171 - mae: 0.0077 - val_loss: 0.0176 - val_mae: 0.0208\n",
      "Epoch 58/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0172 - mae: 0.0112 - val_loss: 0.0169 - val_mae: 0.0093\n",
      "Epoch 59/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0168 - mae: 0.0085 - val_loss: 0.0167 - val_mae: 0.0057\n",
      "Epoch 60/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0074 - val_loss: 0.0168 - val_mae: 0.0165\n",
      "Epoch 61/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0116 - val_loss: 0.0164 - val_mae: 0.0063\n",
      "Epoch 62/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0120 - val_loss: 0.0171 - val_mae: 0.0228\n",
      "Epoch 63/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0170 - mae: 0.0182 - val_loss: 0.0163 - val_mae: 0.0110\n",
      "Epoch 64/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0106 - val_loss: 0.0160 - val_mae: 0.0039\n",
      "Epoch 65/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0057 - val_loss: 0.0160 - val_mae: 0.0109\n",
      "Epoch 66/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0168 - mae: 0.0200 - val_loss: 0.0158 - val_mae: 0.0079\n",
      "Epoch 67/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0157 - mae: 0.0074 - val_loss: 0.0156 - val_mae: 0.0047\n",
      "Epoch 68/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0155 - mae: 0.0060 - val_loss: 0.0155 - val_mae: 0.0060\n",
      "Epoch 69/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0154 - mae: 0.0052 - val_loss: 0.0153 - val_mae: 0.0036\n",
      "Epoch 70/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0153 - mae: 0.0076 - val_loss: 0.0153 - val_mae: 0.0111\n",
      "Epoch 71/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0152 - mae: 0.0094 - val_loss: 0.0153 - val_mae: 0.0125\n",
      "Epoch 72/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0159 - mae: 0.0199 - val_loss: 0.0150 - val_mae: 0.0068\n",
      "Epoch 73/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0148 - mae: 0.0049 - val_loss: 0.0148 - val_mae: 0.0040\n",
      "Epoch 74/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0148 - mae: 0.0071 - val_loss: 0.0148 - val_mae: 0.0110\n",
      "Epoch 75/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0147 - mae: 0.0090 - val_loss: 0.0145 - val_mae: 0.0046\n",
      "Epoch 76/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0146 - mae: 0.0086 - val_loss: 0.0145 - val_mae: 0.0104\n",
      "Epoch 77/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0146 - mae: 0.0130 - val_loss: 0.0150 - val_mae: 0.0222\n",
      "Epoch 78/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0146 - mae: 0.0135 - val_loss: 0.0143 - val_mae: 0.0113\n",
      "Epoch 79/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0141 - mae: 0.0063 - val_loss: 0.0141 - val_mae: 0.0083\n",
      "Epoch 80/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0144 - mae: 0.0146 - val_loss: 0.0147 - val_mae: 0.0230\n",
      "Epoch 81/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0140 - mae: 0.0114 - val_loss: 0.0139 - val_mae: 0.0109\n",
      "Epoch 82/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0140 - mae: 0.0123 - val_loss: 0.0138 - val_mae: 0.0133\n",
      "Epoch 83/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0141 - mae: 0.0161 - val_loss: 0.0135 - val_mae: 0.0062\n",
      "Epoch 84/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0136 - mae: 0.0103 - val_loss: 0.0134 - val_mae: 0.0058\n",
      "Epoch 85/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0134 - mae: 0.0071 - val_loss: 0.0133 - val_mae: 0.0076\n",
      "Epoch 86/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0132 - mae: 0.0062 - val_loss: 0.0134 - val_mae: 0.0125\n",
      "Epoch 87/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0134 - mae: 0.0112 - val_loss: 0.0131 - val_mae: 0.0064\n",
      "Epoch 88/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0130 - mae: 0.0049 - val_loss: 0.0129 - val_mae: 0.0040\n",
      "Epoch 89/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0129 - mae: 0.0068 - val_loss: 0.0128 - val_mae: 0.0031\n",
      "Epoch 90/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0129 - mae: 0.0096 - val_loss: 0.0138 - val_mae: 0.0268\n",
      "Epoch 91/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0130 - mae: 0.0150 - val_loss: 0.0129 - val_mae: 0.0129\n",
      "Epoch 92/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0092 - val_loss: 0.0126 - val_mae: 0.0083\n",
      "Epoch 93/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0119 - val_loss: 0.0128 - val_mae: 0.0169\n",
      "Epoch 94/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0130 - mae: 0.0158 - val_loss: 0.0123 - val_mae: 0.0076\n",
      "Epoch 95/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0122 - mae: 0.0043 - val_loss: 0.0122 - val_mae: 0.0041\n",
      "Epoch 96/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0122 - mae: 0.0061 - val_loss: 0.0121 - val_mae: 0.0055\n",
      "Epoch 97/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0121 - mae: 0.0086 - val_loss: 0.0120 - val_mae: 0.0039\n",
      "Epoch 98/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0121 - mae: 0.0082 - val_loss: 0.0125 - val_mae: 0.0172\n",
      "Epoch 99/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0119 - mae: 0.0075 - val_loss: 0.0126 - val_mae: 0.0228\n",
      "Epoch 100/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0118 - mae: 0.0071 - val_loss: 0.0118 - val_mae: 0.0088\n",
      "Epoch 101/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0117 - mae: 0.0065 - val_loss: 0.0116 - val_mae: 0.0062\n",
      "Epoch 102/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0115 - mae: 0.0049 - val_loss: 0.0115 - val_mae: 0.0024\n",
      "Epoch 103/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0114 - mae: 0.0041 - val_loss: 0.0114 - val_mae: 0.0047\n",
      "Epoch 104/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0164 - val_loss: 0.0113 - val_mae: 0.0070\n",
      "Epoch 105/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0051 - val_loss: 0.0112 - val_mae: 0.0028\n",
      "Epoch 106/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0045 - val_loss: 0.0111 - val_mae: 0.0057\n",
      "Epoch 107/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0111 - mae: 0.0064 - val_loss: 0.0110 - val_mae: 0.0058\n",
      "Epoch 108/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0112 - mae: 0.0117 - val_loss: 0.0111 - val_mae: 0.0106\n",
      "Epoch 109/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0109 - mae: 0.0060 - val_loss: 0.0109 - val_mae: 0.0077\n",
      "Epoch 110/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0108 - mae: 0.0045 - val_loss: 0.0108 - val_mae: 0.0060\n",
      "Epoch 111/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0107 - mae: 0.0061 - val_loss: 0.0106 - val_mae: 0.0029\n",
      "Epoch 112/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0056 - val_loss: 0.0106 - val_mae: 0.0071\n",
      "Epoch 113/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0056 - val_loss: 0.0111 - val_mae: 0.0203\n",
      "Epoch 114/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0096 - val_loss: 0.0104 - val_mae: 0.0063\n",
      "Epoch 115/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0108 - val_loss: 0.0109 - val_mae: 0.0180\n",
      "Epoch 116/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0105 - mae: 0.0102 - val_loss: 0.0102 - val_mae: 0.0025\n",
      "Epoch 117/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0045 - val_loss: 0.0101 - val_mae: 0.0026\n",
      "Epoch 118/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0102 - mae: 0.0054 - val_loss: 0.0102 - val_mae: 0.0077\n",
      "Epoch 119/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0101 - mae: 0.0067 - val_loss: 0.0100 - val_mae: 0.0059\n",
      "Epoch 120/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0063 - val_loss: 0.0099 - val_mae: 0.0031\n",
      "Epoch 121/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0100 - mae: 0.0082 - val_loss: 0.0100 - val_mae: 0.0104\n",
      "Epoch 122/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0099 - mae: 0.0069 - val_loss: 0.0098 - val_mae: 0.0027\n",
      "Epoch 123/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0097 - mae: 0.0036 - val_loss: 0.0097 - val_mae: 0.0062\n",
      "Epoch 124/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0098 - mae: 0.0079 - val_loss: 0.0097 - val_mae: 0.0086\n",
      "Epoch 125/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0097 - mae: 0.0091 - val_loss: 0.0096 - val_mae: 0.0079\n",
      "Epoch 126/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0096 - mae: 0.0060 - val_loss: 0.0095 - val_mae: 0.0034\n",
      "Epoch 127/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0097 - mae: 0.0105 - val_loss: 0.0097 - val_mae: 0.0143\n",
      "Epoch 128/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0094 - mae: 0.0049 - val_loss: 0.0093 - val_mae: 0.0024\n",
      "Epoch 129/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0093 - mae: 0.0035 - val_loss: 0.0093 - val_mae: 0.0040\n",
      "Epoch 130/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0093 - mae: 0.0061 - val_loss: 0.0092 - val_mae: 0.0025\n",
      "Epoch 131/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0068 - val_loss: 0.0092 - val_mae: 0.0069\n",
      "Epoch 132/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0092 - mae: 0.0079 - val_loss: 0.0093 - val_mae: 0.0136\n",
      "Epoch 133/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0056 - val_loss: 0.0090 - val_mae: 0.0045\n",
      "Epoch 134/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0091 - mae: 0.0077 - val_loss: 0.0092 - val_mae: 0.0131\n",
      "Epoch 135/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0048 - val_loss: 0.0089 - val_mae: 0.0034\n",
      "Epoch 136/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0051 - val_loss: 0.0088 - val_mae: 0.0030\n",
      "Epoch 137/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0088 - mae: 0.0031 - val_loss: 0.0088 - val_mae: 0.0052\n",
      "Epoch 138/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0080 - val_loss: 0.0087 - val_mae: 0.0067\n",
      "Epoch 139/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0088 - mae: 0.0090 - val_loss: 0.0087 - val_mae: 0.0061\n",
      "Epoch 140/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0086 - mae: 0.0059 - val_loss: 0.0087 - val_mae: 0.0093\n",
      "Epoch 141/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0086 - mae: 0.0064 - val_loss: 0.0086 - val_mae: 0.0080\n",
      "Epoch 142/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0086 - mae: 0.0081 - val_loss: 0.0086 - val_mae: 0.0108\n",
      "Epoch 143/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0047 - val_loss: 0.0084 - val_mae: 0.0058\n",
      "Epoch 144/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0084 - mae: 0.0061 - val_loss: 0.0083 - val_mae: 0.0046\n",
      "Epoch 145/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0035 - val_loss: 0.0083 - val_mae: 0.0028\n",
      "Epoch 146/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0059 - val_loss: 0.0082 - val_mae: 0.0041\n",
      "Epoch 147/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0049 - val_loss: 0.0082 - val_mae: 0.0079\n",
      "Epoch 148/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0056 - val_loss: 0.0081 - val_mae: 0.0030\n",
      "Epoch 149/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0081 - mae: 0.0053 - val_loss: 0.0081 - val_mae: 0.0067\n",
      "Epoch 150/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0046 - val_loss: 0.0080 - val_mae: 0.0032\n",
      "Epoch 151/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0053 - val_loss: 0.0080 - val_mae: 0.0065\n",
      "Epoch 152/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0080 - mae: 0.0094 - val_loss: 0.0079 - val_mae: 0.0070\n",
      "Epoch 153/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0045 - val_loss: 0.0078 - val_mae: 0.0037\n",
      "Epoch 154/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0078 - mae: 0.0059 - val_loss: 0.0078 - val_mae: 0.0060\n",
      "Epoch 155/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0077 - mae: 0.0034 - val_loss: 0.0077 - val_mae: 0.0048\n",
      "Epoch 156/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0076 - mae: 0.0036 - val_loss: 0.0077 - val_mae: 0.0070\n",
      "Epoch 157/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0120 - val_loss: 0.0076 - val_mae: 0.0071\n",
      "Epoch 158/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0076 - mae: 0.0051 - val_loss: 0.0075 - val_mae: 0.0033\n",
      "Epoch 159/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0076 - mae: 0.0057 - val_loss: 0.0075 - val_mae: 0.0029\n",
      "Epoch 160/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0075 - mae: 0.0054 - val_loss: 0.0074 - val_mae: 0.0026\n",
      "Epoch 161/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0074 - mae: 0.0039 - val_loss: 0.0074 - val_mae: 0.0057\n",
      "Epoch 162/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0074 - mae: 0.0054 - val_loss: 0.0073 - val_mae: 0.0030\n",
      "Epoch 163/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0073 - mae: 0.0040 - val_loss: 0.0073 - val_mae: 0.0038\n",
      "Epoch 164/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0073 - mae: 0.0058 - val_loss: 0.0072 - val_mae: 0.0045\n",
      "Epoch 165/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0072 - mae: 0.0045 - val_loss: 0.0072 - val_mae: 0.0047\n",
      "Epoch 166/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0040 - val_loss: 0.0071 - val_mae: 0.0020\n",
      "Epoch 167/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0072 - mae: 0.0083 - val_loss: 0.0071 - val_mae: 0.0062\n",
      "Epoch 168/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0071 - mae: 0.0051 - val_loss: 0.0071 - val_mae: 0.0080\n",
      "Epoch 169/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0064 - val_loss: 0.0070 - val_mae: 0.0064\n",
      "Epoch 170/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0070 - mae: 0.0072 - val_loss: 0.0069 - val_mae: 0.0030\n",
      "Epoch 171/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0029 - val_loss: 0.0069 - val_mae: 0.0029\n",
      "Epoch 172/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0031 - val_loss: 0.0068 - val_mae: 0.0029\n",
      "Epoch 173/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0069 - mae: 0.0067 - val_loss: 0.0068 - val_mae: 0.0071\n",
      "Epoch 174/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0051 - val_loss: 0.0068 - val_mae: 0.0045\n",
      "Epoch 175/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0068 - mae: 0.0062 - val_loss: 0.0067 - val_mae: 0.0024\n",
      "Epoch 176/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0067 - mae: 0.0058 - val_loss: 0.0068 - val_mae: 0.0079\n",
      "Epoch 177/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0067 - mae: 0.0048 - val_loss: 0.0067 - val_mae: 0.0072\n",
      "Epoch 178/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0068 - mae: 0.0092 - val_loss: 0.0066 - val_mae: 0.0031\n",
      "Epoch 179/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0065 - mae: 0.0031 - val_loss: 0.0065 - val_mae: 0.0054\n",
      "Epoch 180/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0065 - mae: 0.0039 - val_loss: 0.0065 - val_mae: 0.0045\n",
      "Epoch 181/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0065 - mae: 0.0036 - val_loss: 0.0064 - val_mae: 0.0038\n",
      "Epoch 182/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0065 - mae: 0.0054 - val_loss: 0.0064 - val_mae: 0.0051\n",
      "Epoch 183/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0064 - mae: 0.0041 - val_loss: 0.0064 - val_mae: 0.0053\n",
      "Epoch 184/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0063 - mae: 0.0038 - val_loss: 0.0063 - val_mae: 0.0022\n",
      "Epoch 185/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0063 - mae: 0.0050 - val_loss: 0.0063 - val_mae: 0.0055\n",
      "Epoch 186/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0063 - mae: 0.0041 - val_loss: 0.0062 - val_mae: 0.0025\n",
      "Epoch 187/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0063 - mae: 0.0050 - val_loss: 0.0069 - val_mae: 0.0200\n",
      "Epoch 188/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0065 - mae: 0.0115 - val_loss: 0.0061 - val_mae: 0.0031\n",
      "Epoch 189/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0028 - val_loss: 0.0061 - val_mae: 0.0036\n",
      "Epoch 190/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0031 - val_loss: 0.0061 - val_mae: 0.0021\n",
      "Epoch 191/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0031 - val_loss: 0.0060 - val_mae: 0.0034\n",
      "Epoch 192/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0060 - mae: 0.0032 - val_loss: 0.0060 - val_mae: 0.0038\n",
      "Epoch 193/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0060 - mae: 0.0052 - val_loss: 0.0060 - val_mae: 0.0045\n",
      "Epoch 194/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0037 - val_loss: 0.0059 - val_mae: 0.0027\n",
      "Epoch 195/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0060 - mae: 0.0069 - val_loss: 0.0059 - val_mae: 0.0041\n",
      "Epoch 196/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0059 - mae: 0.0036 - val_loss: 0.0061 - val_mae: 0.0113\n",
      "Epoch 197/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0059 - mae: 0.0066 - val_loss: 0.0058 - val_mae: 0.0029\n",
      "Epoch 198/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0058 - mae: 0.0027 - val_loss: 0.0058 - val_mae: 0.0032\n",
      "Epoch 199/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0058 - mae: 0.0037 - val_loss: 0.0059 - val_mae: 0.0095\n",
      "Epoch 200/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0057 - mae: 0.0042 - val_loss: 0.0057 - val_mae: 0.0051\n",
      "Epoch 201/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0057 - mae: 0.0043 - val_loss: 0.0057 - val_mae: 0.0054\n",
      "Epoch 202/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0042 - val_loss: 0.0056 - val_mae: 0.0051\n",
      "Epoch 203/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0056 - mae: 0.0039 - val_loss: 0.0056 - val_mae: 0.0030\n",
      "Epoch 204/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0055 - mae: 0.0037 - val_loss: 0.0056 - val_mae: 0.0068\n",
      "Epoch 205/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0055 - mae: 0.0039 - val_loss: 0.0058 - val_mae: 0.0126\n",
      "Epoch 206/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0058 - mae: 0.0131 - val_loss: 0.0056 - val_mae: 0.0096\n",
      "Epoch 207/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0054 - mae: 0.0035 - val_loss: 0.0054 - val_mae: 0.0023\n",
      "Epoch 208/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0054 - mae: 0.0022 - val_loss: 0.0054 - val_mae: 0.0019\n",
      "Epoch 209/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0054 - mae: 0.0027 - val_loss: 0.0054 - val_mae: 0.0039\n",
      "Epoch 210/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0054 - mae: 0.0051 - val_loss: 0.0053 - val_mae: 0.0044\n",
      "Epoch 211/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0053 - mae: 0.0041 - val_loss: 0.0053 - val_mae: 0.0060\n",
      "Epoch 212/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0053 - mae: 0.0038 - val_loss: 0.0052 - val_mae: 0.0020\n",
      "Epoch 213/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0052 - mae: 0.0037 - val_loss: 0.0052 - val_mae: 0.0021\n",
      "Epoch 214/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0052 - mae: 0.0051 - val_loss: 0.0053 - val_mae: 0.0073\n",
      "Epoch 215/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0052 - mae: 0.0057 - val_loss: 0.0052 - val_mae: 0.0047\n",
      "Epoch 216/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0051 - mae: 0.0030 - val_loss: 0.0051 - val_mae: 0.0023\n",
      "Epoch 217/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0051 - mae: 0.0042 - val_loss: 0.0051 - val_mae: 0.0055\n",
      "Epoch 218/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0051 - mae: 0.0038 - val_loss: 0.0050 - val_mae: 0.0030\n",
      "Epoch 219/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0044 - val_loss: 0.0051 - val_mae: 0.0066\n",
      "Epoch 220/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0050 - mae: 0.0039 - val_loss: 0.0050 - val_mae: 0.0024\n",
      "Epoch 221/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0050 - mae: 0.0046 - val_loss: 0.0050 - val_mae: 0.0059\n",
      "Epoch 222/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0045 - val_loss: 0.0049 - val_mae: 0.0027\n",
      "Epoch 223/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0066 - val_loss: 0.0049 - val_mae: 0.0043\n",
      "Epoch 224/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0072 - val_loss: 0.0048 - val_mae: 0.0026\n",
      "Epoch 225/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0048 - mae: 0.0040 - val_loss: 0.0048 - val_mae: 0.0019\n",
      "Epoch 226/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0048 - mae: 0.0052 - val_loss: 0.0048 - val_mae: 0.0027\n",
      "Epoch 227/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0048 - mae: 0.0029 - val_loss: 0.0047 - val_mae: 0.0036\n",
      "Epoch 228/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0047 - mae: 0.0037 - val_loss: 0.0047 - val_mae: 0.0020\n",
      "Epoch 229/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0047 - mae: 0.0029 - val_loss: 0.0047 - val_mae: 0.0031\n",
      "Epoch 230/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0047 - mae: 0.0029 - val_loss: 0.0046 - val_mae: 0.0023\n",
      "Epoch 231/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0048 - mae: 0.0095 - val_loss: 0.0046 - val_mae: 0.0034\n",
      "Epoch 232/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0032 - val_loss: 0.0046 - val_mae: 0.0019\n",
      "Epoch 233/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0046 - mae: 0.0029 - val_loss: 0.0046 - val_mae: 0.0024\n",
      "Epoch 234/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0046 - mae: 0.0030 - val_loss: 0.0045 - val_mae: 0.0025\n",
      "Epoch 235/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0042 - val_loss: 0.0045 - val_mae: 0.0026\n",
      "Epoch 236/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0031 - val_loss: 0.0045 - val_mae: 0.0020\n",
      "Epoch 237/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0045 - mae: 0.0036 - val_loss: 0.0044 - val_mae: 0.0031\n",
      "Epoch 238/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0044 - mae: 0.0030 - val_loss: 0.0044 - val_mae: 0.0020\n",
      "Epoch 239/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0044 - mae: 0.0038 - val_loss: 0.0044 - val_mae: 0.0028\n",
      "Epoch 240/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0044 - mae: 0.0038 - val_loss: 0.0043 - val_mae: 0.0023\n",
      "Epoch 241/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0044 - mae: 0.0046 - val_loss: 0.0047 - val_mae: 0.0137\n",
      "Epoch 242/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0043 - mae: 0.0045 - val_loss: 0.0043 - val_mae: 0.0025\n",
      "Epoch 243/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0034 - val_loss: 0.0043 - val_mae: 0.0021\n",
      "Epoch 244/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0033 - val_loss: 0.0042 - val_mae: 0.0030\n",
      "Epoch 245/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0042 - val_loss: 0.0042 - val_mae: 0.0043\n",
      "Epoch 246/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0034 - val_loss: 0.0042 - val_mae: 0.0055\n",
      "Epoch 247/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0028 - val_loss: 0.0041 - val_mae: 0.0037\n",
      "Epoch 248/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0078 - val_loss: 0.0041 - val_mae: 0.0040\n",
      "Epoch 249/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0041 - mae: 0.0035 - val_loss: 0.0041 - val_mae: 0.0045\n",
      "Epoch 250/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0041 - mae: 0.0046 - val_loss: 0.0041 - val_mae: 0.0068\n",
      "Epoch 251/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0069 - val_loss: 0.0042 - val_mae: 0.0095\n",
      "Epoch 252/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0041 - mae: 0.0060 - val_loss: 0.0040 - val_mae: 0.0021\n",
      "Epoch 253/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0040 - mae: 0.0027 - val_loss: 0.0040 - val_mae: 0.0032\n",
      "Epoch 254/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0028 - val_loss: 0.0040 - val_mae: 0.0034\n",
      "Epoch 255/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0040 - mae: 0.0037 - val_loss: 0.0040 - val_mae: 0.0044\n",
      "Epoch 256/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0039 - mae: 0.0036 - val_loss: 0.0039 - val_mae: 0.0033\n",
      "Epoch 257/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0039 - mae: 0.0023 - val_loss: 0.0039 - val_mae: 0.0034\n",
      "Epoch 258/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0039 - mae: 0.0030 - val_loss: 0.0039 - val_mae: 0.0021\n",
      "Epoch 259/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0033 - val_loss: 0.0038 - val_mae: 0.0028\n",
      "Epoch 260/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0038 - val_loss: 0.0039 - val_mae: 0.0054\n",
      "Epoch 261/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0034 - val_loss: 0.0038 - val_mae: 0.0021\n",
      "Epoch 262/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0038 - mae: 0.0041 - val_loss: 0.0038 - val_mae: 0.0058\n",
      "Epoch 263/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0039 - val_loss: 0.0037 - val_mae: 0.0025\n",
      "Epoch 264/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0049 - val_loss: 0.0038 - val_mae: 0.0089\n",
      "Epoch 265/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0037 - mae: 0.0045 - val_loss: 0.0037 - val_mae: 0.0035\n",
      "Epoch 266/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0034 - val_loss: 0.0037 - val_mae: 0.0032\n",
      "Epoch 267/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0037 - mae: 0.0036 - val_loss: 0.0036 - val_mae: 0.0028\n",
      "Epoch 268/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0041 - val_loss: 0.0037 - val_mae: 0.0075\n",
      "Epoch 269/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0035 - val_loss: 0.0036 - val_mae: 0.0055\n",
      "Epoch 270/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0036 - mae: 0.0035 - val_loss: 0.0036 - val_mae: 0.0032\n",
      "Epoch 271/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0037 - val_loss: 0.0035 - val_mae: 0.0040\n",
      "Epoch 272/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0036 - mae: 0.0050 - val_loss: 0.0035 - val_mae: 0.0040\n",
      "Epoch 273/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0035 - mae: 0.0055 - val_loss: 0.0035 - val_mae: 0.0046\n",
      "Epoch 274/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0035 - mae: 0.0029 - val_loss: 0.0035 - val_mae: 0.0024\n",
      "Epoch 275/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0035 - mae: 0.0039 - val_loss: 0.0034 - val_mae: 0.0031\n",
      "Epoch 276/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0031 - val_loss: 0.0034 - val_mae: 0.0042\n",
      "Epoch 277/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0034 - mae: 0.0037 - val_loss: 0.0034 - val_mae: 0.0024\n",
      "Epoch 278/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0049 - val_loss: 0.0034 - val_mae: 0.0016\n",
      "Epoch 279/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0042 - val_loss: 0.0035 - val_mae: 0.0089\n",
      "Epoch 280/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0034 - mae: 0.0044 - val_loss: 0.0033 - val_mae: 0.0029\n",
      "Epoch 281/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0033 - mae: 0.0025 - val_loss: 0.0033 - val_mae: 0.0047\n",
      "Epoch 282/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0033 - mae: 0.0037 - val_loss: 0.0033 - val_mae: 0.0032\n",
      "Epoch 283/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0033 - mae: 0.0032 - val_loss: 0.0033 - val_mae: 0.0028\n",
      "Epoch 284/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0033 - mae: 0.0036 - val_loss: 0.0033 - val_mae: 0.0042\n",
      "Epoch 285/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0034 - val_loss: 0.0032 - val_mae: 0.0033\n",
      "Epoch 286/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0033 - mae: 0.0056 - val_loss: 0.0032 - val_mae: 0.0028\n",
      "Epoch 287/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0033 - val_loss: 0.0032 - val_mae: 0.0044\n",
      "Epoch 288/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0037 - val_loss: 0.0032 - val_mae: 0.0059\n",
      "Epoch 289/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0032 - mae: 0.0031 - val_loss: 0.0031 - val_mae: 0.0026\n",
      "Epoch 290/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0032 - mae: 0.0059 - val_loss: 0.0031 - val_mae: 0.0042\n",
      "Epoch 291/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0031 - mae: 0.0039 - val_loss: 0.0032 - val_mae: 0.0064\n",
      "Epoch 292/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0031 - mae: 0.0034 - val_loss: 0.0031 - val_mae: 0.0037\n",
      "Epoch 293/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0032 - val_loss: 0.0031 - val_mae: 0.0041\n",
      "Epoch 294/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0031 - mae: 0.0029 - val_loss: 0.0031 - val_mae: 0.0050\n",
      "Epoch 295/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0050 - val_loss: 0.0030 - val_mae: 0.0019\n",
      "Epoch 296/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0030 - mae: 0.0029 - val_loss: 0.0030 - val_mae: 0.0039\n",
      "Epoch 297/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0035 - val_loss: 0.0030 - val_mae: 0.0034\n",
      "Epoch 298/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0035 - val_loss: 0.0030 - val_mae: 0.0042\n",
      "Epoch 299/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0031 - val_loss: 0.0030 - val_mae: 0.0037\n",
      "Epoch 300/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0030 - mae: 0.0066 - val_loss: 0.0030 - val_mae: 0.0092\n",
      "Epoch 301/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0029 - mae: 0.0028 - val_loss: 0.0029 - val_mae: 0.0037\n",
      "Epoch 302/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0029 - mae: 0.0030 - val_loss: 0.0029 - val_mae: 0.0020\n",
      "Epoch 303/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0029 - mae: 0.0037 - val_loss: 0.0030 - val_mae: 0.0108\n",
      "Epoch 304/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0030 - mae: 0.0080 - val_loss: 0.0030 - val_mae: 0.0111\n",
      "Epoch 305/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0029 - mae: 0.0036 - val_loss: 0.0028 - val_mae: 0.0016\n",
      "Epoch 306/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0029 - mae: 0.0033 - val_loss: 0.0029 - val_mae: 0.0045\n",
      "Epoch 307/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0025 - val_loss: 0.0028 - val_mae: 0.0021\n",
      "Epoch 308/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0026 - val_loss: 0.0028 - val_mae: 0.0019\n",
      "Epoch 309/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0033 - val_loss: 0.0028 - val_mae: 0.0019\n",
      "Epoch 310/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0034 - val_loss: 0.0028 - val_mae: 0.0042\n",
      "Epoch 311/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0045 - val_loss: 0.0028 - val_mae: 0.0042\n",
      "Epoch 312/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0027 - mae: 0.0030 - val_loss: 0.0028 - val_mae: 0.0042\n",
      "Epoch 313/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0033 - val_loss: 0.0030 - val_mae: 0.0133\n",
      "Epoch 314/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0045 - val_loss: 0.0027 - val_mae: 0.0053\n",
      "Epoch 315/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0028 - mae: 0.0060 - val_loss: 0.0027 - val_mae: 0.0022\n",
      "Epoch 316/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0024 - val_loss: 0.0027 - val_mae: 0.0035\n",
      "Epoch 317/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0044 - val_loss: 0.0027 - val_mae: 0.0032\n",
      "Epoch 318/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0027 - mae: 0.0038 - val_loss: 0.0026 - val_mae: 0.0018\n",
      "Epoch 319/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0026 - val_loss: 0.0026 - val_mae: 0.0029\n",
      "Epoch 320/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0042 - val_loss: 0.0026 - val_mae: 0.0018\n",
      "Epoch 321/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0022 - val_loss: 0.0026 - val_mae: 0.0029\n",
      "Epoch 322/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0022 - val_loss: 0.0026 - val_mae: 0.0041\n",
      "Epoch 323/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0032 - val_loss: 0.0026 - val_mae: 0.0031\n",
      "Epoch 324/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0026 - mae: 0.0035 - val_loss: 0.0026 - val_mae: 0.0066\n",
      "Epoch 325/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0026 - mae: 0.0065 - val_loss: 0.0025 - val_mae: 0.0022\n",
      "Epoch 326/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0033 - val_loss: 0.0025 - val_mae: 0.0037\n",
      "Epoch 327/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0025 - val_loss: 0.0025 - val_mae: 0.0018\n",
      "Epoch 328/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0041 - val_loss: 0.0025 - val_mae: 0.0063\n",
      "Epoch 329/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0036 - val_loss: 0.0025 - val_mae: 0.0057\n",
      "Epoch 330/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0035 - val_loss: 0.0025 - val_mae: 0.0023\n",
      "Epoch 331/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0025 - mae: 0.0033 - val_loss: 0.0024 - val_mae: 0.0020\n",
      "Epoch 332/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0035 - val_loss: 0.0024 - val_mae: 0.0029\n",
      "Epoch 333/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0031 - val_loss: 0.0024 - val_mae: 0.0029\n",
      "Epoch 334/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0037 - val_loss: 0.0026 - val_mae: 0.0135\n",
      "Epoch 335/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0067 - val_loss: 0.0024 - val_mae: 0.0023\n",
      "Epoch 336/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0021 - val_loss: 0.0024 - val_mae: 0.0025\n",
      "Epoch 337/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0024 - mae: 0.0023 - val_loss: 0.0024 - val_mae: 0.0023\n",
      "Epoch 338/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0024 - mae: 0.0033 - val_loss: 0.0024 - val_mae: 0.0021\n",
      "Epoch 339/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0024 - mae: 0.0035 - val_loss: 0.0024 - val_mae: 0.0047\n",
      "Epoch 340/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0028 - val_loss: 0.0023 - val_mae: 0.0033\n",
      "Epoch 341/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0039 - val_loss: 0.0023 - val_mae: 0.0021\n",
      "Epoch 342/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0027 - val_loss: 0.0023 - val_mae: 0.0027\n",
      "Epoch 343/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0025 - val_loss: 0.0023 - val_mae: 0.0020\n",
      "Epoch 344/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0030 - val_loss: 0.0023 - val_mae: 0.0036\n",
      "Epoch 345/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0023 - mae: 0.0039 - val_loss: 0.0023 - val_mae: 0.0031\n",
      "Epoch 346/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0049 - val_loss: 0.0023 - val_mae: 0.0035\n",
      "Epoch 347/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0026 - val_loss: 0.0022 - val_mae: 0.0018\n",
      "Epoch 348/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0038 - val_loss: 0.0022 - val_mae: 0.0025\n",
      "Epoch 349/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0034 - val_loss: 0.0022 - val_mae: 0.0022\n",
      "Epoch 350/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0024 - val_loss: 0.0022 - val_mae: 0.0046\n",
      "Epoch 351/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0057 - val_loss: 0.0022 - val_mae: 0.0023\n",
      "Epoch 352/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0030 - val_loss: 0.0022 - val_mae: 0.0022\n",
      "Epoch 353/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0022 - mae: 0.0029 - val_loss: 0.0022 - val_mae: 0.0032\n",
      "Epoch 354/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0021 - val_loss: 0.0022 - val_mae: 0.0044\n",
      "Epoch 355/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0030 - val_loss: 0.0021 - val_mae: 0.0016\n",
      "Epoch 356/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0035 - val_loss: 0.0021 - val_mae: 0.0024\n",
      "Epoch 357/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0037 - val_loss: 0.0021 - val_mae: 0.0027\n",
      "Epoch 358/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0029 - val_loss: 0.0021 - val_mae: 0.0018\n",
      "Epoch 359/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0043 - val_loss: 0.0021 - val_mae: 0.0035\n",
      "Epoch 360/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0027 - val_loss: 0.0021 - val_mae: 0.0027\n",
      "Epoch 361/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0036 - val_loss: 0.0021 - val_mae: 0.0017\n",
      "Epoch 362/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0021 - mae: 0.0068 - val_loss: 0.0020 - val_mae: 0.0023\n",
      "Epoch 363/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0030 - val_loss: 0.0020 - val_mae: 0.0016\n",
      "Epoch 364/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0031 - val_loss: 0.0021 - val_mae: 0.0064\n",
      "Epoch 365/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0033 - val_loss: 0.0020 - val_mae: 0.0020\n",
      "Epoch 366/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0023 - val_loss: 0.0020 - val_mae: 0.0031\n",
      "Epoch 367/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0020 - mae: 0.0026 - val_loss: 0.0020 - val_mae: 0.0029\n",
      "Epoch 368/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0036 - val_loss: 0.0022 - val_mae: 0.0137\n",
      "Epoch 369/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0044 - val_loss: 0.0020 - val_mae: 0.0045\n",
      "Epoch 370/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0030 - val_loss: 0.0020 - val_mae: 0.0044\n",
      "Epoch 371/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0024 - val_loss: 0.0019 - val_mae: 0.0027\n",
      "Epoch 372/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0024 - val_loss: 0.0019 - val_mae: 0.0032\n",
      "Epoch 373/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0024 - val_loss: 0.0019 - val_mae: 0.0023\n",
      "Epoch 374/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0041 - val_loss: 0.0019 - val_mae: 0.0019\n",
      "Epoch 375/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0067 - val_loss: 0.0019 - val_mae: 0.0034\n",
      "Epoch 376/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0021 - val_loss: 0.0019 - val_mae: 0.0017\n",
      "Epoch 377/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0025 - val_loss: 0.0019 - val_mae: 0.0046\n",
      "Epoch 378/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0032 - val_loss: 0.0019 - val_mae: 0.0016\n",
      "Epoch 379/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0026 - val_loss: 0.0019 - val_mae: 0.0015\n",
      "Epoch 380/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0019 - mae: 0.0029 - val_loss: 0.0019 - val_mae: 0.0029\n",
      "Epoch 381/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0020 - mae: 0.0078 - val_loss: 0.0019 - val_mae: 0.0058\n",
      "Epoch 382/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0023 - val_loss: 0.0018 - val_mae: 0.0016\n",
      "Epoch 383/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0020 - val_loss: 0.0018 - val_mae: 0.0028\n",
      "Epoch 384/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0020 - val_loss: 0.0018 - val_mae: 0.0022\n",
      "Epoch 385/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0018 - mae: 0.0028 - val_loss: 0.0018 - val_mae: 0.0053\n",
      "Epoch 386/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0031 - val_loss: 0.0018 - val_mae: 0.0041\n",
      "Epoch 387/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0028 - val_loss: 0.0018 - val_mae: 0.0027\n",
      "Epoch 388/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0037 - val_loss: 0.0018 - val_mae: 0.0034\n",
      "Epoch 389/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0028 - val_loss: 0.0018 - val_mae: 0.0029\n",
      "Epoch 390/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0023 - val_loss: 0.0018 - val_mae: 0.0030\n",
      "Epoch 391/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0018 - mae: 0.0053 - val_loss: 0.0018 - val_mae: 0.0061\n",
      "Epoch 392/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0028 - val_loss: 0.0017 - val_mae: 0.0031\n",
      "Epoch 393/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0029 - val_loss: 0.0017 - val_mae: 0.0030\n",
      "Epoch 394/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0035 - val_loss: 0.0018 - val_mae: 0.0052\n",
      "Epoch 395/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0038 - val_loss: 0.0017 - val_mae: 0.0024\n",
      "Epoch 396/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0048 - val_loss: 0.0017 - val_mae: 0.0040\n",
      "Epoch 397/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0024 - val_loss: 0.0017 - val_mae: 0.0020\n",
      "Epoch 398/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0032 - val_loss: 0.0017 - val_mae: 0.0015\n",
      "Epoch 399/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0017 - mae: 0.0023 - val_loss: 0.0017 - val_mae: 0.0031\n",
      "Epoch 400/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0034 - val_loss: 0.0017 - val_mae: 0.0048\n",
      "Epoch 401/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0025 - val_loss: 0.0017 - val_mae: 0.0020\n",
      "Epoch 402/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0041 - val_loss: 0.0017 - val_mae: 0.0040\n",
      "Epoch 403/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0017 - mae: 0.0037 - val_loss: 0.0016 - val_mae: 0.0043\n",
      "Epoch 404/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0032 - val_loss: 0.0016 - val_mae: 0.0023\n",
      "Epoch 405/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0030 - val_loss: 0.0016 - val_mae: 0.0045\n",
      "Epoch 406/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0030 - val_loss: 0.0017 - val_mae: 0.0055\n",
      "Epoch 407/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0040 - val_loss: 0.0016 - val_mae: 0.0046\n",
      "Epoch 408/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0027 - val_loss: 0.0016 - val_mae: 0.0021\n",
      "Epoch 409/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 0.0016 - mae: 0.0025 - val_loss: 0.0016 - val_mae: 0.0036\n",
      "Epoch 410/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0032 - val_loss: 0.0016 - val_mae: 0.0015\n",
      "Epoch 411/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0033 - val_loss: 0.0016 - val_mae: 0.0043\n",
      "Epoch 412/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0026 - val_loss: 0.0016 - val_mae: 0.0019\n",
      "Epoch 413/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0016 - mae: 0.0032 - val_loss: 0.0015 - val_mae: 0.0028\n",
      "Epoch 414/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0025 - val_loss: 0.0016 - val_mae: 0.0069\n",
      "Epoch 415/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0030 - val_loss: 0.0015 - val_mae: 0.0023\n",
      "Epoch 416/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0032 - val_loss: 0.0015 - val_mae: 0.0023\n",
      "Epoch 417/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0039 - val_loss: 0.0015 - val_mae: 0.0028\n",
      "Epoch 418/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0031 - val_loss: 0.0015 - val_mae: 0.0068\n",
      "Epoch 419/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0034 - val_loss: 0.0015 - val_mae: 0.0040\n",
      "Epoch 420/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0037 - val_loss: 0.0015 - val_mae: 0.0018\n",
      "Epoch 421/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0026 - val_loss: 0.0015 - val_mae: 0.0025\n",
      "Epoch 422/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0027 - val_loss: 0.0015 - val_mae: 0.0018\n",
      "Epoch 423/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0036 - val_loss: 0.0015 - val_mae: 0.0023\n",
      "Epoch 424/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0027 - val_loss: 0.0015 - val_mae: 0.0044\n",
      "Epoch 425/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0050 - val_loss: 0.0014 - val_mae: 0.0020\n",
      "Epoch 426/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0030 - val_loss: 0.0014 - val_mae: 0.0021\n",
      "Epoch 427/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0024 - val_loss: 0.0014 - val_mae: 0.0024\n",
      "Epoch 428/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0031 - val_loss: 0.0015 - val_mae: 0.0054\n",
      "Epoch 429/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0015 - mae: 0.0062 - val_loss: 0.0014 - val_mae: 0.0020\n",
      "Epoch 430/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0021 - val_loss: 0.0014 - val_mae: 0.0017\n",
      "Epoch 431/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0020 - val_loss: 0.0014 - val_mae: 0.0039\n",
      "Epoch 432/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0033 - val_loss: 0.0014 - val_mae: 0.0031\n",
      "Epoch 433/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 0.0014 - val_mae: 0.0018\n",
      "Epoch 434/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 0.0014 - val_mae: 0.0038\n",
      "Epoch 435/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0032 - val_loss: 0.0014 - val_mae: 0.0025\n",
      "Epoch 436/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0035 - val_loss: 0.0014 - val_mae: 0.0018\n",
      "Epoch 437/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0028\n",
      "Epoch 438/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0027 - val_loss: 0.0014 - val_mae: 0.0033\n",
      "Epoch 439/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0056 - val_loss: 0.0014 - val_mae: 0.0040\n",
      "Epoch 440/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 0.0014 - val_mae: 0.0045\n",
      "Epoch 441/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0030 - val_loss: 0.0013 - val_mae: 0.0015\n",
      "Epoch 442/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0027 - val_loss: 0.0013 - val_mae: 0.0022\n",
      "Epoch 443/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0027 - val_loss: 0.0013 - val_mae: 0.0033\n",
      "Epoch 444/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0014 - mae: 0.0061 - val_loss: 0.0013 - val_mae: 0.0015\n",
      "Epoch 445/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 0.0013 - val_mae: 0.0014\n",
      "Epoch 446/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0028 - val_loss: 0.0013 - val_mae: 0.0051\n",
      "Epoch 447/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0025 - val_loss: 0.0013 - val_mae: 0.0026\n",
      "Epoch 448/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0036 - val_loss: 0.0014 - val_mae: 0.0055\n",
      "Epoch 449/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0047 - val_loss: 0.0013 - val_mae: 0.0028\n",
      "Epoch 450/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0023 - val_loss: 0.0013 - val_mae: 0.0020\n",
      "Epoch 451/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0023 - val_loss: 0.0013 - val_mae: 0.0023\n",
      "Epoch 452/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 0.0013 - val_mae: 0.0038\n",
      "Epoch 453/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0033 - val_loss: 0.0013 - val_mae: 0.0065\n",
      "Epoch 454/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0027 - val_loss: 0.0013 - val_mae: 0.0044\n",
      "Epoch 455/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0028 - val_loss: 0.0013 - val_mae: 0.0031\n",
      "Epoch 456/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0034 - val_loss: 0.0013 - val_mae: 0.0023\n",
      "Epoch 457/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0027 - val_loss: 0.0012 - val_mae: 0.0021\n",
      "Epoch 458/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0026 - val_loss: 0.0012 - val_mae: 0.0020\n",
      "Epoch 459/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0063 - val_loss: 0.0012 - val_mae: 0.0020\n",
      "Epoch 460/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0027 - val_loss: 0.0013 - val_mae: 0.0042\n",
      "Epoch 461/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0013 - mae: 0.0039 - val_loss: 0.0012 - val_mae: 0.0023\n",
      "Epoch 462/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0037 - val_loss: 0.0012 - val_mae: 0.0026\n",
      "Epoch 463/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0024 - val_loss: 0.0012 - val_mae: 0.0017\n",
      "Epoch 464/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 0.0012 - val_mae: 0.0027\n",
      "Epoch 465/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0030 - val_loss: 0.0012 - val_mae: 0.0033\n",
      "Epoch 466/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0034 - val_loss: 0.0012 - val_mae: 0.0032\n",
      "Epoch 467/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0032 - val_loss: 0.0012 - val_mae: 0.0014\n",
      "Epoch 468/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0022 - val_loss: 0.0012 - val_mae: 0.0038\n",
      "Epoch 469/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0032 - val_loss: 0.0012 - val_mae: 0.0068\n",
      "Epoch 470/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0033 - val_loss: 0.0012 - val_mae: 0.0021\n",
      "Epoch 471/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0018\n",
      "Epoch 472/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0038 - val_loss: 0.0012 - val_mae: 0.0034\n",
      "Epoch 473/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0026 - val_loss: 0.0012 - val_mae: 0.0042\n",
      "Epoch 474/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0028 - val_loss: 0.0012 - val_mae: 0.0031\n",
      "Epoch 475/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0029 - val_loss: 0.0012 - val_mae: 0.0025\n",
      "Epoch 476/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0028 - val_loss: 0.0012 - val_mae: 0.0028\n",
      "Epoch 477/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0023 - val_loss: 0.0012 - val_mae: 0.0041\n",
      "Epoch 478/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0036 - val_loss: 0.0011 - val_mae: 0.0018\n",
      "Epoch 479/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0034\n",
      "Epoch 480/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0027 - val_loss: 0.0011 - val_mae: 0.0020\n",
      "Epoch 481/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0040 - val_loss: 0.0012 - val_mae: 0.0057\n",
      "Epoch 482/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0027 - val_loss: 0.0011 - val_mae: 0.0021\n",
      "Epoch 483/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0023 - val_loss: 0.0011 - val_mae: 0.0028\n",
      "Epoch 484/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0022 - val_loss: 0.0011 - val_mae: 0.0029\n",
      "Epoch 485/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0032 - val_loss: 0.0011 - val_mae: 0.0023\n",
      "Epoch 486/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0041 - val_loss: 0.0011 - val_mae: 0.0025\n",
      "Epoch 487/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0066\n",
      "Epoch 488/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0023 - val_loss: 0.0011 - val_mae: 0.0021\n",
      "Epoch 489/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0025 - val_loss: 0.0011 - val_mae: 0.0020\n",
      "Epoch 490/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0041 - val_loss: 0.0011 - val_mae: 0.0022\n",
      "Epoch 491/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0022 - val_loss: 0.0011 - val_mae: 0.0022\n",
      "Epoch 492/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0031 - val_loss: 0.0011 - val_mae: 0.0022\n",
      "Epoch 493/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0025 - val_loss: 0.0011 - val_mae: 0.0025\n",
      "Epoch 494/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0024 - val_loss: 0.0011 - val_mae: 0.0019\n",
      "Epoch 495/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0025 - val_loss: 0.0011 - val_mae: 0.0030\n",
      "Epoch 496/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0033 - val_loss: 0.0011 - val_mae: 0.0022\n",
      "Epoch 497/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0020 - val_loss: 0.0010 - val_mae: 0.0013\n",
      "Epoch 498/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0062\n",
      "Epoch 499/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0010 - mae: 0.0028 - val_loss: 0.0010 - val_mae: 0.0019\n",
      "Epoch 500/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0010 - mae: 0.0026 - val_loss: 0.0010 - val_mae: 0.0031\n",
      "Epoch 501/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0047 - val_loss: 0.0011 - val_mae: 0.0048\n",
      "Epoch 502/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0010 - mae: 0.0027 - val_loss: 0.0010 - val_mae: 0.0028\n",
      "Epoch 503/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0010 - mae: 0.0038 - val_loss: 0.0010 - val_mae: 0.0044\n",
      "Epoch 504/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0031 - val_loss: 0.0010 - val_mae: 0.0019\n",
      "Epoch 505/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0010 - mae: 0.0019 - val_loss: 0.0010 - val_mae: 0.0019\n",
      "Epoch 506/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0026 - val_loss: 0.0010 - val_mae: 0.0022\n",
      "Epoch 507/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0026 - val_loss: 0.0010 - val_mae: 0.0017\n",
      "Epoch 508/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0045 - val_loss: 9.9864e-04 - val_mae: 0.0018\n",
      "Epoch 509/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0010 - mae: 0.0034 - val_loss: 9.9704e-04 - val_mae: 0.0021\n",
      "Epoch 510/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9935e-04 - mae: 0.0028 - val_loss: 0.0010 - val_mae: 0.0036\n",
      "Epoch 511/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9597e-04 - mae: 0.0029 - val_loss: 9.8752e-04 - val_mae: 0.0019\n",
      "Epoch 512/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9593e-04 - mae: 0.0033 - val_loss: 9.9136e-04 - val_mae: 0.0028\n",
      "Epoch 513/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.9163e-04 - mae: 0.0032 - val_loss: 9.7780e-04 - val_mae: 0.0019\n",
      "Epoch 514/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 0.0010 - mae: 0.0047 - val_loss: 0.0013 - val_mae: 0.0151\n",
      "Epoch 515/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0048 - val_loss: 9.7960e-04 - val_mae: 0.0031\n",
      "Epoch 516/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.7126e-04 - mae: 0.0021 - val_loss: 9.6851e-04 - val_mae: 0.0017\n",
      "Epoch 517/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.6688e-04 - mae: 0.0019 - val_loss: 9.6481e-04 - val_mae: 0.0017\n",
      "Epoch 518/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.6682e-04 - mae: 0.0023 - val_loss: 9.6590e-04 - val_mae: 0.0023\n",
      "Epoch 519/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.6325e-04 - mae: 0.0024 - val_loss: 9.8058e-04 - val_mae: 0.0043\n",
      "Epoch 520/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.8511e-04 - mae: 0.0042 - val_loss: 9.5678e-04 - val_mae: 0.0021\n",
      "Epoch 521/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6069e-04 - mae: 0.0028 - val_loss: 9.5261e-04 - val_mae: 0.0021\n",
      "Epoch 522/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.5693e-04 - mae: 0.0028 - val_loss: 9.5696e-04 - val_mae: 0.0032\n",
      "Epoch 523/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.5058e-04 - mae: 0.0026 - val_loss: 9.4771e-04 - val_mae: 0.0023\n",
      "Epoch 524/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4578e-04 - mae: 0.0024 - val_loss: 9.6064e-04 - val_mae: 0.0041\n",
      "Epoch 525/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4272e-04 - mae: 0.0025 - val_loss: 9.4541e-04 - val_mae: 0.0033\n",
      "Epoch 526/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.4062e-04 - mae: 0.0027 - val_loss: 9.7388e-04 - val_mae: 0.0058\n",
      "Epoch 527/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.6451e-04 - mae: 0.0045 - val_loss: 9.3646e-04 - val_mae: 0.0033\n",
      "Epoch 528/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3214e-04 - mae: 0.0026 - val_loss: 9.5169e-04 - val_mae: 0.0048\n",
      "Epoch 529/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3827e-04 - mae: 0.0034 - val_loss: 9.2288e-04 - val_mae: 0.0018\n",
      "Epoch 530/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2199e-04 - mae: 0.0022 - val_loss: 9.1784e-04 - val_mae: 0.0017\n",
      "Epoch 531/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.2835e-04 - mae: 0.0030 - val_loss: 9.3415e-04 - val_mae: 0.0041\n",
      "Epoch 532/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.3510e-04 - mae: 0.0037 - val_loss: 9.4105e-04 - val_mae: 0.0051\n",
      "Epoch 533/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.7862e-04 - mae: 0.0062 - val_loss: 9.1096e-04 - val_mae: 0.0025\n",
      "Epoch 534/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.1039e-04 - mae: 0.0025 - val_loss: 9.0370e-04 - val_mae: 0.0015\n",
      "Epoch 535/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0388e-04 - mae: 0.0020 - val_loss: 9.0014e-04 - val_mae: 0.0015\n",
      "Epoch 536/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.0255e-04 - mae: 0.0022 - val_loss: 9.0610e-04 - val_mae: 0.0029\n",
      "Epoch 537/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0035e-04 - mae: 0.0024 - val_loss: 8.9652e-04 - val_mae: 0.0020\n",
      "Epoch 538/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 9.0118e-04 - mae: 0.0029 - val_loss: 9.0567e-04 - val_mae: 0.0029\n",
      "Epoch 539/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.1924e-04 - mae: 0.0043 - val_loss: 9.0137e-04 - val_mae: 0.0036\n",
      "Epoch 540/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8862e-04 - mae: 0.0022 - val_loss: 9.0070e-04 - val_mae: 0.0038\n",
      "Epoch 541/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.8743e-04 - mae: 0.0025 - val_loss: 8.8409e-04 - val_mae: 0.0022\n",
      "Epoch 542/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8349e-04 - mae: 0.0024 - val_loss: 8.7795e-04 - val_mae: 0.0017\n",
      "Epoch 543/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 9.0187e-04 - mae: 0.0041 - val_loss: 8.9567e-04 - val_mae: 0.0045\n",
      "Epoch 544/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.7548e-04 - mae: 0.0022 - val_loss: 8.7708e-04 - val_mae: 0.0027\n",
      "Epoch 545/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.8156e-04 - mae: 0.0031 - val_loss: 8.7259e-04 - val_mae: 0.0026\n",
      "Epoch 546/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 8.7095e-04 - mae: 0.0025 - val_loss: 8.6498e-04 - val_mae: 0.0018\n",
      "Epoch 547/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.6540e-04 - mae: 0.0022 - val_loss: 8.5992e-04 - val_mae: 0.0016\n",
      "Epoch 548/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6903e-04 - mae: 0.0030 - val_loss: 8.5741e-04 - val_mae: 0.0018\n",
      "Epoch 549/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.6275e-04 - mae: 0.0028 - val_loss: 8.5624e-04 - val_mae: 0.0020\n",
      "Epoch 550/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.5169e-04 - mae: 0.0018 - val_loss: 8.6293e-04 - val_mae: 0.0033\n",
      "Epoch 551/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.6382e-04 - mae: 0.0034 - val_loss: 8.6266e-04 - val_mae: 0.0038\n",
      "Epoch 552/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.5197e-04 - mae: 0.0028 - val_loss: 8.4565e-04 - val_mae: 0.0021\n",
      "Epoch 553/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.5842e-04 - mae: 0.0036 - val_loss: 8.5450e-04 - val_mae: 0.0039\n",
      "Epoch 554/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.6576e-04 - mae: 0.0044 - val_loss: 8.3991e-04 - val_mae: 0.0024\n",
      "Epoch 555/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3662e-04 - mae: 0.0021 - val_loss: 8.3251e-04 - val_mae: 0.0015\n",
      "Epoch 556/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3648e-04 - mae: 0.0024 - val_loss: 8.3185e-04 - val_mae: 0.0019\n",
      "Epoch 557/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.3364e-04 - mae: 0.0026 - val_loss: 8.3271e-04 - val_mae: 0.0029\n",
      "Epoch 558/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.3865e-04 - mae: 0.0033 - val_loss: 8.2986e-04 - val_mae: 0.0027\n",
      "Epoch 559/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.7622e-04 - mae: 0.0055 - val_loss: 8.3684e-04 - val_mae: 0.0036\n",
      "Epoch 560/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.2842e-04 - mae: 0.0029 - val_loss: 8.2590e-04 - val_mae: 0.0030\n",
      "Epoch 561/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.1928e-04 - mae: 0.0022 - val_loss: 8.1605e-04 - val_mae: 0.0018\n",
      "Epoch 562/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1681e-04 - mae: 0.0023 - val_loss: 8.2607e-04 - val_mae: 0.0035\n",
      "Epoch 563/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.4822e-04 - mae: 0.0044 - val_loss: 8.1663e-04 - val_mae: 0.0024\n",
      "Epoch 564/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.1044e-04 - mae: 0.0021 - val_loss: 8.0584e-04 - val_mae: 0.0014\n",
      "Epoch 565/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.0866e-04 - mae: 0.0022 - val_loss: 8.0317e-04 - val_mae: 0.0014\n",
      "Epoch 566/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.1149e-04 - mae: 0.0029 - val_loss: 8.3356e-04 - val_mae: 0.0052\n",
      "Epoch 567/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0889e-04 - mae: 0.0027 - val_loss: 7.9901e-04 - val_mae: 0.0019\n",
      "Epoch 568/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0089e-04 - mae: 0.0023 - val_loss: 8.0418e-04 - val_mae: 0.0032\n",
      "Epoch 569/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 8.0814e-04 - mae: 0.0033 - val_loss: 8.1111e-04 - val_mae: 0.0033\n",
      "Epoch 570/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.1169e-04 - mae: 0.0038 - val_loss: 8.1785e-04 - val_mae: 0.0049\n",
      "Epoch 571/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0799e-04 - mae: 0.0038 - val_loss: 7.8871e-04 - val_mae: 0.0020\n",
      "Epoch 572/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8850e-04 - mae: 0.0023 - val_loss: 7.8353e-04 - val_mae: 0.0015\n",
      "Epoch 573/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.8394e-04 - mae: 0.0020 - val_loss: 7.8167e-04 - val_mae: 0.0017\n",
      "Epoch 574/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.8519e-04 - mae: 0.0025 - val_loss: 7.8113e-04 - val_mae: 0.0021\n",
      "Epoch 575/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8238e-04 - mae: 0.0025 - val_loss: 7.7751e-04 - val_mae: 0.0023\n",
      "Epoch 576/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.8160e-04 - mae: 0.0028 - val_loss: 7.7474e-04 - val_mae: 0.0021\n",
      "Epoch 577/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7391e-04 - mae: 0.0023 - val_loss: 7.7275e-04 - val_mae: 0.0022\n",
      "Epoch 578/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.9732e-04 - mae: 0.0044 - val_loss: 7.7285e-04 - val_mae: 0.0027\n",
      "Epoch 579/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.7966e-04 - mae: 0.0035 - val_loss: 7.6754e-04 - val_mae: 0.0023\n",
      "Epoch 580/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6612e-04 - mae: 0.0023 - val_loss: 7.6635e-04 - val_mae: 0.0027\n",
      "Epoch 581/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6055e-04 - mae: 0.0020 - val_loss: 7.5803e-04 - val_mae: 0.0018\n",
      "Epoch 582/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6129e-04 - mae: 0.0025 - val_loss: 7.5620e-04 - val_mae: 0.0018\n",
      "Epoch 583/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6953e-04 - mae: 0.0035 - val_loss: 7.7763e-04 - val_mae: 0.0040\n",
      "Epoch 584/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 7.5921e-04 - mae: 0.0029 - val_loss: 8.1805e-04 - val_mae: 0.0070\n",
      "Epoch 585/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 8.0597e-04 - mae: 0.0059 - val_loss: 7.5372e-04 - val_mae: 0.0025\n",
      "Epoch 586/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.6109e-04 - mae: 0.0033 - val_loss: 7.4404e-04 - val_mae: 0.0016\n",
      "Epoch 587/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4573e-04 - mae: 0.0022 - val_loss: 7.4332e-04 - val_mae: 0.0019\n",
      "Epoch 588/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4179e-04 - mae: 0.0020 - val_loss: 7.4106e-04 - val_mae: 0.0020\n",
      "Epoch 589/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4198e-04 - mae: 0.0024 - val_loss: 7.4107e-04 - val_mae: 0.0026\n",
      "Epoch 590/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3993e-04 - mae: 0.0023 - val_loss: 7.5292e-04 - val_mae: 0.0041\n",
      "Epoch 591/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.4656e-04 - mae: 0.0032 - val_loss: 7.6184e-04 - val_mae: 0.0055\n",
      "Epoch 592/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.4170e-04 - mae: 0.0032 - val_loss: 7.2993e-04 - val_mae: 0.0019\n",
      "Epoch 593/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3273e-04 - mae: 0.0024 - val_loss: 7.3826e-04 - val_mae: 0.0035\n",
      "Epoch 594/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.3661e-04 - mae: 0.0031 - val_loss: 7.2916e-04 - val_mae: 0.0026\n",
      "Epoch 595/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.2491e-04 - mae: 0.0021 - val_loss: 7.2316e-04 - val_mae: 0.0022\n",
      "Epoch 596/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2276e-04 - mae: 0.0022 - val_loss: 7.1845e-04 - val_mae: 0.0016\n",
      "Epoch 597/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2234e-04 - mae: 0.0025 - val_loss: 7.2143e-04 - val_mae: 0.0028\n",
      "Epoch 598/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2997e-04 - mae: 0.0034 - val_loss: 7.2214e-04 - val_mae: 0.0030\n",
      "Epoch 599/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1457e-04 - mae: 0.0022 - val_loss: 7.2364e-04 - val_mae: 0.0031\n",
      "Epoch 600/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2099e-04 - mae: 0.0032 - val_loss: 7.1113e-04 - val_mae: 0.0022\n",
      "Epoch 601/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.1038e-04 - mae: 0.0023 - val_loss: 7.1614e-04 - val_mae: 0.0034\n",
      "Epoch 602/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.2413e-04 - mae: 0.0037 - val_loss: 7.0407e-04 - val_mae: 0.0019\n",
      "Epoch 603/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 7.2291e-04 - mae: 0.0039 - val_loss: 7.2697e-04 - val_mae: 0.0040\n",
      "Epoch 604/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9947e-04 - mae: 0.0019 - val_loss: 6.9606e-04 - val_mae: 0.0012\n",
      "Epoch 605/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.0044e-04 - mae: 0.0024 - val_loss: 6.9521e-04 - val_mae: 0.0018\n",
      "Epoch 606/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9972e-04 - mae: 0.0026 - val_loss: 6.9441e-04 - val_mae: 0.0022\n",
      "Epoch 607/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 7.9706e-04 - mae: 0.0069 - val_loss: 7.2083e-04 - val_mae: 0.0052\n",
      "Epoch 608/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.9121e-04 - mae: 0.0020 - val_loss: 6.8848e-04 - val_mae: 0.0016\n",
      "Epoch 609/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8984e-04 - mae: 0.0021 - val_loss: 6.9080e-04 - val_mae: 0.0024\n",
      "Epoch 610/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8762e-04 - mae: 0.0021 - val_loss: 6.8400e-04 - val_mae: 0.0014\n",
      "Epoch 611/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8342e-04 - mae: 0.0018 - val_loss: 6.8467e-04 - val_mae: 0.0018\n",
      "Epoch 612/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8246e-04 - mae: 0.0020 - val_loss: 6.8056e-04 - val_mae: 0.0016\n",
      "Epoch 613/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8157e-04 - mae: 0.0022 - val_loss: 6.7946e-04 - val_mae: 0.0019\n",
      "Epoch 614/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8602e-04 - mae: 0.0028 - val_loss: 6.9917e-04 - val_mae: 0.0040\n",
      "Epoch 615/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7923e-04 - mae: 0.0025 - val_loss: 6.7411e-04 - val_mae: 0.0020\n",
      "Epoch 616/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.8053e-04 - mae: 0.0029 - val_loss: 6.7274e-04 - val_mae: 0.0019\n",
      "Epoch 617/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7860e-04 - mae: 0.0029 - val_loss: 6.6832e-04 - val_mae: 0.0018\n",
      "Epoch 618/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.7923e-04 - mae: 0.0032 - val_loss: 6.7289e-04 - val_mae: 0.0030\n",
      "Epoch 619/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.6949e-04 - mae: 0.0024 - val_loss: 6.6712e-04 - val_mae: 0.0025\n",
      "Epoch 620/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.6810e-04 - mae: 0.0025 - val_loss: 6.6336e-04 - val_mae: 0.0020\n",
      "Epoch 621/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.6657e-04 - mae: 0.0025 - val_loss: 6.7198e-04 - val_mae: 0.0038\n",
      "Epoch 622/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.6728e-04 - mae: 0.0029 - val_loss: 6.6112e-04 - val_mae: 0.0026\n",
      "Epoch 623/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.6121e-04 - mae: 0.0026 - val_loss: 6.6200e-04 - val_mae: 0.0030\n",
      "Epoch 624/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.5911e-04 - mae: 0.0026 - val_loss: 6.5447e-04 - val_mae: 0.0022\n",
      "Epoch 625/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.5715e-04 - mae: 0.0027 - val_loss: 6.5977e-04 - val_mae: 0.0033\n",
      "Epoch 626/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.5685e-04 - mae: 0.0029 - val_loss: 6.4812e-04 - val_mae: 0.0021\n",
      "Epoch 627/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.5874e-04 - mae: 0.0031 - val_loss: 6.5197e-04 - val_mae: 0.0026\n",
      "Epoch 628/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4871e-04 - mae: 0.0026 - val_loss: 6.4594e-04 - val_mae: 0.0026\n",
      "Epoch 629/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.4757e-04 - mae: 0.0027 - val_loss: 6.4264e-04 - val_mae: 0.0021\n",
      "Epoch 630/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.4218e-04 - mae: 0.0023 - val_loss: 6.4187e-04 - val_mae: 0.0024\n",
      "Epoch 631/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.5399e-04 - mae: 0.0034 - val_loss: 6.3629e-04 - val_mae: 0.0019\n",
      "Epoch 632/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.3750e-04 - mae: 0.0022 - val_loss: 6.3660e-04 - val_mae: 0.0023\n",
      "Epoch 633/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.3643e-04 - mae: 0.0025 - val_loss: 6.4514e-04 - val_mae: 0.0037\n",
      "Epoch 634/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.5307e-04 - mae: 0.0040 - val_loss: 6.3914e-04 - val_mae: 0.0031\n",
      "Epoch 635/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.3104e-04 - mae: 0.0022 - val_loss: 6.2966e-04 - val_mae: 0.0019\n",
      "Epoch 636/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3528e-04 - mae: 0.0030 - val_loss: 6.2527e-04 - val_mae: 0.0019\n",
      "Epoch 637/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3439e-04 - mae: 0.0031 - val_loss: 6.2956e-04 - val_mae: 0.0027\n",
      "Epoch 638/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2962e-04 - mae: 0.0027 - val_loss: 6.5406e-04 - val_mae: 0.0059\n",
      "Epoch 639/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.3274e-04 - mae: 0.0032 - val_loss: 6.2561e-04 - val_mae: 0.0028\n",
      "Epoch 640/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.3329e-04 - mae: 0.0035 - val_loss: 6.2067e-04 - val_mae: 0.0024\n",
      "Epoch 641/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1714e-04 - mae: 0.0020 - val_loss: 6.2053e-04 - val_mae: 0.0027\n",
      "Epoch 642/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.2273e-04 - mae: 0.0029 - val_loss: 6.1687e-04 - val_mae: 0.0026\n",
      "Epoch 643/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1589e-04 - mae: 0.0025 - val_loss: 6.1832e-04 - val_mae: 0.0029\n",
      "Epoch 644/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2177e-04 - mae: 0.0031 - val_loss: 6.2114e-04 - val_mae: 0.0033\n",
      "Epoch 645/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.2068e-04 - mae: 0.0034 - val_loss: 6.3577e-04 - val_mae: 0.0044\n",
      "Epoch 646/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 6.1154e-04 - mae: 0.0026 - val_loss: 6.0452e-04 - val_mae: 0.0017\n",
      "Epoch 647/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1191e-04 - mae: 0.0028 - val_loss: 6.2435e-04 - val_mae: 0.0044\n",
      "Epoch 648/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.0749e-04 - mae: 0.0025 - val_loss: 6.0419e-04 - val_mae: 0.0022\n",
      "Epoch 649/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1131e-04 - mae: 0.0032 - val_loss: 6.0168e-04 - val_mae: 0.0024\n",
      "Epoch 650/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.9969e-04 - mae: 0.0021 - val_loss: 5.9746e-04 - val_mae: 0.0018\n",
      "Epoch 651/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.0323e-04 - mae: 0.0027 - val_loss: 5.9512e-04 - val_mae: 0.0019\n",
      "Epoch 652/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.1299e-04 - mae: 0.0037 - val_loss: 5.9472e-04 - val_mae: 0.0019\n",
      "Epoch 653/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.9271e-04 - mae: 0.0020 - val_loss: 5.9049e-04 - val_mae: 0.0017\n",
      "Epoch 654/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.9740e-04 - mae: 0.0027 - val_loss: 5.8852e-04 - val_mae: 0.0016\n",
      "Epoch 655/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 6.0088e-04 - mae: 0.0031 - val_loss: 5.9818e-04 - val_mae: 0.0033\n",
      "Epoch 656/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9735e-04 - mae: 0.0031 - val_loss: 5.9339e-04 - val_mae: 0.0029\n",
      "Epoch 657/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8570e-04 - mae: 0.0020 - val_loss: 5.8238e-04 - val_mae: 0.0014\n",
      "Epoch 658/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9007e-04 - mae: 0.0027 - val_loss: 5.9683e-04 - val_mae: 0.0034\n",
      "Epoch 659/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.8890e-04 - mae: 0.0028 - val_loss: 5.8676e-04 - val_mae: 0.0026\n",
      "Epoch 660/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.8943e-04 - mae: 0.0029 - val_loss: 5.8215e-04 - val_mae: 0.0023\n",
      "Epoch 661/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.8045e-04 - mae: 0.0023 - val_loss: 5.8678e-04 - val_mae: 0.0032\n",
      "Epoch 662/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.9550e-04 - mae: 0.0039 - val_loss: 5.7687e-04 - val_mae: 0.0024\n",
      "Epoch 663/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7736e-04 - mae: 0.0024 - val_loss: 5.7721e-04 - val_mae: 0.0026\n",
      "Epoch 664/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8539e-04 - mae: 0.0034 - val_loss: 5.7937e-04 - val_mae: 0.0028\n",
      "Epoch 665/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7120e-04 - mae: 0.0020 - val_loss: 5.8886e-04 - val_mae: 0.0041\n",
      "Epoch 666/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7692e-04 - mae: 0.0029 - val_loss: 6.5818e-04 - val_mae: 0.0083\n",
      "Epoch 667/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8063e-04 - mae: 0.0031 - val_loss: 5.6782e-04 - val_mae: 0.0022\n",
      "Epoch 668/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.6517e-04 - mae: 0.0020 - val_loss: 5.6146e-04 - val_mae: 0.0014\n",
      "Epoch 669/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.6686e-04 - mae: 0.0025 - val_loss: 5.7219e-04 - val_mae: 0.0029\n",
      "Epoch 670/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.7245e-04 - mae: 0.0031 - val_loss: 5.6629e-04 - val_mae: 0.0029\n",
      "Epoch 671/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.6798e-04 - mae: 0.0027 - val_loss: 5.7634e-04 - val_mae: 0.0039\n",
      "Epoch 672/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.6174e-04 - mae: 0.0025 - val_loss: 5.5951e-04 - val_mae: 0.0022\n",
      "Epoch 673/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.6478e-04 - mae: 0.0028 - val_loss: 5.5447e-04 - val_mae: 0.0018\n",
      "Epoch 674/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.6512e-04 - mae: 0.0029 - val_loss: 5.5605e-04 - val_mae: 0.0020\n",
      "Epoch 675/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5360e-04 - mae: 0.0021 - val_loss: 5.5436e-04 - val_mae: 0.0024\n",
      "Epoch 676/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5707e-04 - mae: 0.0027 - val_loss: 5.5648e-04 - val_mae: 0.0029\n",
      "Epoch 677/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5289e-04 - mae: 0.0024 - val_loss: 5.4936e-04 - val_mae: 0.0022\n",
      "Epoch 678/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.5055e-04 - mae: 0.0024 - val_loss: 5.9242e-04 - val_mae: 0.0061\n",
      "Epoch 679/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.8619e-04 - mae: 0.0052 - val_loss: 5.6461e-04 - val_mae: 0.0040\n",
      "Epoch 680/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.4778e-04 - mae: 0.0024 - val_loss: 5.4866e-04 - val_mae: 0.0027\n",
      "Epoch 681/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4342e-04 - mae: 0.0021 - val_loss: 5.4125e-04 - val_mae: 0.0019\n",
      "Epoch 682/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4546e-04 - mae: 0.0026 - val_loss: 5.3768e-04 - val_mae: 0.0014\n",
      "Epoch 683/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4290e-04 - mae: 0.0024 - val_loss: 5.5968e-04 - val_mae: 0.0043\n",
      "Epoch 684/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4458e-04 - mae: 0.0027 - val_loss: 5.4872e-04 - val_mae: 0.0034\n",
      "Epoch 685/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.3870e-04 - mae: 0.0023 - val_loss: 5.3982e-04 - val_mae: 0.0026\n",
      "Epoch 686/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4524e-04 - mae: 0.0031 - val_loss: 5.3160e-04 - val_mae: 0.0015\n",
      "Epoch 687/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4153e-04 - mae: 0.0028 - val_loss: 5.3574e-04 - val_mae: 0.0023\n",
      "Epoch 688/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4625e-04 - mae: 0.0035 - val_loss: 5.2887e-04 - val_mae: 0.0016\n",
      "Epoch 689/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.3703e-04 - mae: 0.0027 - val_loss: 5.2918e-04 - val_mae: 0.0018\n",
      "Epoch 690/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.4950e-04 - mae: 0.0037 - val_loss: 5.2486e-04 - val_mae: 0.0013\n",
      "Epoch 691/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.3369e-04 - mae: 0.0028 - val_loss: 5.3481e-04 - val_mae: 0.0032\n",
      "Epoch 692/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.2966e-04 - mae: 0.0025 - val_loss: 5.2333e-04 - val_mae: 0.0015\n",
      "Epoch 693/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.2448e-04 - mae: 0.0021 - val_loss: 5.2144e-04 - val_mae: 0.0016\n",
      "Epoch 694/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.2722e-04 - mae: 0.0026 - val_loss: 5.3107e-04 - val_mae: 0.0034\n",
      "Epoch 695/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.2848e-04 - mae: 0.0029 - val_loss: 5.2887e-04 - val_mae: 0.0028\n",
      "Epoch 696/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.2551e-04 - mae: 0.0026 - val_loss: 5.2103e-04 - val_mae: 0.0025\n",
      "Epoch 697/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.1962e-04 - mae: 0.0022 - val_loss: 5.1595e-04 - val_mae: 0.0018\n",
      "Epoch 698/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.2456e-04 - mae: 0.0028 - val_loss: 5.1286e-04 - val_mae: 0.0014\n",
      "Epoch 699/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.2061e-04 - mae: 0.0027 - val_loss: 5.1243e-04 - val_mae: 0.0017\n",
      "Epoch 700/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.1714e-04 - mae: 0.0024 - val_loss: 5.3113e-04 - val_mae: 0.0039\n",
      "Epoch 701/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.1740e-04 - mae: 0.0028 - val_loss: 5.1774e-04 - val_mae: 0.0030\n",
      "Epoch 702/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.2014e-04 - mae: 0.0032 - val_loss: 5.0736e-04 - val_mae: 0.0016\n",
      "Epoch 703/1000\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 5.1074e-04 - mae: 0.0023 - val_loss: 5.0962e-04 - val_mae: 0.0020\n",
      "Epoch 704/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.1624e-04 - mae: 0.0030 - val_loss: 5.3201e-04 - val_mae: 0.0049\n",
      "Epoch 705/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.2271e-04 - mae: 0.0035 - val_loss: 5.7928e-04 - val_mae: 0.0074\n",
      "Epoch 706/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.1060e-04 - mae: 0.0026 - val_loss: 5.0696e-04 - val_mae: 0.0024\n",
      "Epoch 707/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0404e-04 - mae: 0.0021 - val_loss: 5.0921e-04 - val_mae: 0.0031\n",
      "Epoch 708/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.1040e-04 - mae: 0.0030 - val_loss: 5.0133e-04 - val_mae: 0.0021\n",
      "Epoch 709/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0223e-04 - mae: 0.0023 - val_loss: 5.0188e-04 - val_mae: 0.0022\n",
      "Epoch 710/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0758e-04 - mae: 0.0030 - val_loss: 5.0020e-04 - val_mae: 0.0025\n",
      "Epoch 711/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9784e-04 - mae: 0.0021 - val_loss: 4.9675e-04 - val_mae: 0.0021\n",
      "Epoch 712/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0585e-04 - mae: 0.0031 - val_loss: 4.9503e-04 - val_mae: 0.0019\n",
      "Epoch 713/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0222e-04 - mae: 0.0028 - val_loss: 6.4656e-04 - val_mae: 0.0113\n",
      "Epoch 714/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0299e-04 - mae: 0.0030 - val_loss: 4.8994e-04 - val_mae: 0.0016\n",
      "Epoch 715/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9580e-04 - mae: 0.0026 - val_loss: 5.0418e-04 - val_mae: 0.0034\n",
      "Epoch 716/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9782e-04 - mae: 0.0029 - val_loss: 4.8671e-04 - val_mae: 0.0016\n",
      "Epoch 717/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0968e-04 - mae: 0.0038 - val_loss: 4.9182e-04 - val_mae: 0.0024\n",
      "Epoch 718/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.9186e-04 - mae: 0.0025 - val_loss: 4.8946e-04 - val_mae: 0.0025\n",
      "Epoch 719/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8772e-04 - mae: 0.0022 - val_loss: 4.9105e-04 - val_mae: 0.0027\n",
      "Epoch 720/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8849e-04 - mae: 0.0025 - val_loss: 4.8389e-04 - val_mae: 0.0018\n",
      "Epoch 721/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.8935e-04 - mae: 0.0027 - val_loss: 4.8861e-04 - val_mae: 0.0025\n",
      "Epoch 722/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9455e-04 - mae: 0.0032 - val_loss: 4.9577e-04 - val_mae: 0.0039\n",
      "Epoch 723/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9325e-04 - mae: 0.0033 - val_loss: 4.7911e-04 - val_mae: 0.0018\n",
      "Epoch 724/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.8087e-04 - mae: 0.0021 - val_loss: 4.7816e-04 - val_mae: 0.0018\n",
      "Epoch 725/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.8326e-04 - mae: 0.0025 - val_loss: 4.9422e-04 - val_mae: 0.0034\n",
      "Epoch 726/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 5.0256e-04 - mae: 0.0041 - val_loss: 4.7371e-04 - val_mae: 0.0014\n",
      "Epoch 727/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.8214e-04 - mae: 0.0027 - val_loss: 4.8605e-04 - val_mae: 0.0033\n",
      "Epoch 728/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 5.0517e-04 - mae: 0.0044 - val_loss: 4.7929e-04 - val_mae: 0.0029\n",
      "Epoch 729/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.7619e-04 - mae: 0.0023 - val_loss: 4.7305e-04 - val_mae: 0.0019\n",
      "Epoch 730/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.7289e-04 - mae: 0.0020 - val_loss: 4.9565e-04 - val_mae: 0.0043\n",
      "Epoch 731/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.7373e-04 - mae: 0.0023 - val_loss: 4.7246e-04 - val_mae: 0.0020\n",
      "Epoch 732/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.7298e-04 - mae: 0.0024 - val_loss: 4.7484e-04 - val_mae: 0.0028\n",
      "Epoch 733/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.7530e-04 - mae: 0.0027 - val_loss: 4.6618e-04 - val_mae: 0.0015\n",
      "Epoch 734/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9567e-04 - mae: 0.0042 - val_loss: 5.0713e-04 - val_mae: 0.0061\n",
      "Epoch 735/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.7171e-04 - mae: 0.0025 - val_loss: 4.7044e-04 - val_mae: 0.0027\n",
      "Epoch 736/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.6892e-04 - mae: 0.0024 - val_loss: 4.6533e-04 - val_mae: 0.0022\n",
      "Epoch 737/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.6476e-04 - mae: 0.0020 - val_loss: 4.6881e-04 - val_mae: 0.0025\n",
      "Epoch 738/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.7054e-04 - mae: 0.0028 - val_loss: 4.6358e-04 - val_mae: 0.0021\n",
      "Epoch 739/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.6543e-04 - mae: 0.0024 - val_loss: 4.6025e-04 - val_mae: 0.0018\n",
      "Epoch 740/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.6189e-04 - mae: 0.0021 - val_loss: 4.5795e-04 - val_mae: 0.0015\n",
      "Epoch 741/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.6898e-04 - mae: 0.0031 - val_loss: 4.6451e-04 - val_mae: 0.0027\n",
      "Epoch 742/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.6779e-04 - mae: 0.0030 - val_loss: 4.7482e-04 - val_mae: 0.0040\n",
      "Epoch 743/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.6720e-04 - mae: 0.0031 - val_loss: 4.5718e-04 - val_mae: 0.0019\n",
      "Epoch 744/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.6787e-04 - mae: 0.0032 - val_loss: 4.6050e-04 - val_mae: 0.0026\n",
      "Epoch 745/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.7372e-04 - mae: 0.0035 - val_loss: 4.5828e-04 - val_mae: 0.0026\n",
      "Epoch 746/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.5841e-04 - mae: 0.0024 - val_loss: 4.5298e-04 - val_mae: 0.0020\n",
      "Epoch 747/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.5371e-04 - mae: 0.0021 - val_loss: 4.5330e-04 - val_mae: 0.0022\n",
      "Epoch 748/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.5218e-04 - mae: 0.0021 - val_loss: 4.5170e-04 - val_mae: 0.0022\n",
      "Epoch 749/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.5747e-04 - mae: 0.0028 - val_loss: 4.5507e-04 - val_mae: 0.0027\n",
      "Epoch 750/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.5720e-04 - mae: 0.0028 - val_loss: 4.5760e-04 - val_mae: 0.0031\n",
      "Epoch 751/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.6063e-04 - mae: 0.0033 - val_loss: 4.4809e-04 - val_mae: 0.0021\n",
      "Epoch 752/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.4993e-04 - mae: 0.0023 - val_loss: 4.4621e-04 - val_mae: 0.0020\n",
      "Epoch 753/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.5730e-04 - mae: 0.0031 - val_loss: 4.4793e-04 - val_mae: 0.0024\n",
      "Epoch 754/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.4863e-04 - mae: 0.0025 - val_loss: 4.4434e-04 - val_mae: 0.0019\n",
      "Epoch 755/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.4585e-04 - mae: 0.0022 - val_loss: 4.5171e-04 - val_mae: 0.0028\n",
      "Epoch 756/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.5051e-04 - mae: 0.0029 - val_loss: 4.4177e-04 - val_mae: 0.0021\n",
      "Epoch 757/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.4592e-04 - mae: 0.0025 - val_loss: 4.4013e-04 - val_mae: 0.0018\n",
      "Epoch 758/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.4352e-04 - mae: 0.0023 - val_loss: 4.5160e-04 - val_mae: 0.0033\n",
      "Epoch 759/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.7524e-04 - mae: 0.0044 - val_loss: 4.4421e-04 - val_mae: 0.0028\n",
      "Epoch 760/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.4430e-04 - mae: 0.0026 - val_loss: 4.3622e-04 - val_mae: 0.0016\n",
      "Epoch 761/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.3973e-04 - mae: 0.0022 - val_loss: 4.3677e-04 - val_mae: 0.0018\n",
      "Epoch 762/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.3747e-04 - mae: 0.0021 - val_loss: 4.3559e-04 - val_mae: 0.0021\n",
      "Epoch 763/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.3928e-04 - mae: 0.0025 - val_loss: 4.6706e-04 - val_mae: 0.0052\n",
      "Epoch 764/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.9842e-04 - mae: 0.0059 - val_loss: 4.3292e-04 - val_mae: 0.0017\n",
      "Epoch 765/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.3605e-04 - mae: 0.0023 - val_loss: 4.5082e-04 - val_mae: 0.0044\n",
      "Epoch 766/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.3615e-04 - mae: 0.0024 - val_loss: 4.3121e-04 - val_mae: 0.0017\n",
      "Epoch 767/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.3229e-04 - mae: 0.0020 - val_loss: 4.2967e-04 - val_mae: 0.0016\n",
      "Epoch 768/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.3828e-04 - mae: 0.0028 - val_loss: 4.3221e-04 - val_mae: 0.0019\n",
      "Epoch 769/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.3264e-04 - mae: 0.0023 - val_loss: 4.2870e-04 - val_mae: 0.0017\n",
      "Epoch 770/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.3690e-04 - mae: 0.0027 - val_loss: 4.3353e-04 - val_mae: 0.0022\n",
      "Epoch 771/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.2828e-04 - mae: 0.0020 - val_loss: 4.2489e-04 - val_mae: 0.0016\n",
      "Epoch 772/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.2966e-04 - mae: 0.0023 - val_loss: 4.3247e-04 - val_mae: 0.0026\n",
      "Epoch 773/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.3126e-04 - mae: 0.0027 - val_loss: 4.2296e-04 - val_mae: 0.0015\n",
      "Epoch 774/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.2444e-04 - mae: 0.0018 - val_loss: 4.2545e-04 - val_mae: 0.0021\n",
      "Epoch 775/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.2341e-04 - mae: 0.0019 - val_loss: 4.2707e-04 - val_mae: 0.0028\n",
      "Epoch 776/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.2865e-04 - mae: 0.0027 - val_loss: 4.4533e-04 - val_mae: 0.0038\n",
      "Epoch 777/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.5294e-04 - mae: 0.0041 - val_loss: 4.1963e-04 - val_mae: 0.0016\n",
      "Epoch 778/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.2207e-04 - mae: 0.0021 - val_loss: 4.3061e-04 - val_mae: 0.0031\n",
      "Epoch 779/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.2847e-04 - mae: 0.0029 - val_loss: 4.3409e-04 - val_mae: 0.0039\n",
      "Epoch 780/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.2240e-04 - mae: 0.0025 - val_loss: 4.1636e-04 - val_mae: 0.0015\n",
      "Epoch 781/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.2021e-04 - mae: 0.0022 - val_loss: 4.1574e-04 - val_mae: 0.0014\n",
      "Epoch 782/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.2449e-04 - mae: 0.0028 - val_loss: 4.2334e-04 - val_mae: 0.0031\n",
      "Epoch 783/1000\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.2567e-04 - mae: 0.0030 - val_loss: 4.2183e-04 - val_mae: 0.0030\n",
      "Epoch 784/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.1902e-04 - mae: 0.0025 - val_loss: 4.2482e-04 - val_mae: 0.0032\n",
      "Epoch 785/1000\n",
      "163/163 [==============================] - 0s 2ms/step - loss: 4.1883e-04 - mae: 0.0026 - val_loss: 4.4367e-04 - val_mae: 0.0055\n",
      "Epoch 786/1000\n",
      "146/163 [=========================>....] - ETA: 0s - loss: 4.1696e-04 - mae: 0.0024Restoring model weights from the end of the best epoch: 781.\n",
      "163/163 [==============================] - 0s 3ms/step - loss: 4.1651e-04 - mae: 0.0024 - val_loss: 4.2957e-04 - val_mae: 0.0042\n",
      "Epoch 786: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "\n",
    "    Dense(248, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(296, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(280, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(216, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(184, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(40, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(312, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(200, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(104, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 25\n",
    "epochs = 1000\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "model.save('D3_1.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T21:52:53.854863400Z",
     "start_time": "2024-04-06T21:47:18.836174500Z"
    }
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialisiere Listen, um Ergebnisse zu speichern\n",
    "val_loss_results = []\n",
    "val_mae_results = []\n",
    "\n",
    "# Funktion, um das Modell zu erstellen\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "            Dense(248, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "        \n",
    "            Dense(296, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(280, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(216, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(184, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(40, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(312, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(200, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(104, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(1 , activation = 'linear')\n",
    "\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-Validation Konfiguration\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Leistungsüberwachung\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_scaled[train_index], y_train_scaled[val_index]\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "    print(f'Training für Fold {fold_no}...')\n",
    "    history = model.fit(X_train_fold, y_train_fold, batch_size=25, epochs=1000, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    # Speichere die Ergebnisse des aktuellen Folds\n",
    "    val_loss_results.append(min(history.history['val_loss']))\n",
    "    val_mae_results.append(min(history.history['val_mae']))\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "# Berechne den Durchschnitt über alle Folds\n",
    "average_val_loss = np.mean(val_loss_results)\n",
    "average_val_mae = np.mean(val_mae_results)\n",
    "\n",
    "# Gib die durchschnittlichen Ergebnisse aus\n",
    "print(f'Durchschnittlicher Validation Loss: {average_val_loss}')\n",
    "print(f'Durchschnittlicher Validation MAE: {average_val_mae}')\n",
    "\n",
    "# Umwandeln der Listen in Pandas DataFrames\n",
    "val_loss_df = pd.DataFrame(val_loss_results, columns=['Validation Loss'])\n",
    "val_mae_df = pd.DataFrame(val_mae_results, columns=['Validation MAE'])\n",
    "\n",
    "# Speichern der DataFrames in CSV-Dateien\n",
    "val_loss_df.to_csv('val_loss_results_D3_1.csv', index=False)\n",
    "val_mae_df.to_csv('val_mae_results_D3_1.csv', index=False)\n",
    "\n",
    "# Gib die durchschnittlichen Ergebnisse aus\n",
    "print(f'Durchschnittlicher Validation Loss: {average_val_loss}')\n",
    "print(f'Durchschnittlicher Validation MAE: {average_val_mae}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T19:20:01.477738600Z",
     "start_time": "2024-04-06T19:20:01.475739Z"
    }
   },
   "id": "7ff728388210d66b"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 - 0s - loss: 4.1657e-04 - mae: 0.0015 - 89ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.0004165704012848437, 0.0015293625183403492]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modellevalierung\n",
    "results = model.evaluate(X_test_scaled, y_test_scaled, verbose=2)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T22:30:17.238593900Z",
     "start_time": "2024-04-06T22:30:17.098742600Z"
    }
   },
   "id": "9050ab66ab36f169"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bsp. Predicted: [1463.8202] Actual: [1463.6] \n",
      "Durchschnittliche Abweichung (MAE): [1.44688342]\n",
      "0.1641199614798558\n"
     ]
    }
   ],
   "source": [
    "#Rückrechnung des skalierten MAE zum unskalierten MAE für eines bessere Einschätzung des Ergebnisses\n",
    "\n",
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Führen Sie die Rücktransformation der skalierten Werte durch\n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[100]} Actual: {original_actual_values[100]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "\n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "\n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")\n",
    "\n",
    "# Berechnung desd MAPE\n",
    "errors = np.abs((original_actual_values - original_predicted_values) / original_actual_values)\n",
    "mape = np.mean(errors) * 100\n",
    "print(mape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T22:30:18.301053700Z",
     "start_time": "2024-04-06T22:30:18.046514700Z"
    }
   },
   "id": "10cd79ca028075dd"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 17\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m r_squared\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Berechnung von R^2 mit den bereitgestellten Listen\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m r_squared \u001B[38;5;241m=\u001B[39m calculate_r_squared(\u001B[43mlist1\u001B[49m, list2)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mR^2: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mr_squared\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'list1' is not defined"
     ]
    }
   ],
   "source": [
    "# Berechnung der Auswertungsgröße R^2\n",
    "def calculate_r_squared(predicted, actual):\n",
    "    # Berechnung des Mittelwerts der tatsächlichen Werte\n",
    "    mean_actual = sum(actual) / len(actual)\n",
    "    \n",
    "    # Berechnung der totalen Summe der Quadrate (SST)\n",
    "    sst = sum((x - mean_actual) ** 2 for x in actual)\n",
    "    \n",
    "    # Berechnung der Summe der Quadrate der Residuen (SSE)\n",
    "    sse = sum((actual[i] - predicted[i]) ** 2 for i in range(len(actual)))\n",
    "    \n",
    "    # Berechnung des R^2-Wertes\n",
    "    r_squared = 1 - (sse / sst)\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "# Berechnung von R^2 mit den bereitgestellten Listen\n",
    "r_squared = calculate_r_squared(list1, list2)\n",
    "\n",
    "print(f\"R^2: {r_squared}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T11:11:42.577322Z",
     "start_time": "2024-04-07T11:11:42.333794200Z"
    }
   },
   "id": "d0505d16afcbef4a"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Werte die kleiner sind: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": "            Echt  Vorhergesagt  X-Koordinate  Y-Koordinate  Differenz\n302   729.686279        755.11      1.000000          0.93 -25.423721\n733   849.313904        874.06      1.000000          0.07 -24.746096\n393   802.833862        824.74      0.983871          0.05 -21.906138\n715   872.196167        893.58      1.000000          0.08 -21.383833\n183   825.964783        845.81      0.967742          0.06 -19.845217\n...          ...           ...           ...           ...        ...\n482   679.387512        660.80      0.806452          0.00  18.587512\n1127  681.046509        661.84      0.870968          0.00  19.206509\n1104  693.178528        672.09      0.983871          0.00  21.088528\n1195  691.094849        669.01      0.032258          0.00  22.084849\n354   705.022644        669.05      0.000000          0.00  35.972644\n\n[1273 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Echt</th>\n      <th>Vorhergesagt</th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Differenz</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>302</th>\n      <td>729.686279</td>\n      <td>755.11</td>\n      <td>1.000000</td>\n      <td>0.93</td>\n      <td>-25.423721</td>\n    </tr>\n    <tr>\n      <th>733</th>\n      <td>849.313904</td>\n      <td>874.06</td>\n      <td>1.000000</td>\n      <td>0.07</td>\n      <td>-24.746096</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>802.833862</td>\n      <td>824.74</td>\n      <td>0.983871</td>\n      <td>0.05</td>\n      <td>-21.906138</td>\n    </tr>\n    <tr>\n      <th>715</th>\n      <td>872.196167</td>\n      <td>893.58</td>\n      <td>1.000000</td>\n      <td>0.08</td>\n      <td>-21.383833</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>825.964783</td>\n      <td>845.81</td>\n      <td>0.967742</td>\n      <td>0.06</td>\n      <td>-19.845217</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>482</th>\n      <td>679.387512</td>\n      <td>660.80</td>\n      <td>0.806452</td>\n      <td>0.00</td>\n      <td>18.587512</td>\n    </tr>\n    <tr>\n      <th>1127</th>\n      <td>681.046509</td>\n      <td>661.84</td>\n      <td>0.870968</td>\n      <td>0.00</td>\n      <td>19.206509</td>\n    </tr>\n    <tr>\n      <th>1104</th>\n      <td>693.178528</td>\n      <td>672.09</td>\n      <td>0.983871</td>\n      <td>0.00</td>\n      <td>21.088528</td>\n    </tr>\n    <tr>\n      <th>1195</th>\n      <td>691.094849</td>\n      <td>669.01</td>\n      <td>0.032258</td>\n      <td>0.00</td>\n      <td>22.084849</td>\n    </tr>\n    <tr>\n      <th>354</th>\n      <td>705.022644</td>\n      <td>669.05</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>35.972644</td>\n    </tr>\n  </tbody>\n</table>\n<p>1273 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zeigt die größten Differenzen von vorhergesagten Wert und echten Wert an\n",
    "df_result = pd.DataFrame({'Echt': [val[0] for val in list2], 'Vorhergesagt': [val[0] for val in list1]})\n",
    "df_result['X-Koordinate'] = X_test_scaled[:, 0]\n",
    "df_result['Y-Koordinate'] = X_test_scaled[:, 1]\n",
    "\n",
    "df_result['Differenz'] = abs(df_result['Echt'] - df_result['Vorhergesagt'])\n",
    "df_result['Differenz'].sort_values()\n",
    "sorted_df = df_result.sort_values(by= 'Differenz')\n",
    "Anzahl_Punkte = (sorted_df['Differenz'] < -20).sum()\n",
    "print(\"Anzahl der Werte die kleiner sind:\", Anzahl_Punkte)\n",
    "\n",
    "sorted_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T10:01:02.224416400Z",
     "start_time": "2024-03-15T10:01:02.210676800Z"
    }
   },
   "id": "47d4a39665ed1159"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotten des Trainingsprozesses"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8efc22d71713adfd"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae = history.history['loss']\n",
    "val_mae = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(mae) + 1)\n",
    "\n",
    "# MAE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mae, 'r', label='Training MSE')\n",
    "plt.plot(epochs, val_mae, 'b', label='Validation MSE')\n",
    "plt.title('Training and Validation MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Bilder Diplomarbeit/MSE_NeuroNetz/MSE_NeuroNetz_D3_2')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T10:01:02.437200300Z",
     "start_time": "2024-03-15T10:01:02.225416500Z"
    }
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f6f672a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T10:01:02.439200700Z",
     "start_time": "2024-03-15T10:01:02.436683600Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def build_model(learning_rate=0.001, activation='relu', regularization=0.0001, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, activation=activation, input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(448, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(352, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(320, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(256, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(416, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(128, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(96, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))    \n",
    "\n",
    "    model.add(Dense(32, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Verwenden Sie eine Funktion, um das Modell zu instanziieren, für scikit-learn Wrapper\n",
    "model = KerasRegressor(model=build_model, verbose=2)\n",
    "\n",
    "# Anpassung der Parameter im param_grid\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'model__regularization': [0.001, 0.0001, 0.00001],\n",
    "    'fit__batch_size': [25, 50, 75, 100],\n",
    "    'fit__epochs': [50],\n",
    "    'model__dropout_rate' : [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "# Hinweis: Stellen Sie sicher, dass Ihre Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "# Beste Parameter und Score ausgeben\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "print(\"Beste Genauigkeit:\", grid_search.best_score_)\n",
    "\n",
    "with open(\"Gridsearch_D3.txt\", \"w\") as f:\n",
    "    f.write(f\"Beste Parameter: {grid_search.best_params_}\\n\")\n",
    "    f.write(f\"Beste Genauigkeit: {grid_search.best_score_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T10:01:02.450167100Z",
     "start_time": "2024-03-15T10:01:02.439200700Z"
    }
   },
   "id": "578403f6e218787a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Search"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b9aef262bcb2c3"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "492cf0ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T10:01:02.450167100Z",
     "start_time": "2024-03-15T10:01:02.443220200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Funktion zum Erstellen des Modells\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp.Int('input_units', min_value=8, max_value=328, step=16), input_shape=(2,), activation='relu'))\n",
    "    for i in range(hp.Int('n_layers', 1, 10)):\n",
    "        model.add(Dense(hp.Int(f'units_{i}', min_value=8, max_value=328, step=16), activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Durchführung der Random Search dreimal\n",
    "for run in range(1, 4):\n",
    "    # Anpassen des Verzeichnisses und des Projektnamens für jeden Durchlauf\n",
    "    directory = 'random_search'\n",
    "    project_name = f'random_search_D3_{run}'\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=100,\n",
    "        executions_per_trial=1,\n",
    "        directory=directory,\n",
    "        project_name=project_name\n",
    "    )\n",
    "\n",
    "    # Durchführung des Random Search\n",
    "    tuner.search(X_train_scaled, y_train_scaled, epochs=50, verbose =0, batch_size=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "    # Abrufen und Speichern des besten Modells\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    model_path = os.path.join(directory, project_name, 'best_model.h5') \n",
    "    best_model.save(model_path)\n",
    "\n",
    "\n",
    "    # Optional: Abrufen und Ausgeben der besten Hyperparameter\n",
    "    best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    # Konvertieren der Hyperparameter in ein DataFrame\n",
    "    df_hyperparameters = pd.DataFrame([best_hyperparameters.values])\n",
    "    # Speichern des DataFrame als CSV\n",
    "    df_hyperparameters.to_csv(f'random_search_D3_{run}.csv', index=False)\n",
    "\n",
    "    print(f\"Beste Hyperparameter für Lauf {run}: {best_hyperparameters.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T10:01:02.450167100Z",
     "start_time": "2024-03-15T10:01:02.447649400Z"
    }
   },
   "id": "412f38f9b1e03d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

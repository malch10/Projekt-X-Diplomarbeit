{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b0518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T22:20:17.378746Z",
     "start_time": "2024-04-02T22:20:09.803905800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ff61b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T22:20:17.433662800Z",
     "start_time": "2024-04-02T22:20:17.379746500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n0             0.0000      -0.00200        100   6000   5000      449.80\n1             0.0000      -0.00192        100   6000   5000      479.76\n2             0.0000      -0.00184        100   6000   5000      506.60\n3             0.0000      -0.00176        100   6000   5000      530.80\n4             0.0000      -0.00168        100   6000   5000      552.15\n...              ...           ...        ...    ...    ...         ...\n351283        0.0024       0.00168        500   9000   5000     1365.50\n351284        0.0024       0.00176        500   9000   5000     1247.20\n351285        0.0024       0.00184        500   9000   5000     1114.10\n351286        0.0024       0.00192        500   9000   5000      983.97\n351287        0.0024       0.00200        500   9000   5000      942.84\n\n[179928 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>449.80</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00192</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>479.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00184</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>506.60</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00176</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>530.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00168</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>552.15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>351283</th>\n      <td>0.0024</td>\n      <td>0.00168</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>1365.50</td>\n    </tr>\n    <tr>\n      <th>351284</th>\n      <td>0.0024</td>\n      <td>0.00176</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>1247.20</td>\n    </tr>\n    <tr>\n      <th>351285</th>\n      <td>0.0024</td>\n      <td>0.00184</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>1114.10</td>\n    </tr>\n    <tr>\n      <th>351286</th>\n      <td>0.0024</td>\n      <td>0.00192</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>983.97</td>\n    </tr>\n    <tr>\n      <th>351287</th>\n      <td>0.0024</td>\n      <td>0.00200</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>942.84</td>\n    </tr>\n  </tbody>\n</table>\n<p>179928 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/Finish_ALL_D4_t_21_I_F_PKL.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "966e3c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:54:07.099578Z",
     "start_time": "2024-04-03T08:54:07.033246300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n",
      "131733        0.0000      -0.00200        100   7000   6000      478.13\n",
      "131734        0.0000      -0.00192        100   7000   6000      511.16\n",
      "131735        0.0000      -0.00184        100   7000   6000      541.43\n",
      "131736        0.0000      -0.00176        100   7000   6000      569.14\n",
      "131737        0.0000      -0.00168        100   7000   6000      594.14\n",
      "...              ...           ...        ...    ...    ...         ...\n",
      "175639        0.0024       0.00168        500   7000   6000      942.96\n",
      "175640        0.0024       0.00176        500   7000   6000      865.10\n",
      "175641        0.0024       0.00184        500   7000   6000      786.99\n",
      "175642        0.0024       0.00192        500   7000   6000      708.11\n",
      "175643        0.0024       0.00200        500   7000   6000      690.48\n",
      "\n",
      "[22491 rows x 6 columns]\n",
      "Empty DataFrame\n",
      "Columns: [X-Koordinate, Y-Koordinate, Zeitpunkt, Strom, Kraft, Temperatur]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [X-Koordinate, Y-Koordinate, Zeitpunkt, Strom, Kraft, Temperatur]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft\n0            0.00084       0.00040        260   7000   9000\n1            0.00084       0.00152        460   8000   7000\n2            0.00012      -0.00104        320   8000   7000\n3            0.00132       0.00120        400   8000   7000\n4            0.00132       0.00064        280   7000   9000\n...              ...           ...        ...    ...    ...\n157432       0.00228       0.00032        220   8000   7000\n157433       0.00204      -0.00112        340   8000   6000\n157434       0.00036       0.00168        460   8000   7000\n157435       0.00024       0.00104        320   9000   5000\n157436       0.00216      -0.00064        260   8000   7000\n\n[157437 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00084</td>\n      <td>0.00040</td>\n      <td>260</td>\n      <td>7000</td>\n      <td>9000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00084</td>\n      <td>0.00152</td>\n      <td>460</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00012</td>\n      <td>-0.00104</td>\n      <td>320</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00132</td>\n      <td>0.00120</td>\n      <td>400</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00132</td>\n      <td>0.00064</td>\n      <td>280</td>\n      <td>7000</td>\n      <td>9000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>157432</th>\n      <td>0.00228</td>\n      <td>0.00032</td>\n      <td>220</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>157433</th>\n      <td>0.00204</td>\n      <td>-0.00112</td>\n      <td>340</td>\n      <td>8000</td>\n      <td>6000</td>\n    </tr>\n    <tr>\n      <th>157434</th>\n      <td>0.00036</td>\n      <td>0.00168</td>\n      <td>460</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>157435</th>\n      <td>0.00024</td>\n      <td>0.00104</td>\n      <td>320</td>\n      <td>9000</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>157436</th>\n      <td>0.00216</td>\n      <td>-0.00064</td>\n      <td>260</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n  </tbody>\n</table>\n<p>157437 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedingung = (data['Kraft'] == 6000) & (data['Strom'] == 7000)\n",
    "df_test = data[bedingung].copy()\n",
    "print(df_test)\n",
    "data_all = data.drop(df_test.index)\n",
    "\n",
    "print(data_all[(data_all['Kraft'] == 6000) & (data_all['Strom'] == 7000)])\n",
    "#print(data_all[(data_all['Kraft'] == 6000) & (data_all['Strom'] == 7000)])\n",
    "df_test_500 = df_test[(df_test['Zeitpunkt'] == 500)]\n",
    "\n",
    "\n",
    "#Shufflen\n",
    "data_all = data_all.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "df_reset = data_all.reset_index(drop=True)\n",
    "df_reset\n",
    "\n",
    "#Trainingsdaten\n",
    "y = df_reset[\"Temperatur\"]\n",
    "X = df_reset.drop(\"Temperatur\", axis=1)\n",
    "\n",
    "#Testdaten\n",
    "y_2 = df_test[\"Temperatur\"]\n",
    "X_2 = df_test.drop(\"Temperatur\", axis=1)\n",
    "X_2 = X_2.reset_index(drop=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7fa289a50d87423"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c705edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:54:08.125902600Z",
     "start_time": "2024-04-03T08:54:08.079134200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X)\n",
    "X_test_scaled = scaler_features.transform(X_2)\n",
    "# Initialisiere einen SEPARATEN MinMaxScaler für das Ziel, wenn nötig\n",
    "scaler_target = MinMaxScaler()\n",
    "# Skaliere y_train und y_test. Beachte, dass y_train.reshape(-1, 1) verwendet wird, da MinMaxScaler \n",
    "# erwartet, dass die Eingaben als 2D-Arrays kommen, und Ziele normalerweise als 1D-Arrays vorliegen.\n",
    "y_train_scaled = scaler_target.fit_transform(y.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_2.values.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbefe631e495b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T22:20:17.650816700Z",
     "start_time": "2024-04-02T22:20:17.532221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 1.        , 0.33333333, 0.25      ],\n       [0.        , 0.02      , 1.        , 0.33333333, 0.25      ],\n       [0.        , 0.04      , 1.        , 0.33333333, 0.25      ],\n       ...,\n       [1.        , 0.96      , 1.        , 0.33333333, 0.25      ],\n       [1.        , 0.98      , 1.        , 0.33333333, 0.25      ],\n       [1.        , 1.        , 1.        , 0.33333333, 0.25      ]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.20401097],\n       [0.24270037],\n       [0.28272435],\n       ...,\n       [0.19284284],\n       [0.15727799],\n       [0.1493291 ]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T22:20:17.650816700Z",
     "start_time": "2024-04-02T22:20:17.542287400Z"
    }
   },
   "id": "ce04ce43aac2242f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "Epoch 1/2000\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "630/630 [==============================] - 6s 7ms/step - loss: 0.0511 - mae: 0.0694 - val_loss: 0.0312 - val_mae: 0.0334\n",
      "Epoch 2/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0287 - mae: 0.0257 - val_loss: 0.0271 - val_mae: 0.0215\n",
      "Epoch 3/2000\n",
      "630/630 [==============================] - 5s 7ms/step - loss: 0.0260 - mae: 0.0173 - val_loss: 0.0253 - val_mae: 0.0174\n",
      "Epoch 4/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0245 - mae: 0.0135 - val_loss: 0.0239 - val_mae: 0.0122\n",
      "Epoch 5/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0235 - mae: 0.0122 - val_loss: 0.0230 - val_mae: 0.0112\n",
      "Epoch 6/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0226 - mae: 0.0104 - val_loss: 0.0221 - val_mae: 0.0089\n",
      "Epoch 7/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0218 - mae: 0.0101 - val_loss: 0.0213 - val_mae: 0.0099\n",
      "Epoch 8/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0209 - mae: 0.0094 - val_loss: 0.0207 - val_mae: 0.0147\n",
      "Epoch 9/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0201 - mae: 0.0098 - val_loss: 0.0197 - val_mae: 0.0089\n",
      "Epoch 10/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0194 - mae: 0.0099 - val_loss: 0.0189 - val_mae: 0.0067\n",
      "Epoch 11/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0185 - mae: 0.0087 - val_loss: 0.0181 - val_mae: 0.0080\n",
      "Epoch 12/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0178 - mae: 0.0092 - val_loss: 0.0173 - val_mae: 0.0071\n",
      "Epoch 13/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0170 - mae: 0.0094 - val_loss: 0.0166 - val_mae: 0.0078\n",
      "Epoch 14/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0162 - mae: 0.0081 - val_loss: 0.0158 - val_mae: 0.0072\n",
      "Epoch 15/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0155 - mae: 0.0081 - val_loss: 0.0152 - val_mae: 0.0088\n",
      "Epoch 16/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0148 - mae: 0.0082 - val_loss: 0.0144 - val_mae: 0.0069\n",
      "Epoch 17/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0142 - mae: 0.0084 - val_loss: 0.0138 - val_mae: 0.0070\n",
      "Epoch 18/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0135 - mae: 0.0073 - val_loss: 0.0132 - val_mae: 0.0064\n",
      "Epoch 19/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0129 - mae: 0.0073 - val_loss: 0.0126 - val_mae: 0.0070\n",
      "Epoch 20/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0123 - mae: 0.0072 - val_loss: 0.0120 - val_mae: 0.0068\n",
      "Epoch 21/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0118 - mae: 0.0072 - val_loss: 0.0115 - val_mae: 0.0054\n",
      "Epoch 22/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0112 - mae: 0.0064 - val_loss: 0.0113 - val_mae: 0.0163\n",
      "Epoch 23/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0108 - mae: 0.0070 - val_loss: 0.0106 - val_mae: 0.0077\n",
      "Epoch 24/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0103 - mae: 0.0069 - val_loss: 0.0101 - val_mae: 0.0076\n",
      "Epoch 25/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0099 - mae: 0.0064 - val_loss: 0.0097 - val_mae: 0.0060\n",
      "Epoch 26/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0095 - mae: 0.0064 - val_loss: 0.0093 - val_mae: 0.0075\n",
      "Epoch 27/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0091 - mae: 0.0064 - val_loss: 0.0089 - val_mae: 0.0064\n",
      "Epoch 28/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0087 - mae: 0.0065 - val_loss: 0.0085 - val_mae: 0.0050\n",
      "Epoch 29/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0084 - mae: 0.0061 - val_loss: 0.0082 - val_mae: 0.0063\n",
      "Epoch 30/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0081 - mae: 0.0065 - val_loss: 0.0079 - val_mae: 0.0050\n",
      "Epoch 31/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0077 - mae: 0.0059 - val_loss: 0.0076 - val_mae: 0.0051\n",
      "Epoch 32/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0074 - mae: 0.0059 - val_loss: 0.0073 - val_mae: 0.0052\n",
      "Epoch 33/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0072 - mae: 0.0059 - val_loss: 0.0070 - val_mae: 0.0064\n",
      "Epoch 34/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0069 - mae: 0.0058 - val_loss: 0.0067 - val_mae: 0.0053\n",
      "Epoch 35/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0066 - mae: 0.0058 - val_loss: 0.0065 - val_mae: 0.0056\n",
      "Epoch 36/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0063 - mae: 0.0058 - val_loss: 0.0062 - val_mae: 0.0048\n",
      "Epoch 37/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0061 - mae: 0.0057 - val_loss: 0.0060 - val_mae: 0.0060\n",
      "Epoch 38/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0059 - mae: 0.0057 - val_loss: 0.0058 - val_mae: 0.0087\n",
      "Epoch 39/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0056 - mae: 0.0056 - val_loss: 0.0055 - val_mae: 0.0046\n",
      "Epoch 40/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0054 - mae: 0.0055 - val_loss: 0.0053 - val_mae: 0.0056\n",
      "Epoch 41/2000\n",
      "630/630 [==============================] - 5s 7ms/step - loss: 0.0052 - mae: 0.0053 - val_loss: 0.0051 - val_mae: 0.0052\n",
      "Epoch 42/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0050 - mae: 0.0057 - val_loss: 0.0049 - val_mae: 0.0057\n",
      "Epoch 43/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0048 - mae: 0.0055 - val_loss: 0.0047 - val_mae: 0.0048\n",
      "Epoch 44/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0046 - mae: 0.0051 - val_loss: 0.0045 - val_mae: 0.0044\n",
      "Epoch 45/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0045 - mae: 0.0053 - val_loss: 0.0044 - val_mae: 0.0058\n",
      "Epoch 46/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0043 - mae: 0.0053 - val_loss: 0.0042 - val_mae: 0.0060\n",
      "Epoch 47/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0041 - mae: 0.0051 - val_loss: 0.0041 - val_mae: 0.0059\n",
      "Epoch 48/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0040 - mae: 0.0051 - val_loss: 0.0039 - val_mae: 0.0047\n",
      "Epoch 49/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0038 - mae: 0.0055 - val_loss: 0.0038 - val_mae: 0.0060\n",
      "Epoch 50/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0037 - mae: 0.0051 - val_loss: 0.0037 - val_mae: 0.0078\n",
      "Epoch 51/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0050 - val_loss: 0.0035 - val_mae: 0.0045\n",
      "Epoch 52/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0034 - mae: 0.0051 - val_loss: 0.0034 - val_mae: 0.0052\n",
      "Epoch 53/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0033 - mae: 0.0050 - val_loss: 0.0032 - val_mae: 0.0044\n",
      "Epoch 54/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0032 - mae: 0.0053 - val_loss: 0.0031 - val_mae: 0.0046\n",
      "Epoch 55/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0031 - mae: 0.0050 - val_loss: 0.0030 - val_mae: 0.0043\n",
      "Epoch 56/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0030 - mae: 0.0051 - val_loss: 0.0029 - val_mae: 0.0052\n",
      "Epoch 57/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0028 - mae: 0.0048 - val_loss: 0.0028 - val_mae: 0.0045\n",
      "Epoch 58/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0028 - mae: 0.0049 - val_loss: 0.0027 - val_mae: 0.0039\n",
      "Epoch 59/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0027 - mae: 0.0048 - val_loss: 0.0026 - val_mae: 0.0043\n",
      "Epoch 60/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0026 - mae: 0.0050 - val_loss: 0.0025 - val_mae: 0.0055\n",
      "Epoch 61/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0025 - mae: 0.0047 - val_loss: 0.0024 - val_mae: 0.0052\n",
      "Epoch 62/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0024 - mae: 0.0047 - val_loss: 0.0024 - val_mae: 0.0054\n",
      "Epoch 63/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0023 - mae: 0.0047 - val_loss: 0.0023 - val_mae: 0.0046\n",
      "Epoch 64/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0022 - mae: 0.0048 - val_loss: 0.0022 - val_mae: 0.0043\n",
      "Epoch 65/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0022 - mae: 0.0049 - val_loss: 0.0021 - val_mae: 0.0055\n",
      "Epoch 66/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0021 - val_mae: 0.0042\n",
      "Epoch 67/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0020 - val_mae: 0.0041\n",
      "Epoch 68/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0020 - mae: 0.0045 - val_loss: 0.0019 - val_mae: 0.0036\n",
      "Epoch 69/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0045\n",
      "Epoch 70/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0054\n",
      "Epoch 71/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0037\n",
      "Epoch 72/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0044\n",
      "Epoch 73/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0017 - mae: 0.0045 - val_loss: 0.0017 - val_mae: 0.0055\n",
      "Epoch 74/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0016 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0039\n",
      "Epoch 75/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0016 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0042\n",
      "Epoch 76/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0015 - mae: 0.0043 - val_loss: 0.0015 - val_mae: 0.0054\n",
      "Epoch 77/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0037\n",
      "Epoch 78/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0062\n",
      "Epoch 79/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0059\n",
      "Epoch 80/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0048\n",
      "Epoch 81/2000\n",
      "630/630 [==============================] - 5s 8ms/step - loss: 0.0013 - mae: 0.0043 - val_loss: 0.0013 - val_mae: 0.0042\n",
      "Epoch 82/2000\n",
      "630/630 [==============================] - 5s 7ms/step - loss: 0.0013 - mae: 0.0046 - val_loss: 0.0013 - val_mae: 0.0048\n",
      "Epoch 83/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0013 - mae: 0.0042 - val_loss: 0.0013 - val_mae: 0.0037\n",
      "Epoch 84/2000\n",
      "630/630 [==============================] - 5s 7ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0036\n",
      "Epoch 85/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0042\n",
      "Epoch 86/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0012 - mae: 0.0042 - val_loss: 0.0012 - val_mae: 0.0041\n",
      "Epoch 87/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0012 - mae: 0.0043 - val_loss: 0.0012 - val_mae: 0.0054\n",
      "Epoch 88/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0038\n",
      "Epoch 89/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0035\n",
      "Epoch 90/2000\n",
      "630/630 [==============================] - 5s 8ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0047\n",
      "Epoch 91/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0010 - val_mae: 0.0039\n",
      "Epoch 92/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0010 - mae: 0.0045 - val_loss: 0.0010 - val_mae: 0.0035\n",
      "Epoch 93/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 0.0010 - mae: 0.0043 - val_loss: 0.0010 - val_mae: 0.0075\n",
      "Epoch 94/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.8781e-04 - mae: 0.0042 - val_loss: 9.7878e-04 - val_mae: 0.0044\n",
      "Epoch 95/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.6708e-04 - mae: 0.0043 - val_loss: 9.4862e-04 - val_mae: 0.0035\n",
      "Epoch 96/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.4729e-04 - mae: 0.0043 - val_loss: 0.0010 - val_mae: 0.0078\n",
      "Epoch 97/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.2741e-04 - mae: 0.0043 - val_loss: 9.1652e-04 - val_mae: 0.0041\n",
      "Epoch 98/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.0866e-04 - mae: 0.0043 - val_loss: 8.9892e-04 - val_mae: 0.0042\n",
      "Epoch 99/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.8985e-04 - mae: 0.0042 - val_loss: 8.9874e-04 - val_mae: 0.0056\n",
      "Epoch 100/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.7258e-04 - mae: 0.0043 - val_loss: 8.8843e-04 - val_mae: 0.0063\n",
      "Epoch 101/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.5550e-04 - mae: 0.0043 - val_loss: 8.5883e-04 - val_mae: 0.0050\n",
      "Epoch 102/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.3994e-04 - mae: 0.0043 - val_loss: 8.2825e-04 - val_mae: 0.0039\n",
      "Epoch 103/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.2235e-04 - mae: 0.0041 - val_loss: 8.1117e-04 - val_mae: 0.0039\n",
      "Epoch 104/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.0791e-04 - mae: 0.0042 - val_loss: 8.0061e-04 - val_mae: 0.0042\n",
      "Epoch 105/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.9123e-04 - mae: 0.0041 - val_loss: 8.1160e-04 - val_mae: 0.0064\n",
      "Epoch 106/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.7940e-04 - mae: 0.0043 - val_loss: 7.9023e-04 - val_mae: 0.0056\n",
      "Epoch 107/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.6238e-04 - mae: 0.0040 - val_loss: 7.4976e-04 - val_mae: 0.0035\n",
      "Epoch 108/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.4999e-04 - mae: 0.0041 - val_loss: 7.5483e-04 - val_mae: 0.0051\n",
      "Epoch 109/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.3665e-04 - mae: 0.0041 - val_loss: 7.2198e-04 - val_mae: 0.0032\n",
      "Epoch 110/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.2422e-04 - mae: 0.0041 - val_loss: 7.1862e-04 - val_mae: 0.0040\n",
      "Epoch 111/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.1198e-04 - mae: 0.0041 - val_loss: 7.0211e-04 - val_mae: 0.0038\n",
      "Epoch 112/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.0035e-04 - mae: 0.0041 - val_loss: 6.9090e-04 - val_mae: 0.0039\n",
      "Epoch 113/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.8788e-04 - mae: 0.0040 - val_loss: 6.7864e-04 - val_mae: 0.0038\n",
      "Epoch 114/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.7599e-04 - mae: 0.0040 - val_loss: 6.7507e-04 - val_mae: 0.0044\n",
      "Epoch 115/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.6593e-04 - mae: 0.0040 - val_loss: 6.5379e-04 - val_mae: 0.0033\n",
      "Epoch 116/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.5633e-04 - mae: 0.0041 - val_loss: 6.5293e-04 - val_mae: 0.0042\n",
      "Epoch 117/2000\n",
      "630/630 [==============================] - 4s 7ms/step - loss: 6.4564e-04 - mae: 0.0040 - val_loss: 6.3994e-04 - val_mae: 0.0040\n",
      "Epoch 118/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.3540e-04 - mae: 0.0040 - val_loss: 6.2433e-04 - val_mae: 0.0034\n",
      "Epoch 119/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.2786e-04 - mae: 0.0041 - val_loss: 6.1920e-04 - val_mae: 0.0037\n",
      "Epoch 120/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.1719e-04 - mae: 0.0040 - val_loss: 6.1686e-04 - val_mae: 0.0044\n",
      "Epoch 121/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.0928e-04 - mae: 0.0041 - val_loss: 5.9943e-04 - val_mae: 0.0036\n",
      "Epoch 122/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.0240e-04 - mae: 0.0042 - val_loss: 5.9109e-04 - val_mae: 0.0034\n",
      "Epoch 123/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.9185e-04 - mae: 0.0040 - val_loss: 6.0293e-04 - val_mae: 0.0054\n",
      "Epoch 124/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.8421e-04 - mae: 0.0040 - val_loss: 5.9657e-04 - val_mae: 0.0054\n",
      "Epoch 125/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.7550e-04 - mae: 0.0039 - val_loss: 5.8072e-04 - val_mae: 0.0046\n",
      "Epoch 126/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.6856e-04 - mae: 0.0040 - val_loss: 6.0080e-04 - val_mae: 0.0070\n",
      "Epoch 127/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.6047e-04 - mae: 0.0039 - val_loss: 5.5319e-04 - val_mae: 0.0036\n",
      "Epoch 128/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.5342e-04 - mae: 0.0039 - val_loss: 5.4213e-04 - val_mae: 0.0032\n",
      "Epoch 129/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.4690e-04 - mae: 0.0040 - val_loss: 5.3757e-04 - val_mae: 0.0034\n",
      "Epoch 130/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.4489e-04 - mae: 0.0043 - val_loss: 5.5149e-04 - val_mae: 0.0050\n",
      "Epoch 131/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.3208e-04 - mae: 0.0038 - val_loss: 5.2214e-04 - val_mae: 0.0032\n",
      "Epoch 132/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.2667e-04 - mae: 0.0039 - val_loss: 5.3472e-04 - val_mae: 0.0048\n",
      "Epoch 133/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.2137e-04 - mae: 0.0040 - val_loss: 5.1311e-04 - val_mae: 0.0034\n",
      "Epoch 134/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.1502e-04 - mae: 0.0040 - val_loss: 5.0906e-04 - val_mae: 0.0036\n",
      "Epoch 135/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.1056e-04 - mae: 0.0040 - val_loss: 5.0557e-04 - val_mae: 0.0037\n",
      "Epoch 136/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.0505e-04 - mae: 0.0041 - val_loss: 4.9782e-04 - val_mae: 0.0036\n",
      "Epoch 137/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.9883e-04 - mae: 0.0040 - val_loss: 4.9339e-04 - val_mae: 0.0038\n",
      "Epoch 138/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.9281e-04 - mae: 0.0039 - val_loss: 4.9043e-04 - val_mae: 0.0040\n",
      "Epoch 139/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.8803e-04 - mae: 0.0039 - val_loss: 5.0273e-04 - val_mae: 0.0054\n",
      "Epoch 140/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.8269e-04 - mae: 0.0040 - val_loss: 4.7409e-04 - val_mae: 0.0034\n",
      "Epoch 141/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.7532e-04 - mae: 0.0038 - val_loss: 4.7095e-04 - val_mae: 0.0036\n",
      "Epoch 142/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.7213e-04 - mae: 0.0039 - val_loss: 4.7878e-04 - val_mae: 0.0045\n",
      "Epoch 143/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.6898e-04 - mae: 0.0040 - val_loss: 4.5685e-04 - val_mae: 0.0031\n",
      "Epoch 144/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.6278e-04 - mae: 0.0039 - val_loss: 4.6523e-04 - val_mae: 0.0042\n",
      "Epoch 145/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.5821e-04 - mae: 0.0039 - val_loss: 4.5435e-04 - val_mae: 0.0038\n",
      "Epoch 146/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.5402e-04 - mae: 0.0039 - val_loss: 4.5112e-04 - val_mae: 0.0039\n",
      "Epoch 147/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4926e-04 - mae: 0.0039 - val_loss: 4.5950e-04 - val_mae: 0.0052\n",
      "Epoch 148/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4514e-04 - mae: 0.0039 - val_loss: 4.4360e-04 - val_mae: 0.0040\n",
      "Epoch 149/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4106e-04 - mae: 0.0039 - val_loss: 4.3721e-04 - val_mae: 0.0037\n",
      "Epoch 150/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.3961e-04 - mae: 0.0041 - val_loss: 4.2966e-04 - val_mae: 0.0033\n",
      "Epoch 151/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.3460e-04 - mae: 0.0040 - val_loss: 4.2590e-04 - val_mae: 0.0033\n",
      "Epoch 152/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.2901e-04 - mae: 0.0039 - val_loss: 4.3417e-04 - val_mae: 0.0046\n",
      "Epoch 153/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.2739e-04 - mae: 0.0040 - val_loss: 4.3579e-04 - val_mae: 0.0051\n",
      "Epoch 154/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.2323e-04 - mae: 0.0040 - val_loss: 4.1654e-04 - val_mae: 0.0035\n",
      "Epoch 155/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1943e-04 - mae: 0.0039 - val_loss: 4.1016e-04 - val_mae: 0.0031\n",
      "Epoch 156/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1501e-04 - mae: 0.0039 - val_loss: 4.1568e-04 - val_mae: 0.0041\n",
      "Epoch 157/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1143e-04 - mae: 0.0039 - val_loss: 4.1195e-04 - val_mae: 0.0041\n",
      "Epoch 158/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.0844e-04 - mae: 0.0039 - val_loss: 4.0902e-04 - val_mae: 0.0040\n",
      "Epoch 159/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.0620e-04 - mae: 0.0039 - val_loss: 4.0285e-04 - val_mae: 0.0039\n",
      "Epoch 160/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.0435e-04 - mae: 0.0041 - val_loss: 3.9526e-04 - val_mae: 0.0033\n",
      "Epoch 161/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9907e-04 - mae: 0.0039 - val_loss: 3.9050e-04 - val_mae: 0.0033\n",
      "Epoch 162/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9474e-04 - mae: 0.0038 - val_loss: 3.9516e-04 - val_mae: 0.0040\n",
      "Epoch 163/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9168e-04 - mae: 0.0038 - val_loss: 3.9121e-04 - val_mae: 0.0039\n",
      "Epoch 164/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9149e-04 - mae: 0.0040 - val_loss: 3.8495e-04 - val_mae: 0.0035\n",
      "Epoch 165/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.8874e-04 - mae: 0.0040 - val_loss: 3.8577e-04 - val_mae: 0.0039\n",
      "Epoch 166/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.8343e-04 - mae: 0.0038 - val_loss: 4.1144e-04 - val_mae: 0.0063\n",
      "Epoch 167/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.8232e-04 - mae: 0.0040 - val_loss: 4.0414e-04 - val_mae: 0.0059\n",
      "Epoch 168/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7871e-04 - mae: 0.0039 - val_loss: 3.7445e-04 - val_mae: 0.0035\n",
      "Epoch 169/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7717e-04 - mae: 0.0040 - val_loss: 3.8163e-04 - val_mae: 0.0046\n",
      "Epoch 170/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7163e-04 - mae: 0.0037 - val_loss: 3.6702e-04 - val_mae: 0.0034\n",
      "Epoch 171/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7077e-04 - mae: 0.0039 - val_loss: 3.7126e-04 - val_mae: 0.0040\n",
      "Epoch 172/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6935e-04 - mae: 0.0040 - val_loss: 3.6334e-04 - val_mae: 0.0035\n",
      "Epoch 173/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6639e-04 - mae: 0.0039 - val_loss: 3.5809e-04 - val_mae: 0.0032\n",
      "Epoch 174/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6272e-04 - mae: 0.0038 - val_loss: 3.6252e-04 - val_mae: 0.0041\n",
      "Epoch 175/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.5965e-04 - mae: 0.0038 - val_loss: 3.9745e-04 - val_mae: 0.0067\n",
      "Epoch 176/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6052e-04 - mae: 0.0040 - val_loss: 3.9767e-04 - val_mae: 0.0067\n",
      "Epoch 177/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.5707e-04 - mae: 0.0039 - val_loss: 3.5118e-04 - val_mae: 0.0034\n",
      "Epoch 178/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.5347e-04 - mae: 0.0038 - val_loss: 3.5164e-04 - val_mae: 0.0037\n",
      "Epoch 179/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.5092e-04 - mae: 0.0038 - val_loss: 3.4520e-04 - val_mae: 0.0033\n",
      "Epoch 180/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.5038e-04 - mae: 0.0040 - val_loss: 3.4531e-04 - val_mae: 0.0036\n",
      "Epoch 181/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.4645e-04 - mae: 0.0038 - val_loss: 3.5628e-04 - val_mae: 0.0049\n",
      "Epoch 182/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.4668e-04 - mae: 0.0040 - val_loss: 3.5235e-04 - val_mae: 0.0047\n",
      "Epoch 183/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.4230e-04 - mae: 0.0038 - val_loss: 3.4452e-04 - val_mae: 0.0042\n",
      "Epoch 184/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.4212e-04 - mae: 0.0040 - val_loss: 3.3490e-04 - val_mae: 0.0033\n",
      "Epoch 185/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3974e-04 - mae: 0.0039 - val_loss: 3.4503e-04 - val_mae: 0.0044\n",
      "Epoch 186/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3674e-04 - mae: 0.0038 - val_loss: 3.3696e-04 - val_mae: 0.0039\n",
      "Epoch 187/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3512e-04 - mae: 0.0039 - val_loss: 3.3215e-04 - val_mae: 0.0036\n",
      "Epoch 188/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3335e-04 - mae: 0.0039 - val_loss: 3.3098e-04 - val_mae: 0.0038\n",
      "Epoch 189/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3203e-04 - mae: 0.0039 - val_loss: 3.2426e-04 - val_mae: 0.0032\n",
      "Epoch 190/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2886e-04 - mae: 0.0038 - val_loss: 3.3149e-04 - val_mae: 0.0042\n",
      "Epoch 191/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2731e-04 - mae: 0.0038 - val_loss: 3.2344e-04 - val_mae: 0.0036\n",
      "Epoch 192/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2705e-04 - mae: 0.0040 - val_loss: 3.2995e-04 - val_mae: 0.0044\n",
      "Epoch 193/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2347e-04 - mae: 0.0038 - val_loss: 3.2183e-04 - val_mae: 0.0036\n",
      "Epoch 194/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2231e-04 - mae: 0.0039 - val_loss: 3.1633e-04 - val_mae: 0.0033\n",
      "Epoch 195/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2113e-04 - mae: 0.0039 - val_loss: 3.1838e-04 - val_mae: 0.0037\n",
      "Epoch 196/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2066e-04 - mae: 0.0040 - val_loss: 3.1590e-04 - val_mae: 0.0036\n",
      "Epoch 197/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1637e-04 - mae: 0.0038 - val_loss: 3.1429e-04 - val_mae: 0.0035\n",
      "Epoch 198/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1635e-04 - mae: 0.0039 - val_loss: 3.1057e-04 - val_mae: 0.0034\n",
      "Epoch 199/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1435e-04 - mae: 0.0039 - val_loss: 3.1885e-04 - val_mae: 0.0044\n",
      "Epoch 200/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1264e-04 - mae: 0.0039 - val_loss: 3.0967e-04 - val_mae: 0.0036\n",
      "Epoch 201/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1174e-04 - mae: 0.0039 - val_loss: 3.3604e-04 - val_mae: 0.0060\n",
      "Epoch 202/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1006e-04 - mae: 0.0039 - val_loss: 3.0373e-04 - val_mae: 0.0034\n",
      "Epoch 203/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0619e-04 - mae: 0.0037 - val_loss: 3.0246e-04 - val_mae: 0.0033\n",
      "Epoch 204/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0852e-04 - mae: 0.0040 - val_loss: 3.0060e-04 - val_mae: 0.0032\n",
      "Epoch 205/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0348e-04 - mae: 0.0037 - val_loss: 2.9731e-04 - val_mae: 0.0031\n",
      "Epoch 206/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0278e-04 - mae: 0.0038 - val_loss: 3.0623e-04 - val_mae: 0.0042\n",
      "Epoch 207/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0227e-04 - mae: 0.0039 - val_loss: 3.0076e-04 - val_mae: 0.0038\n",
      "Epoch 208/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0385e-04 - mae: 0.0041 - val_loss: 2.9479e-04 - val_mae: 0.0033\n",
      "Epoch 209/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0056e-04 - mae: 0.0040 - val_loss: 2.9335e-04 - val_mae: 0.0033\n",
      "Epoch 210/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9842e-04 - mae: 0.0039 - val_loss: 2.9195e-04 - val_mae: 0.0033\n",
      "Epoch 211/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9591e-04 - mae: 0.0038 - val_loss: 3.0440e-04 - val_mae: 0.0044\n",
      "Epoch 212/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9568e-04 - mae: 0.0039 - val_loss: 2.8779e-04 - val_mae: 0.0031\n",
      "Epoch 213/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9546e-04 - mae: 0.0040 - val_loss: 3.0538e-04 - val_mae: 0.0050\n",
      "Epoch 214/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9259e-04 - mae: 0.0038 - val_loss: 2.8854e-04 - val_mae: 0.0034\n",
      "Epoch 215/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9349e-04 - mae: 0.0040 - val_loss: 2.8828e-04 - val_mae: 0.0035\n",
      "Epoch 216/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8990e-04 - mae: 0.0038 - val_loss: 2.8329e-04 - val_mae: 0.0032\n",
      "Epoch 217/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8840e-04 - mae: 0.0038 - val_loss: 2.8381e-04 - val_mae: 0.0034\n",
      "Epoch 218/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8867e-04 - mae: 0.0039 - val_loss: 2.8306e-04 - val_mae: 0.0034\n",
      "Epoch 219/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8717e-04 - mae: 0.0039 - val_loss: 2.7989e-04 - val_mae: 0.0031\n",
      "Epoch 220/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8542e-04 - mae: 0.0038 - val_loss: 2.9073e-04 - val_mae: 0.0044\n",
      "Epoch 221/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8438e-04 - mae: 0.0039 - val_loss: 3.0777e-04 - val_mae: 0.0055\n",
      "Epoch 222/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8493e-04 - mae: 0.0040 - val_loss: 2.7701e-04 - val_mae: 0.0033\n",
      "Epoch 223/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8212e-04 - mae: 0.0038 - val_loss: 3.4992e-04 - val_mae: 0.0085\n",
      "Epoch 224/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8242e-04 - mae: 0.0040 - val_loss: 2.7514e-04 - val_mae: 0.0032\n",
      "Epoch 225/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7921e-04 - mae: 0.0038 - val_loss: 2.8369e-04 - val_mae: 0.0043\n",
      "Epoch 226/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8023e-04 - mae: 0.0040 - val_loss: 2.8926e-04 - val_mae: 0.0048\n",
      "Epoch 227/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7879e-04 - mae: 0.0040 - val_loss: 2.7888e-04 - val_mae: 0.0040\n",
      "Epoch 228/2000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7652e-04 - mae: 0.0038 - val_loss: 2.9224e-04 - val_mae: 0.0054\n",
      "Epoch 229/2000\n",
      "626/630 [============================>.] - ETA: 0s - loss: 2.7638e-04 - mae: 0.0039Restoring model weights from the end of the best epoch: 224.\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7644e-04 - mae: 0.0039 - val_loss: 2.8204e-04 - val_mae: 0.0045\n",
      "Epoch 229: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "\n",
    "    Dense(280, activation='relu', input_shape=(5,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(136, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(328, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(328, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(152, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(104, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(248, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(152, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001) \n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 200\n",
    "epochs = 2000\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "model.save('D4_t_21_I_F_3.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T22:36:35.515083Z",
     "start_time": "2024-04-02T22:21:35.811462Z"
    }
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training für Fold 1...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 5s 6ms/step - loss: 0.0463 - mae: 0.0608 - val_loss: 0.0315 - val_mae: 0.0282\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0295 - mae: 0.0222 - val_loss: 0.0280 - val_mae: 0.0187\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0269 - mae: 0.0160 - val_loss: 0.0260 - val_mae: 0.0134\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0254 - mae: 0.0126 - val_loss: 0.0247 - val_mae: 0.0101\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0242 - mae: 0.0105 - val_loss: 0.0237 - val_mae: 0.0096\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0232 - mae: 0.0101 - val_loss: 0.0228 - val_mae: 0.0106\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0223 - mae: 0.0096 - val_loss: 0.0219 - val_mae: 0.0117\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0213 - mae: 0.0091 - val_loss: 0.0209 - val_mae: 0.0082\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0205 - mae: 0.0094 - val_loss: 0.0199 - val_mae: 0.0076\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0195 - mae: 0.0084 - val_loss: 0.0191 - val_mae: 0.0077\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0187 - mae: 0.0087 - val_loss: 0.0183 - val_mae: 0.0100\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0178 - mae: 0.0088 - val_loss: 0.0174 - val_mae: 0.0075\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0170 - mae: 0.0085 - val_loss: 0.0165 - val_mae: 0.0074\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0162 - mae: 0.0080 - val_loss: 0.0158 - val_mae: 0.0063\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0154 - mae: 0.0081 - val_loss: 0.0151 - val_mae: 0.0080\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0147 - mae: 0.0071 - val_loss: 0.0143 - val_mae: 0.0057\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0140 - mae: 0.0075 - val_loss: 0.0136 - val_mae: 0.0060\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0133 - mae: 0.0072 - val_loss: 0.0130 - val_mae: 0.0073\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0127 - mae: 0.0069 - val_loss: 0.0124 - val_mae: 0.0072\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0121 - mae: 0.0074 - val_loss: 0.0118 - val_mae: 0.0053\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0115 - mae: 0.0069 - val_loss: 0.0112 - val_mae: 0.0055\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0110 - mae: 0.0064 - val_loss: 0.0107 - val_mae: 0.0053\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0105 - mae: 0.0064 - val_loss: 0.0102 - val_mae: 0.0063\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0100 - mae: 0.0063 - val_loss: 0.0098 - val_mae: 0.0059\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0095 - mae: 0.0062 - val_loss: 0.0093 - val_mae: 0.0071\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0091 - mae: 0.0060 - val_loss: 0.0089 - val_mae: 0.0055\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0087 - mae: 0.0059 - val_loss: 0.0085 - val_mae: 0.0055\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0083 - mae: 0.0061 - val_loss: 0.0082 - val_mae: 0.0091\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0079 - mae: 0.0057 - val_loss: 0.0078 - val_mae: 0.0067\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0076 - mae: 0.0057 - val_loss: 0.0074 - val_mae: 0.0066\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0072 - mae: 0.0060 - val_loss: 0.0070 - val_mae: 0.0045\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0069 - mae: 0.0056 - val_loss: 0.0068 - val_mae: 0.0060\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0066 - mae: 0.0055 - val_loss: 0.0065 - val_mae: 0.0087\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0063 - mae: 0.0054 - val_loss: 0.0062 - val_mae: 0.0049\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0060 - mae: 0.0058 - val_loss: 0.0059 - val_mae: 0.0044\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0058 - mae: 0.0053 - val_loss: 0.0056 - val_mae: 0.0051\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0055 - mae: 0.0053 - val_loss: 0.0054 - val_mae: 0.0062\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0053 - mae: 0.0056 - val_loss: 0.0052 - val_mae: 0.0054\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0051 - mae: 0.0054 - val_loss: 0.0050 - val_mae: 0.0072\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0049 - mae: 0.0054 - val_loss: 0.0047 - val_mae: 0.0050\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0046 - mae: 0.0051 - val_loss: 0.0046 - val_mae: 0.0088\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0045 - mae: 0.0052 - val_loss: 0.0044 - val_mae: 0.0072\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0043 - mae: 0.0053 - val_loss: 0.0042 - val_mae: 0.0043\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0041 - mae: 0.0054 - val_loss: 0.0040 - val_mae: 0.0054\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0039 - mae: 0.0050 - val_loss: 0.0039 - val_mae: 0.0047\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0038 - mae: 0.0052 - val_loss: 0.0037 - val_mae: 0.0046\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0036 - mae: 0.0051 - val_loss: 0.0036 - val_mae: 0.0055\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0035 - mae: 0.0052 - val_loss: 0.0034 - val_mae: 0.0046\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0034 - mae: 0.0051 - val_loss: 0.0033 - val_mae: 0.0050\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0032 - mae: 0.0050 - val_loss: 0.0032 - val_mae: 0.0047\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0031 - mae: 0.0052 - val_loss: 0.0031 - val_mae: 0.0052\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0030 - mae: 0.0050 - val_loss: 0.0030 - val_mae: 0.0061\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0029 - mae: 0.0051 - val_loss: 0.0028 - val_mae: 0.0041\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0028 - mae: 0.0047 - val_loss: 0.0027 - val_mae: 0.0051\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0027 - mae: 0.0051 - val_loss: 0.0026 - val_mae: 0.0051\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0026 - mae: 0.0050 - val_loss: 0.0025 - val_mae: 0.0042\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0025 - mae: 0.0051 - val_loss: 0.0025 - val_mae: 0.0041\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0024 - mae: 0.0048 - val_loss: 0.0024 - val_mae: 0.0042\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0023 - mae: 0.0048 - val_loss: 0.0023 - val_mae: 0.0040\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0023 - mae: 0.0051 - val_loss: 0.0022 - val_mae: 0.0039\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0022 - mae: 0.0048 - val_loss: 0.0022 - val_mae: 0.0059\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0021 - val_mae: 0.0047\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0021 - mae: 0.0048 - val_loss: 0.0020 - val_mae: 0.0051\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0020 - val_mae: 0.0046\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0044\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0019 - mae: 0.0046 - val_loss: 0.0018 - val_mae: 0.0044\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0018 - mae: 0.0048 - val_loss: 0.0018 - val_mae: 0.0055\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0018 - mae: 0.0048 - val_loss: 0.0017 - val_mae: 0.0053\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0017 - mae: 0.0047 - val_loss: 0.0017 - val_mae: 0.0039\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0017 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0048\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0016 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0055\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0053\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0044\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0039\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0044\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0044\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0051\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0036\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0013 - mae: 0.0048 - val_loss: 0.0013 - val_mae: 0.0041\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0013 - mae: 0.0043 - val_loss: 0.0013 - val_mae: 0.0049\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0012 - mae: 0.0047 - val_loss: 0.0012 - val_mae: 0.0040\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0012 - mae: 0.0046 - val_loss: 0.0012 - val_mae: 0.0038\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0044\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0043\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0011 - mae: 0.0046 - val_loss: 0.0011 - val_mae: 0.0039\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0035\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0035\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0010 - mae: 0.0043 - val_loss: 0.0010 - val_mae: 0.0051\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0010 - mae: 0.0047 - val_loss: 0.0010 - val_mae: 0.0036\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0010 - mae: 0.0044 - val_loss: 9.8280e-04 - val_mae: 0.0034\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.8178e-04 - mae: 0.0043 - val_loss: 9.6276e-04 - val_mae: 0.0036\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.6182e-04 - mae: 0.0044 - val_loss: 9.5609e-04 - val_mae: 0.0047\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.4146e-04 - mae: 0.0044 - val_loss: 9.2429e-04 - val_mae: 0.0037\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.2278e-04 - mae: 0.0044 - val_loss: 9.0657e-04 - val_mae: 0.0038\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.0300e-04 - mae: 0.0043 - val_loss: 8.8875e-04 - val_mae: 0.0039\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.8424e-04 - mae: 0.0043 - val_loss: 8.6741e-04 - val_mae: 0.0035\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.6704e-04 - mae: 0.0043 - val_loss: 8.5697e-04 - val_mae: 0.0042\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.5235e-04 - mae: 0.0045 - val_loss: 8.4312e-04 - val_mae: 0.0044\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.3232e-04 - mae: 0.0042 - val_loss: 8.2845e-04 - val_mae: 0.0045\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.1776e-04 - mae: 0.0043 - val_loss: 8.3432e-04 - val_mae: 0.0062\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.0427e-04 - mae: 0.0044 - val_loss: 8.2140e-04 - val_mae: 0.0064\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.8730e-04 - mae: 0.0042 - val_loss: 7.8244e-04 - val_mae: 0.0044\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.7165e-04 - mae: 0.0042 - val_loss: 7.5922e-04 - val_mae: 0.0037\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.6224e-04 - mae: 0.0045 - val_loss: 7.4208e-04 - val_mae: 0.0033\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.4562e-04 - mae: 0.0043 - val_loss: 7.3684e-04 - val_mae: 0.0040\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.3299e-04 - mae: 0.0043 - val_loss: 7.1930e-04 - val_mae: 0.0036\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.2195e-04 - mae: 0.0044 - val_loss: 7.1951e-04 - val_mae: 0.0047\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.0848e-04 - mae: 0.0042 - val_loss: 7.0850e-04 - val_mae: 0.0049\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.9674e-04 - mae: 0.0042 - val_loss: 7.1021e-04 - val_mae: 0.0055\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.8378e-04 - mae: 0.0041 - val_loss: 6.7514e-04 - val_mae: 0.0037\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.7405e-04 - mae: 0.0042 - val_loss: 6.8103e-04 - val_mae: 0.0051\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.6140e-04 - mae: 0.0041 - val_loss: 6.4962e-04 - val_mae: 0.0035\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.5088e-04 - mae: 0.0041 - val_loss: 6.3903e-04 - val_mae: 0.0034\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.4268e-04 - mae: 0.0042 - val_loss: 6.4380e-04 - val_mae: 0.0044\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.3069e-04 - mae: 0.0041 - val_loss: 6.4213e-04 - val_mae: 0.0055\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.2030e-04 - mae: 0.0040 - val_loss: 6.0917e-04 - val_mae: 0.0034\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.1042e-04 - mae: 0.0040 - val_loss: 6.0360e-04 - val_mae: 0.0038\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.0550e-04 - mae: 0.0043 - val_loss: 6.0836e-04 - val_mae: 0.0050\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.9478e-04 - mae: 0.0041 - val_loss: 5.8768e-04 - val_mae: 0.0039\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.8637e-04 - mae: 0.0041 - val_loss: 5.8273e-04 - val_mae: 0.0040\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.7942e-04 - mae: 0.0042 - val_loss: 5.6794e-04 - val_mae: 0.0035\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.7244e-04 - mae: 0.0042 - val_loss: 5.7945e-04 - val_mae: 0.0050\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.6101e-04 - mae: 0.0039 - val_loss: 5.7212e-04 - val_mae: 0.0052\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.5568e-04 - mae: 0.0041 - val_loss: 5.6687e-04 - val_mae: 0.0053\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.4795e-04 - mae: 0.0041 - val_loss: 5.3697e-04 - val_mae: 0.0033\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.4170e-04 - mae: 0.0041 - val_loss: 5.2798e-04 - val_mae: 0.0031\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.3393e-04 - mae: 0.0041 - val_loss: 5.5682e-04 - val_mae: 0.0062\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.2802e-04 - mae: 0.0041 - val_loss: 5.1688e-04 - val_mae: 0.0033\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.2264e-04 - mae: 0.0042 - val_loss: 5.1138e-04 - val_mae: 0.0033\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.1326e-04 - mae: 0.0039 - val_loss: 5.2713e-04 - val_mae: 0.0054\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.0873e-04 - mae: 0.0041 - val_loss: 5.1494e-04 - val_mae: 0.0050\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.0417e-04 - mae: 0.0041 - val_loss: 5.3058e-04 - val_mae: 0.0063\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.9736e-04 - mae: 0.0041 - val_loss: 4.9685e-04 - val_mae: 0.0042\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.9116e-04 - mae: 0.0040 - val_loss: 4.8148e-04 - val_mae: 0.0033\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.8801e-04 - mae: 0.0042 - val_loss: 4.7522e-04 - val_mae: 0.0032\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.7865e-04 - mae: 0.0039 - val_loss: 4.7183e-04 - val_mae: 0.0034\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.7723e-04 - mae: 0.0042 - val_loss: 4.7389e-04 - val_mae: 0.0041\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.6876e-04 - mae: 0.0039 - val_loss: 4.6936e-04 - val_mae: 0.0043\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.6596e-04 - mae: 0.0041 - val_loss: 4.6122e-04 - val_mae: 0.0039\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.5916e-04 - mae: 0.0039 - val_loss: 4.5079e-04 - val_mae: 0.0033\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.5633e-04 - mae: 0.0041 - val_loss: 4.8535e-04 - val_mae: 0.0065\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.5163e-04 - mae: 0.0041 - val_loss: 4.9243e-04 - val_mae: 0.0071\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4621e-04 - mae: 0.0040 - val_loss: 4.3782e-04 - val_mae: 0.0034\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4173e-04 - mae: 0.0040 - val_loss: 4.4914e-04 - val_mae: 0.0047\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.3661e-04 - mae: 0.0040 - val_loss: 4.2935e-04 - val_mae: 0.0033\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.3446e-04 - mae: 0.0041 - val_loss: 4.3435e-04 - val_mae: 0.0044\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.2901e-04 - mae: 0.0040 - val_loss: 4.1845e-04 - val_mae: 0.0032\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.2559e-04 - mae: 0.0041 - val_loss: 4.5579e-04 - val_mae: 0.0067\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1975e-04 - mae: 0.0039 - val_loss: 4.3435e-04 - val_mae: 0.0053\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1563e-04 - mae: 0.0039 - val_loss: 4.4643e-04 - val_mae: 0.0064\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1303e-04 - mae: 0.0040 - val_loss: 4.0696e-04 - val_mae: 0.0036\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.0877e-04 - mae: 0.0040 - val_loss: 4.1267e-04 - val_mae: 0.0043\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.0625e-04 - mae: 0.0041 - val_loss: 4.1787e-04 - val_mae: 0.0053\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.0244e-04 - mae: 0.0040 - val_loss: 3.9287e-04 - val_mae: 0.0032\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9690e-04 - mae: 0.0039 - val_loss: 3.9310e-04 - val_mae: 0.0037\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9570e-04 - mae: 0.0040 - val_loss: 3.8831e-04 - val_mae: 0.0035\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9103e-04 - mae: 0.0039 - val_loss: 3.8189e-04 - val_mae: 0.0031\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.8617e-04 - mae: 0.0038 - val_loss: 3.8019e-04 - val_mae: 0.0032\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.8524e-04 - mae: 0.0040 - val_loss: 3.8850e-04 - val_mae: 0.0045\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.8293e-04 - mae: 0.0041 - val_loss: 3.7096e-04 - val_mae: 0.0030\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7605e-04 - mae: 0.0038 - val_loss: 3.8865e-04 - val_mae: 0.0051\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7613e-04 - mae: 0.0040 - val_loss: 3.7298e-04 - val_mae: 0.0038\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7256e-04 - mae: 0.0040 - val_loss: 3.6673e-04 - val_mae: 0.0035\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6864e-04 - mae: 0.0039 - val_loss: 3.6399e-04 - val_mae: 0.0035\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6630e-04 - mae: 0.0039 - val_loss: 3.6091e-04 - val_mae: 0.0034\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6446e-04 - mae: 0.0040 - val_loss: 3.6919e-04 - val_mae: 0.0042\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6186e-04 - mae: 0.0040 - val_loss: 3.6609e-04 - val_mae: 0.0045\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.5780e-04 - mae: 0.0038 - val_loss: 3.5217e-04 - val_mae: 0.0035\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.5513e-04 - mae: 0.0039 - val_loss: 3.5121e-04 - val_mae: 0.0036\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.5250e-04 - mae: 0.0039 - val_loss: 3.5278e-04 - val_mae: 0.0040\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.5124e-04 - mae: 0.0040 - val_loss: 3.4781e-04 - val_mae: 0.0036\n",
      "Epoch 172/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.4807e-04 - mae: 0.0039 - val_loss: 3.4567e-04 - val_mae: 0.0037\n",
      "Epoch 173/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.4520e-04 - mae: 0.0038 - val_loss: 3.3935e-04 - val_mae: 0.0034\n",
      "Epoch 174/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.4532e-04 - mae: 0.0041 - val_loss: 3.4992e-04 - val_mae: 0.0044\n",
      "Epoch 175/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.4055e-04 - mae: 0.0039 - val_loss: 3.3291e-04 - val_mae: 0.0031\n",
      "Epoch 176/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3733e-04 - mae: 0.0038 - val_loss: 3.3117e-04 - val_mae: 0.0032\n",
      "Epoch 177/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3609e-04 - mae: 0.0039 - val_loss: 3.2885e-04 - val_mae: 0.0032\n",
      "Epoch 178/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3560e-04 - mae: 0.0040 - val_loss: 3.3453e-04 - val_mae: 0.0042\n",
      "Epoch 179/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3167e-04 - mae: 0.0039 - val_loss: 3.2893e-04 - val_mae: 0.0038\n",
      "Epoch 180/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2869e-04 - mae: 0.0038 - val_loss: 3.2114e-04 - val_mae: 0.0030\n",
      "Epoch 181/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2773e-04 - mae: 0.0039 - val_loss: 3.2032e-04 - val_mae: 0.0032\n",
      "Epoch 182/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2718e-04 - mae: 0.0040 - val_loss: 3.1993e-04 - val_mae: 0.0034\n",
      "Epoch 183/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2284e-04 - mae: 0.0038 - val_loss: 3.1753e-04 - val_mae: 0.0034\n",
      "Epoch 184/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2001e-04 - mae: 0.0037 - val_loss: 3.2627e-04 - val_mae: 0.0045\n",
      "Epoch 185/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1938e-04 - mae: 0.0039 - val_loss: 3.1811e-04 - val_mae: 0.0035\n",
      "Epoch 186/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1695e-04 - mae: 0.0038 - val_loss: 3.2152e-04 - val_mae: 0.0043\n",
      "Epoch 187/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1736e-04 - mae: 0.0040 - val_loss: 3.1419e-04 - val_mae: 0.0038\n",
      "Epoch 188/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1267e-04 - mae: 0.0038 - val_loss: 3.0515e-04 - val_mae: 0.0030\n",
      "Epoch 189/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1086e-04 - mae: 0.0038 - val_loss: 3.0610e-04 - val_mae: 0.0033\n",
      "Epoch 190/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1083e-04 - mae: 0.0039 - val_loss: 3.0784e-04 - val_mae: 0.0036\n",
      "Epoch 191/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0773e-04 - mae: 0.0038 - val_loss: 3.0695e-04 - val_mae: 0.0036\n",
      "Epoch 192/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0700e-04 - mae: 0.0039 - val_loss: 3.0589e-04 - val_mae: 0.0039\n",
      "Epoch 193/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0555e-04 - mae: 0.0039 - val_loss: 2.9916e-04 - val_mae: 0.0033\n",
      "Epoch 194/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0285e-04 - mae: 0.0038 - val_loss: 3.0187e-04 - val_mae: 0.0038\n",
      "Epoch 195/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0085e-04 - mae: 0.0038 - val_loss: 2.9458e-04 - val_mae: 0.0031\n",
      "Epoch 196/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.0295e-04 - mae: 0.0041 - val_loss: 2.9268e-04 - val_mae: 0.0030\n",
      "Epoch 197/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9823e-04 - mae: 0.0038 - val_loss: 2.9114e-04 - val_mae: 0.0032\n",
      "Epoch 198/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9486e-04 - mae: 0.0036 - val_loss: 2.8925e-04 - val_mae: 0.0030\n",
      "Epoch 199/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9649e-04 - mae: 0.0039 - val_loss: 2.9329e-04 - val_mae: 0.0036\n",
      "Epoch 200/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9261e-04 - mae: 0.0037 - val_loss: 2.8862e-04 - val_mae: 0.0033\n",
      "Epoch 201/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9341e-04 - mae: 0.0039 - val_loss: 2.8768e-04 - val_mae: 0.0035\n",
      "Epoch 202/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9001e-04 - mae: 0.0037 - val_loss: 2.8428e-04 - val_mae: 0.0031\n",
      "Epoch 203/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8855e-04 - mae: 0.0037 - val_loss: 2.9260e-04 - val_mae: 0.0042\n",
      "Epoch 204/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8781e-04 - mae: 0.0038 - val_loss: 2.9939e-04 - val_mae: 0.0047\n",
      "Epoch 205/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8751e-04 - mae: 0.0039 - val_loss: 3.1715e-04 - val_mae: 0.0061\n",
      "Epoch 206/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8540e-04 - mae: 0.0038 - val_loss: 2.8813e-04 - val_mae: 0.0041\n",
      "Epoch 207/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8352e-04 - mae: 0.0038 - val_loss: 2.8074e-04 - val_mae: 0.0036\n",
      "Epoch 208/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8315e-04 - mae: 0.0038 - val_loss: 2.8529e-04 - val_mae: 0.0042\n",
      "Epoch 209/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8148e-04 - mae: 0.0038 - val_loss: 2.7545e-04 - val_mae: 0.0033\n",
      "Epoch 210/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7926e-04 - mae: 0.0037 - val_loss: 2.7365e-04 - val_mae: 0.0031\n",
      "Epoch 211/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7883e-04 - mae: 0.0038 - val_loss: 2.8148e-04 - val_mae: 0.0042\n",
      "Epoch 212/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7890e-04 - mae: 0.0039 - val_loss: 2.7297e-04 - val_mae: 0.0033\n",
      "Epoch 213/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7689e-04 - mae: 0.0038 - val_loss: 2.6916e-04 - val_mae: 0.0031\n",
      "Epoch 214/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7461e-04 - mae: 0.0037 - val_loss: 2.7464e-04 - val_mae: 0.0038\n",
      "Epoch 215/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7592e-04 - mae: 0.0039 - val_loss: 2.6720e-04 - val_mae: 0.0031\n",
      "Epoch 216/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7442e-04 - mae: 0.0039 - val_loss: 2.7052e-04 - val_mae: 0.0035\n",
      "Epoch 217/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7102e-04 - mae: 0.0037 - val_loss: 2.6767e-04 - val_mae: 0.0033\n",
      "Epoch 218/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.6887e-04 - mae: 0.0036 - val_loss: 2.7095e-04 - val_mae: 0.0039\n",
      "Epoch 219/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7055e-04 - mae: 0.0039 - val_loss: 2.6844e-04 - val_mae: 0.0037\n",
      "Epoch 220/1000\n",
      "628/630 [============================>.] - ETA: 0s - loss: 2.6726e-04 - mae: 0.0037Restoring model weights from the end of the best epoch: 215.\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.6726e-04 - mae: 0.0037 - val_loss: 2.7197e-04 - val_mae: 0.0042\n",
      "Epoch 220: early stopping\n",
      "Training für Fold 2...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 5s 6ms/step - loss: 0.0373 - mae: 0.0470 - val_loss: 0.0299 - val_mae: 0.0253\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0276 - mae: 0.0190 - val_loss: 0.0258 - val_mae: 0.0142\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0247 - mae: 0.0132 - val_loss: 0.0237 - val_mae: 0.0103\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0230 - mae: 0.0112 - val_loss: 0.0223 - val_mae: 0.0097\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0218 - mae: 0.0109 - val_loss: 0.0216 - val_mae: 0.0193\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0207 - mae: 0.0097 - val_loss: 0.0201 - val_mae: 0.0078\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0197 - mae: 0.0094 - val_loss: 0.0192 - val_mae: 0.0089\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0188 - mae: 0.0093 - val_loss: 0.0183 - val_mae: 0.0092\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0179 - mae: 0.0094 - val_loss: 0.0174 - val_mae: 0.0068\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0171 - mae: 0.0091 - val_loss: 0.0167 - val_mae: 0.0079\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0163 - mae: 0.0081 - val_loss: 0.0161 - val_mae: 0.0121\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0156 - mae: 0.0081 - val_loss: 0.0152 - val_mae: 0.0073\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0149 - mae: 0.0083 - val_loss: 0.0146 - val_mae: 0.0089\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0142 - mae: 0.0072 - val_loss: 0.0139 - val_mae: 0.0075\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0136 - mae: 0.0078 - val_loss: 0.0134 - val_mae: 0.0101\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0130 - mae: 0.0074 - val_loss: 0.0127 - val_mae: 0.0074\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0125 - mae: 0.0072 - val_loss: 0.0122 - val_mae: 0.0057\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0119 - mae: 0.0070 - val_loss: 0.0117 - val_mae: 0.0073\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0114 - mae: 0.0069 - val_loss: 0.0112 - val_mae: 0.0061\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0109 - mae: 0.0066 - val_loss: 0.0107 - val_mae: 0.0084\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0105 - mae: 0.0066 - val_loss: 0.0103 - val_mae: 0.0094\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0100 - mae: 0.0064 - val_loss: 0.0099 - val_mae: 0.0074\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0096 - mae: 0.0062 - val_loss: 0.0094 - val_mae: 0.0062\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0092 - mae: 0.0063 - val_loss: 0.0090 - val_mae: 0.0058\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0088 - mae: 0.0062 - val_loss: 0.0087 - val_mae: 0.0070\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0085 - mae: 0.0062 - val_loss: 0.0084 - val_mae: 0.0108\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0081 - mae: 0.0059 - val_loss: 0.0080 - val_mae: 0.0069\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0078 - mae: 0.0059 - val_loss: 0.0076 - val_mae: 0.0045\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0075 - mae: 0.0061 - val_loss: 0.0073 - val_mae: 0.0057\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0072 - mae: 0.0056 - val_loss: 0.0070 - val_mae: 0.0045\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0069 - mae: 0.0058 - val_loss: 0.0067 - val_mae: 0.0069\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0066 - mae: 0.0055 - val_loss: 0.0064 - val_mae: 0.0046\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0063 - mae: 0.0056 - val_loss: 0.0062 - val_mae: 0.0048\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0061 - mae: 0.0055 - val_loss: 0.0060 - val_mae: 0.0068\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0058 - mae: 0.0054 - val_loss: 0.0057 - val_mae: 0.0046\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0056 - mae: 0.0055 - val_loss: 0.0054 - val_mae: 0.0056\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0053 - mae: 0.0056 - val_loss: 0.0052 - val_mae: 0.0058\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0051 - mae: 0.0051 - val_loss: 0.0050 - val_mae: 0.0049\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0049 - mae: 0.0053 - val_loss: 0.0048 - val_mae: 0.0044\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0047 - mae: 0.0051 - val_loss: 0.0046 - val_mae: 0.0045\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0045 - mae: 0.0055 - val_loss: 0.0044 - val_mae: 0.0052\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0043 - mae: 0.0050 - val_loss: 0.0043 - val_mae: 0.0053\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0042 - mae: 0.0051 - val_loss: 0.0041 - val_mae: 0.0046\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0040 - mae: 0.0051 - val_loss: 0.0039 - val_mae: 0.0050\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0038 - mae: 0.0051 - val_loss: 0.0037 - val_mae: 0.0041\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0037 - mae: 0.0051 - val_loss: 0.0036 - val_mae: 0.0043\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0036 - mae: 0.0051 - val_loss: 0.0035 - val_mae: 0.0074\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0034 - mae: 0.0052 - val_loss: 0.0033 - val_mae: 0.0043\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0033 - mae: 0.0047 - val_loss: 0.0032 - val_mae: 0.0046\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0032 - mae: 0.0050 - val_loss: 0.0031 - val_mae: 0.0057\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0031 - mae: 0.0050 - val_loss: 0.0030 - val_mae: 0.0056\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0029 - mae: 0.0049 - val_loss: 0.0029 - val_mae: 0.0076\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0028 - mae: 0.0048 - val_loss: 0.0028 - val_mae: 0.0049\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0027 - mae: 0.0050 - val_loss: 0.0027 - val_mae: 0.0044\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0026 - mae: 0.0048 - val_loss: 0.0026 - val_mae: 0.0041\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0025 - mae: 0.0048 - val_loss: 0.0025 - val_mae: 0.0043\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0025 - mae: 0.0046 - val_loss: 0.0024 - val_mae: 0.0055\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0024 - mae: 0.0048 - val_loss: 0.0023 - val_mae: 0.0046\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0023 - mae: 0.0047 - val_loss: 0.0023 - val_mae: 0.0049\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0022 - mae: 0.0050 - val_loss: 0.0022 - val_mae: 0.0041\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0021 - mae: 0.0045 - val_loss: 0.0021 - val_mae: 0.0038\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0020 - val_mae: 0.0053\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0020 - mae: 0.0047 - val_loss: 0.0020 - val_mae: 0.0037\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0019 - mae: 0.0045 - val_loss: 0.0019 - val_mae: 0.0044\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0019 - mae: 0.0046 - val_loss: 0.0019 - val_mae: 0.0054\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0018 - mae: 0.0046 - val_loss: 0.0018 - val_mae: 0.0064\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0017 - val_mae: 0.0050\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0044\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0017 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0051\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0016 - mae: 0.0045 - val_loss: 0.0016 - val_mae: 0.0045\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0067\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0037\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0048\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0046\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0037\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0013 - val_mae: 0.0042\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0013 - mae: 0.0046 - val_loss: 0.0013 - val_mae: 0.0045\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0043\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0039\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0035\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0047\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0046\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0045\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0040\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0045\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0010 - val_mae: 0.0036\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0010 - mae: 0.0044 - val_loss: 0.0010 - val_mae: 0.0058\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0010 - mae: 0.0046 - val_loss: 9.9520e-04 - val_mae: 0.0041\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.8735e-04 - mae: 0.0044 - val_loss: 9.7284e-04 - val_mae: 0.0042\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.6482e-04 - mae: 0.0043 - val_loss: 9.5622e-04 - val_mae: 0.0047\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.4393e-04 - mae: 0.0043 - val_loss: 9.3991e-04 - val_mae: 0.0050\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.2541e-04 - mae: 0.0044 - val_loss: 9.0537e-04 - val_mae: 0.0036\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 9.0595e-04 - mae: 0.0044 - val_loss: 8.8561e-04 - val_mae: 0.0034\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.8720e-04 - mae: 0.0044 - val_loss: 8.7581e-04 - val_mae: 0.0042\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.6810e-04 - mae: 0.0043 - val_loss: 8.7055e-04 - val_mae: 0.0053\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.5086e-04 - mae: 0.0043 - val_loss: 8.4017e-04 - val_mae: 0.0041\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.3400e-04 - mae: 0.0043 - val_loss: 8.2847e-04 - val_mae: 0.0045\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.1777e-04 - mae: 0.0042 - val_loss: 8.3179e-04 - val_mae: 0.0060\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 8.0165e-04 - mae: 0.0042 - val_loss: 7.8607e-04 - val_mae: 0.0035\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.8941e-04 - mae: 0.0045 - val_loss: 8.1544e-04 - val_mae: 0.0071\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.7455e-04 - mae: 0.0044 - val_loss: 7.5864e-04 - val_mae: 0.0037\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.5686e-04 - mae: 0.0041 - val_loss: 7.4856e-04 - val_mae: 0.0039\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.4430e-04 - mae: 0.0041 - val_loss: 7.3410e-04 - val_mae: 0.0039\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.3325e-04 - mae: 0.0044 - val_loss: 7.2180e-04 - val_mae: 0.0039\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.2041e-04 - mae: 0.0043 - val_loss: 7.1174e-04 - val_mae: 0.0042\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.0620e-04 - mae: 0.0041 - val_loss: 6.9587e-04 - val_mae: 0.0038\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.9727e-04 - mae: 0.0043 - val_loss: 6.9556e-04 - val_mae: 0.0048\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.8550e-04 - mae: 0.0043 - val_loss: 6.7620e-04 - val_mae: 0.0042\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.7314e-04 - mae: 0.0042 - val_loss: 6.7274e-04 - val_mae: 0.0046\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.6232e-04 - mae: 0.0042 - val_loss: 6.5550e-04 - val_mae: 0.0040\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.5314e-04 - mae: 0.0042 - val_loss: 6.4193e-04 - val_mae: 0.0038\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.4264e-04 - mae: 0.0042 - val_loss: 6.3589e-04 - val_mae: 0.0040\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.3173e-04 - mae: 0.0041 - val_loss: 6.4604e-04 - val_mae: 0.0055\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.2635e-04 - mae: 0.0044 - val_loss: 6.1112e-04 - val_mae: 0.0035\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.1215e-04 - mae: 0.0040 - val_loss: 6.1069e-04 - val_mae: 0.0043\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.0592e-04 - mae: 0.0042 - val_loss: 6.2421e-04 - val_mae: 0.0061\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.9612e-04 - mae: 0.0041 - val_loss: 5.9068e-04 - val_mae: 0.0041\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.9031e-04 - mae: 0.0043 - val_loss: 5.7427e-04 - val_mae: 0.0032\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.8156e-04 - mae: 0.0042 - val_loss: 5.7158e-04 - val_mae: 0.0038\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.7201e-04 - mae: 0.0041 - val_loss: 5.5969e-04 - val_mae: 0.0033\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.6539e-04 - mae: 0.0042 - val_loss: 5.6079e-04 - val_mae: 0.0040\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.5853e-04 - mae: 0.0042 - val_loss: 5.4904e-04 - val_mae: 0.0038\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.5021e-04 - mae: 0.0041 - val_loss: 5.4114e-04 - val_mae: 0.0035\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.4217e-04 - mae: 0.0040 - val_loss: 5.3206e-04 - val_mae: 0.0034\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.3909e-04 - mae: 0.0043 - val_loss: 5.2595e-04 - val_mae: 0.0034\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.3018e-04 - mae: 0.0041 - val_loss: 5.4366e-04 - val_mae: 0.0053\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.2353e-04 - mae: 0.0041 - val_loss: 5.1215e-04 - val_mae: 0.0033\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.1719e-04 - mae: 0.0041 - val_loss: 5.3221e-04 - val_mae: 0.0056\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.1223e-04 - mae: 0.0042 - val_loss: 5.1053e-04 - val_mae: 0.0044\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 5.0686e-04 - mae: 0.0042 - val_loss: 5.0188e-04 - val_mae: 0.0040\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.9929e-04 - mae: 0.0040 - val_loss: 4.9575e-04 - val_mae: 0.0041\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.9419e-04 - mae: 0.0041 - val_loss: 4.9041e-04 - val_mae: 0.0040\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.9262e-04 - mae: 0.0043 - val_loss: 4.9389e-04 - val_mae: 0.0046\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.8324e-04 - mae: 0.0040 - val_loss: 4.7982e-04 - val_mae: 0.0040\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.7799e-04 - mae: 0.0040 - val_loss: 4.6732e-04 - val_mae: 0.0032\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.7197e-04 - mae: 0.0039 - val_loss: 4.6500e-04 - val_mae: 0.0035\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.6992e-04 - mae: 0.0041 - val_loss: 4.6560e-04 - val_mae: 0.0041\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.6614e-04 - mae: 0.0042 - val_loss: 4.5391e-04 - val_mae: 0.0033\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.5919e-04 - mae: 0.0040 - val_loss: 4.6566e-04 - val_mae: 0.0048\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.5569e-04 - mae: 0.0041 - val_loss: 4.8930e-04 - val_mae: 0.0071\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.5193e-04 - mae: 0.0041 - val_loss: 4.4213e-04 - val_mae: 0.0034\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4635e-04 - mae: 0.0040 - val_loss: 4.4347e-04 - val_mae: 0.0040\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.4300e-04 - mae: 0.0041 - val_loss: 4.4696e-04 - val_mae: 0.0045\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.3773e-04 - mae: 0.0040 - val_loss: 4.3744e-04 - val_mae: 0.0040\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.3416e-04 - mae: 0.0040 - val_loss: 4.3611e-04 - val_mae: 0.0045\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.3260e-04 - mae: 0.0042 - val_loss: 4.2283e-04 - val_mae: 0.0035\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.2541e-04 - mae: 0.0040 - val_loss: 4.1522e-04 - val_mae: 0.0031\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.2356e-04 - mae: 0.0041 - val_loss: 4.1611e-04 - val_mae: 0.0036\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1800e-04 - mae: 0.0040 - val_loss: 4.1701e-04 - val_mae: 0.0040\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1559e-04 - mae: 0.0041 - val_loss: 4.0575e-04 - val_mae: 0.0033\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1221e-04 - mae: 0.0041 - val_loss: 4.3181e-04 - val_mae: 0.0057\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.1037e-04 - mae: 0.0042 - val_loss: 4.0449e-04 - val_mae: 0.0038\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.0475e-04 - mae: 0.0040 - val_loss: 3.9622e-04 - val_mae: 0.0034\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.0399e-04 - mae: 0.0042 - val_loss: 4.0226e-04 - val_mae: 0.0041\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9888e-04 - mae: 0.0040 - val_loss: 4.2574e-04 - val_mae: 0.0064\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9457e-04 - mae: 0.0040 - val_loss: 3.8922e-04 - val_mae: 0.0036\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.9379e-04 - mae: 0.0041 - val_loss: 4.1497e-04 - val_mae: 0.0061\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.8824e-04 - mae: 0.0039 - val_loss: 3.8388e-04 - val_mae: 0.0035\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.8754e-04 - mae: 0.0041 - val_loss: 3.9160e-04 - val_mae: 0.0047\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.8265e-04 - mae: 0.0039 - val_loss: 3.7575e-04 - val_mae: 0.0034\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7970e-04 - mae: 0.0040 - val_loss: 3.8627e-04 - val_mae: 0.0047\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7796e-04 - mae: 0.0040 - val_loss: 3.7946e-04 - val_mae: 0.0044\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7631e-04 - mae: 0.0041 - val_loss: 3.6898e-04 - val_mae: 0.0036\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7237e-04 - mae: 0.0040 - val_loss: 3.6346e-04 - val_mae: 0.0032\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6873e-04 - mae: 0.0039 - val_loss: 3.6121e-04 - val_mae: 0.0033\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6743e-04 - mae: 0.0041 - val_loss: 3.7109e-04 - val_mae: 0.0045\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6464e-04 - mae: 0.0040 - val_loss: 3.5415e-04 - val_mae: 0.0031\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6236e-04 - mae: 0.0040 - val_loss: 3.5674e-04 - val_mae: 0.0036\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5996e-04 - mae: 0.0040 - val_loss: 3.6048e-04 - val_mae: 0.0042\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5765e-04 - mae: 0.0040 - val_loss: 3.5660e-04 - val_mae: 0.0041\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5501e-04 - mae: 0.0040 - val_loss: 3.7406e-04 - val_mae: 0.0057\n",
      "Epoch 172/1000\n",
      "621/630 [============================>.] - ETA: 0s - loss: 3.5457e-04 - mae: 0.0041Restoring model weights from the end of the best epoch: 167.\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5444e-04 - mae: 0.0041 - val_loss: 3.5603e-04 - val_mae: 0.0044\n",
      "Epoch 172: early stopping\n",
      "Training für Fold 3...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 5s 6ms/step - loss: 0.0345 - mae: 0.0364 - val_loss: 0.0288 - val_mae: 0.0212\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0264 - mae: 0.0159 - val_loss: 0.0245 - val_mae: 0.0121\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0232 - mae: 0.0108 - val_loss: 0.0222 - val_mae: 0.0124\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0213 - mae: 0.0103 - val_loss: 0.0205 - val_mae: 0.0096\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0197 - mae: 0.0094 - val_loss: 0.0193 - val_mae: 0.0159\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0185 - mae: 0.0092 - val_loss: 0.0179 - val_mae: 0.0081\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0174 - mae: 0.0087 - val_loss: 0.0168 - val_mae: 0.0083\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0163 - mae: 0.0083 - val_loss: 0.0159 - val_mae: 0.0096\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0154 - mae: 0.0077 - val_loss: 0.0150 - val_mae: 0.0094\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0145 - mae: 0.0081 - val_loss: 0.0141 - val_mae: 0.0067\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0138 - mae: 0.0075 - val_loss: 0.0134 - val_mae: 0.0072\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0131 - mae: 0.0076 - val_loss: 0.0127 - val_mae: 0.0063\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0124 - mae: 0.0072 - val_loss: 0.0121 - val_mae: 0.0068\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0118 - mae: 0.0070 - val_loss: 0.0115 - val_mae: 0.0060\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0112 - mae: 0.0069 - val_loss: 0.0109 - val_mae: 0.0063\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0107 - mae: 0.0064 - val_loss: 0.0104 - val_mae: 0.0069\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0102 - mae: 0.0069 - val_loss: 0.0099 - val_mae: 0.0053\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0097 - mae: 0.0063 - val_loss: 0.0095 - val_mae: 0.0061\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0093 - mae: 0.0063 - val_loss: 0.0091 - val_mae: 0.0070\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0089 - mae: 0.0060 - val_loss: 0.0087 - val_mae: 0.0059\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0085 - mae: 0.0063 - val_loss: 0.0083 - val_mae: 0.0059\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0081 - mae: 0.0062 - val_loss: 0.0079 - val_mae: 0.0053\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0077 - mae: 0.0058 - val_loss: 0.0075 - val_mae: 0.0050\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0074 - mae: 0.0060 - val_loss: 0.0072 - val_mae: 0.0047\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0071 - mae: 0.0060 - val_loss: 0.0069 - val_mae: 0.0060\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0068 - mae: 0.0056 - val_loss: 0.0066 - val_mae: 0.0049\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0065 - mae: 0.0056 - val_loss: 0.0063 - val_mae: 0.0049\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0062 - mae: 0.0056 - val_loss: 0.0061 - val_mae: 0.0046\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0059 - mae: 0.0055 - val_loss: 0.0058 - val_mae: 0.0048\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0057 - mae: 0.0055 - val_loss: 0.0056 - val_mae: 0.0044\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0055 - mae: 0.0056 - val_loss: 0.0053 - val_mae: 0.0056\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0052 - mae: 0.0055 - val_loss: 0.0051 - val_mae: 0.0048\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0050 - mae: 0.0056 - val_loss: 0.0049 - val_mae: 0.0050\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 0.0048 - mae: 0.0053 - val_loss: 0.0048 - val_mae: 0.0079\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 0.0046 - mae: 0.0055 - val_loss: 0.0045 - val_mae: 0.0060\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0044 - mae: 0.0051 - val_loss: 0.0043 - val_mae: 0.0044\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0043 - mae: 0.0053 - val_loss: 0.0042 - val_mae: 0.0061\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0041 - mae: 0.0053 - val_loss: 0.0040 - val_mae: 0.0046\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0039 - mae: 0.0053 - val_loss: 0.0038 - val_mae: 0.0048\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0038 - mae: 0.0051 - val_loss: 0.0037 - val_mae: 0.0044\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0036 - mae: 0.0054 - val_loss: 0.0035 - val_mae: 0.0045\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0035 - mae: 0.0050 - val_loss: 0.0034 - val_mae: 0.0052\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0034 - mae: 0.0050 - val_loss: 0.0033 - val_mae: 0.0043\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0032 - mae: 0.0052 - val_loss: 0.0032 - val_mae: 0.0044\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0031 - mae: 0.0052 - val_loss: 0.0031 - val_mae: 0.0045\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0030 - mae: 0.0049 - val_loss: 0.0030 - val_mae: 0.0049\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0029 - mae: 0.0050 - val_loss: 0.0028 - val_mae: 0.0042\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0028 - mae: 0.0049 - val_loss: 0.0027 - val_mae: 0.0045\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0027 - mae: 0.0049 - val_loss: 0.0026 - val_mae: 0.0042\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0026 - mae: 0.0047 - val_loss: 0.0026 - val_mae: 0.0073\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 0.0025 - mae: 0.0049 - val_loss: 0.0025 - val_mae: 0.0041\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0024 - mae: 0.0049 - val_loss: 0.0024 - val_mae: 0.0044\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0023 - mae: 0.0049 - val_loss: 0.0023 - val_mae: 0.0045\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0023 - mae: 0.0049 - val_loss: 0.0023 - val_mae: 0.0062\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0022 - mae: 0.0048 - val_loss: 0.0021 - val_mae: 0.0038\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0021 - val_mae: 0.0080\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0020 - val_mae: 0.0044\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0020 - mae: 0.0049 - val_loss: 0.0020 - val_mae: 0.0049\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0041\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0019 - mae: 0.0045 - val_loss: 0.0018 - val_mae: 0.0041\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0018 - mae: 0.0046 - val_loss: 0.0018 - val_mae: 0.0057\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0018 - mae: 0.0048 - val_loss: 0.0017 - val_mae: 0.0038\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0060\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0017 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0047\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0016 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0041\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0016 - mae: 0.0044 - val_loss: 0.0016 - val_mae: 0.0079\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0067\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0041\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0046\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0036\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0045\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0043 - val_loss: 0.0013 - val_mae: 0.0035\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0043 - val_loss: 0.0013 - val_mae: 0.0054\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0037\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0037\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0045\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0053\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0047\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0038\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0035\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0048\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0010 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0059\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0010 - mae: 0.0043 - val_loss: 9.9925e-04 - val_mae: 0.0034\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.9982e-04 - mae: 0.0043 - val_loss: 0.0010 - val_mae: 0.0054\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.8075e-04 - mae: 0.0044 - val_loss: 9.7312e-04 - val_mae: 0.0048\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.5791e-04 - mae: 0.0042 - val_loss: 9.4708e-04 - val_mae: 0.0041\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.3976e-04 - mae: 0.0043 - val_loss: 9.2484e-04 - val_mae: 0.0039\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.2009e-04 - mae: 0.0042 - val_loss: 9.0642e-04 - val_mae: 0.0039\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.0471e-04 - mae: 0.0044 - val_loss: 8.8762e-04 - val_mae: 0.0036\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.8391e-04 - mae: 0.0041 - val_loss: 9.0708e-04 - val_mae: 0.0066\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.6823e-04 - mae: 0.0042 - val_loss: 8.5392e-04 - val_mae: 0.0037\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.5368e-04 - mae: 0.0044 - val_loss: 8.5065e-04 - val_mae: 0.0048\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.3555e-04 - mae: 0.0042 - val_loss: 8.2972e-04 - val_mae: 0.0045\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.2391e-04 - mae: 0.0044 - val_loss: 8.0404e-04 - val_mae: 0.0033\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 8.0417e-04 - mae: 0.0040 - val_loss: 7.9318e-04 - val_mae: 0.0038\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.9319e-04 - mae: 0.0043 - val_loss: 7.7683e-04 - val_mae: 0.0035\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 7.7917e-04 - mae: 0.0042 - val_loss: 7.7171e-04 - val_mae: 0.0042\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.6435e-04 - mae: 0.0041 - val_loss: 7.5905e-04 - val_mae: 0.0042\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 7.5222e-04 - mae: 0.0041 - val_loss: 7.3786e-04 - val_mae: 0.0035\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 7.3727e-04 - mae: 0.0040 - val_loss: 7.2331e-04 - val_mae: 0.0032\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 7.2606e-04 - mae: 0.0041 - val_loss: 7.1169e-04 - val_mae: 0.0034\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 7.1515e-04 - mae: 0.0042 - val_loss: 7.1840e-04 - val_mae: 0.0050\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.0190e-04 - mae: 0.0040 - val_loss: 7.1202e-04 - val_mae: 0.0054\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 6.9257e-04 - mae: 0.0042 - val_loss: 6.7880e-04 - val_mae: 0.0035\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.8313e-04 - mae: 0.0042 - val_loss: 6.6790e-04 - val_mae: 0.0033\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 6.6992e-04 - mae: 0.0040 - val_loss: 6.7999e-04 - val_mae: 0.0049\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.6300e-04 - mae: 0.0042 - val_loss: 6.6606e-04 - val_mae: 0.0050\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.5158e-04 - mae: 0.0041 - val_loss: 6.4634e-04 - val_mae: 0.0040\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.4225e-04 - mae: 0.0041 - val_loss: 6.5679e-04 - val_mae: 0.0056\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 6.3345e-04 - mae: 0.0041 - val_loss: 6.6323e-04 - val_mae: 0.0064\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 6.2346e-04 - mae: 0.0040 - val_loss: 6.1929e-04 - val_mae: 0.0041\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.1621e-04 - mae: 0.0041 - val_loss: 6.1172e-04 - val_mae: 0.0043\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 6.0813e-04 - mae: 0.0041 - val_loss: 6.0170e-04 - val_mae: 0.0040\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 6.0120e-04 - mae: 0.0042 - val_loss: 5.9237e-04 - val_mae: 0.0038\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 5.9256e-04 - mae: 0.0041 - val_loss: 5.8695e-04 - val_mae: 0.0041\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 5.8507e-04 - mae: 0.0041 - val_loss: 6.0786e-04 - val_mae: 0.0062\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 5.7728e-04 - mae: 0.0040 - val_loss: 5.6786e-04 - val_mae: 0.0036\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.7094e-04 - mae: 0.0041 - val_loss: 6.0950e-04 - val_mae: 0.0069\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 5.6490e-04 - mae: 0.0041 - val_loss: 5.5289e-04 - val_mae: 0.0035\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.5494e-04 - mae: 0.0039 - val_loss: 5.5231e-04 - val_mae: 0.0041\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.5074e-04 - mae: 0.0041 - val_loss: 5.4588e-04 - val_mae: 0.0040\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 5.4297e-04 - mae: 0.0040 - val_loss: 5.3992e-04 - val_mae: 0.0041\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.3909e-04 - mae: 0.0042 - val_loss: 5.3993e-04 - val_mae: 0.0046\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 5.3110e-04 - mae: 0.0040 - val_loss: 5.2612e-04 - val_mae: 0.0039\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.2613e-04 - mae: 0.0041 - val_loss: 5.2195e-04 - val_mae: 0.0041\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 5.1957e-04 - mae: 0.0040 - val_loss: 5.1723e-04 - val_mae: 0.0040\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.1330e-04 - mae: 0.0040 - val_loss: 5.0982e-04 - val_mae: 0.0039\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 5.0795e-04 - mae: 0.0040 - val_loss: 4.9690e-04 - val_mae: 0.0032\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 5.0658e-04 - mae: 0.0042 - val_loss: 5.2423e-04 - val_mae: 0.0060\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.9531e-04 - mae: 0.0038 - val_loss: 4.8848e-04 - val_mae: 0.0034\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.9178e-04 - mae: 0.0039 - val_loss: 4.9504e-04 - val_mae: 0.0043\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.8935e-04 - mae: 0.0041 - val_loss: 4.8366e-04 - val_mae: 0.0039\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.8118e-04 - mae: 0.0039 - val_loss: 4.7073e-04 - val_mae: 0.0031\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.7649e-04 - mae: 0.0038 - val_loss: 4.7798e-04 - val_mae: 0.0043\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.7285e-04 - mae: 0.0040 - val_loss: 4.6549e-04 - val_mae: 0.0035\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.6765e-04 - mae: 0.0039 - val_loss: 4.5738e-04 - val_mae: 0.0032\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.6381e-04 - mae: 0.0040 - val_loss: 4.5153e-04 - val_mae: 0.0030\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.6039e-04 - mae: 0.0041 - val_loss: 4.6831e-04 - val_mae: 0.0050\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.5407e-04 - mae: 0.0039 - val_loss: 4.6292e-04 - val_mae: 0.0049\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.4968e-04 - mae: 0.0038 - val_loss: 4.4198e-04 - val_mae: 0.0032\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.4623e-04 - mae: 0.0039 - val_loss: 4.3688e-04 - val_mae: 0.0032\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.4270e-04 - mae: 0.0039 - val_loss: 4.3547e-04 - val_mae: 0.0034\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.4091e-04 - mae: 0.0041 - val_loss: 4.3187e-04 - val_mae: 0.0035\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.3555e-04 - mae: 0.0040 - val_loss: 4.2345e-04 - val_mae: 0.0030\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.3069e-04 - mae: 0.0039 - val_loss: 4.2097e-04 - val_mae: 0.0032\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.2924e-04 - mae: 0.0040 - val_loss: 4.2192e-04 - val_mae: 0.0035\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 4.2253e-04 - mae: 0.0038 - val_loss: 4.2124e-04 - val_mae: 0.0039\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.2078e-04 - mae: 0.0039 - val_loss: 4.1412e-04 - val_mae: 0.0035\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.1582e-04 - mae: 0.0038 - val_loss: 4.3641e-04 - val_mae: 0.0057\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.1402e-04 - mae: 0.0040 - val_loss: 4.0507e-04 - val_mae: 0.0033\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.0793e-04 - mae: 0.0037 - val_loss: 3.9968e-04 - val_mae: 0.0031\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 4.0664e-04 - mae: 0.0039 - val_loss: 3.9633e-04 - val_mae: 0.0030\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.0357e-04 - mae: 0.0039 - val_loss: 3.9793e-04 - val_mae: 0.0036\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.9941e-04 - mae: 0.0038 - val_loss: 3.9435e-04 - val_mae: 0.0034\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.9703e-04 - mae: 0.0039 - val_loss: 4.1260e-04 - val_mae: 0.0055\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9533e-04 - mae: 0.0040 - val_loss: 3.8516e-04 - val_mae: 0.0031\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.8927e-04 - mae: 0.0038 - val_loss: 3.8060e-04 - val_mae: 0.0029\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.8894e-04 - mae: 0.0039 - val_loss: 3.8110e-04 - val_mae: 0.0033\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.8543e-04 - mae: 0.0039 - val_loss: 3.7754e-04 - val_mae: 0.0033\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.8554e-04 - mae: 0.0041 - val_loss: 3.8237e-04 - val_mae: 0.0040\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.8100e-04 - mae: 0.0040 - val_loss: 3.7323e-04 - val_mae: 0.0033\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.7703e-04 - mae: 0.0038 - val_loss: 3.7521e-04 - val_mae: 0.0038\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.7577e-04 - mae: 0.0039 - val_loss: 3.6517e-04 - val_mae: 0.0031\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.7265e-04 - mae: 0.0039 - val_loss: 3.7358e-04 - val_mae: 0.0042\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.7068e-04 - mae: 0.0039 - val_loss: 3.6435e-04 - val_mae: 0.0034\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.6609e-04 - mae: 0.0037 - val_loss: 3.6743e-04 - val_mae: 0.0041\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.6404e-04 - mae: 0.0038 - val_loss: 3.5874e-04 - val_mae: 0.0035\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6384e-04 - mae: 0.0040 - val_loss: 3.5307e-04 - val_mae: 0.0030\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.6104e-04 - mae: 0.0039 - val_loss: 3.5052e-04 - val_mae: 0.0030\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5791e-04 - mae: 0.0039 - val_loss: 3.7227e-04 - val_mae: 0.0054\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5516e-04 - mae: 0.0038 - val_loss: 3.4682e-04 - val_mae: 0.0031\n",
      "Epoch 172/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.5360e-04 - mae: 0.0039 - val_loss: 3.4892e-04 - val_mae: 0.0035\n",
      "Epoch 173/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5170e-04 - mae: 0.0039 - val_loss: 3.4325e-04 - val_mae: 0.0033\n",
      "Epoch 174/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.4729e-04 - mae: 0.0037 - val_loss: 4.0332e-04 - val_mae: 0.0069\n",
      "Epoch 175/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.4930e-04 - mae: 0.0040 - val_loss: 3.4678e-04 - val_mae: 0.0039\n",
      "Epoch 176/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.4681e-04 - mae: 0.0040 - val_loss: 3.4064e-04 - val_mae: 0.0036\n",
      "Epoch 177/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.4179e-04 - mae: 0.0038 - val_loss: 3.4054e-04 - val_mae: 0.0038\n",
      "Epoch 178/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.4202e-04 - mae: 0.0039 - val_loss: 3.3402e-04 - val_mae: 0.0032\n",
      "Epoch 179/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.3761e-04 - mae: 0.0037 - val_loss: 3.3457e-04 - val_mae: 0.0037\n",
      "Epoch 180/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.3802e-04 - mae: 0.0039 - val_loss: 3.4440e-04 - val_mae: 0.0047\n",
      "Epoch 181/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.3572e-04 - mae: 0.0039 - val_loss: 3.5361e-04 - val_mae: 0.0057\n",
      "Epoch 182/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.3280e-04 - mae: 0.0038 - val_loss: 3.2435e-04 - val_mae: 0.0030\n",
      "Epoch 183/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.2995e-04 - mae: 0.0037 - val_loss: 3.3386e-04 - val_mae: 0.0043\n",
      "Epoch 184/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.3038e-04 - mae: 0.0039 - val_loss: 3.3878e-04 - val_mae: 0.0050\n",
      "Epoch 185/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.2758e-04 - mae: 0.0038 - val_loss: 3.2656e-04 - val_mae: 0.0039\n",
      "Epoch 186/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2525e-04 - mae: 0.0038 - val_loss: 3.2280e-04 - val_mae: 0.0036\n",
      "Epoch 187/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2368e-04 - mae: 0.0038 - val_loss: 3.1528e-04 - val_mae: 0.0030\n",
      "Epoch 188/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.2109e-04 - mae: 0.0037 - val_loss: 3.1775e-04 - val_mae: 0.0035\n",
      "Epoch 189/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2163e-04 - mae: 0.0040 - val_loss: 3.1902e-04 - val_mae: 0.0038\n",
      "Epoch 190/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.1867e-04 - mae: 0.0039 - val_loss: 3.2141e-04 - val_mae: 0.0039\n",
      "Epoch 191/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.1689e-04 - mae: 0.0038 - val_loss: 3.0917e-04 - val_mae: 0.0032\n",
      "Epoch 192/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 3.1594e-04 - mae: 0.0039 - val_loss: 3.1972e-04 - val_mae: 0.0040\n",
      "Epoch 193/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.1195e-04 - mae: 0.0037 - val_loss: 3.1142e-04 - val_mae: 0.0038\n",
      "Epoch 194/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.1151e-04 - mae: 0.0038 - val_loss: 3.0523e-04 - val_mae: 0.0033\n",
      "Epoch 195/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.1056e-04 - mae: 0.0039 - val_loss: 3.2571e-04 - val_mae: 0.0051\n",
      "Epoch 196/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.0973e-04 - mae: 0.0039 - val_loss: 3.0550e-04 - val_mae: 0.0036\n",
      "Epoch 197/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.0611e-04 - mae: 0.0038 - val_loss: 3.0886e-04 - val_mae: 0.0040\n",
      "Epoch 198/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.0602e-04 - mae: 0.0039 - val_loss: 3.1028e-04 - val_mae: 0.0042\n",
      "Epoch 199/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.0387e-04 - mae: 0.0038 - val_loss: 3.0089e-04 - val_mae: 0.0036\n",
      "Epoch 200/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.0232e-04 - mae: 0.0038 - val_loss: 3.0639e-04 - val_mae: 0.0044\n",
      "Epoch 201/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 3.0176e-04 - mae: 0.0039 - val_loss: 2.9481e-04 - val_mae: 0.0034\n",
      "Epoch 202/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.9852e-04 - mae: 0.0037 - val_loss: 2.9696e-04 - val_mae: 0.0036\n",
      "Epoch 203/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.9844e-04 - mae: 0.0039 - val_loss: 2.9437e-04 - val_mae: 0.0035\n",
      "Epoch 204/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.9571e-04 - mae: 0.0037 - val_loss: 2.9799e-04 - val_mae: 0.0040\n",
      "Epoch 205/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.9563e-04 - mae: 0.0038 - val_loss: 2.9554e-04 - val_mae: 0.0041\n",
      "Epoch 206/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.9413e-04 - mae: 0.0038 - val_loss: 2.9393e-04 - val_mae: 0.0040\n",
      "Epoch 207/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.9133e-04 - mae: 0.0037 - val_loss: 2.9688e-04 - val_mae: 0.0042\n",
      "Epoch 208/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.9407e-04 - mae: 0.0040 - val_loss: 2.8862e-04 - val_mae: 0.0036\n",
      "Epoch 209/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.8823e-04 - mae: 0.0037 - val_loss: 2.8535e-04 - val_mae: 0.0036\n",
      "Epoch 210/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8885e-04 - mae: 0.0038 - val_loss: 2.8484e-04 - val_mae: 0.0035\n",
      "Epoch 211/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.8879e-04 - mae: 0.0039 - val_loss: 2.8147e-04 - val_mae: 0.0034\n",
      "Epoch 212/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.8567e-04 - mae: 0.0037 - val_loss: 2.9940e-04 - val_mae: 0.0049\n",
      "Epoch 213/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.8659e-04 - mae: 0.0039 - val_loss: 2.8354e-04 - val_mae: 0.0038\n",
      "Epoch 214/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.8486e-04 - mae: 0.0039 - val_loss: 2.8508e-04 - val_mae: 0.0040\n",
      "Epoch 215/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.8139e-04 - mae: 0.0037 - val_loss: 2.7797e-04 - val_mae: 0.0034\n",
      "Epoch 216/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.8147e-04 - mae: 0.0038 - val_loss: 2.7372e-04 - val_mae: 0.0031\n",
      "Epoch 217/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.7985e-04 - mae: 0.0037 - val_loss: 2.7880e-04 - val_mae: 0.0037\n",
      "Epoch 218/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.8195e-04 - mae: 0.0040 - val_loss: 3.0339e-04 - val_mae: 0.0060\n",
      "Epoch 219/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7701e-04 - mae: 0.0037 - val_loss: 2.7081e-04 - val_mae: 0.0031\n",
      "Epoch 220/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.7680e-04 - mae: 0.0037 - val_loss: 2.7298e-04 - val_mae: 0.0035\n",
      "Epoch 221/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.7699e-04 - mae: 0.0039 - val_loss: 2.8297e-04 - val_mae: 0.0045\n",
      "Epoch 222/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7412e-04 - mae: 0.0037 - val_loss: 2.6995e-04 - val_mae: 0.0034\n",
      "Epoch 223/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.7526e-04 - mae: 0.0039 - val_loss: 2.8667e-04 - val_mae: 0.0049\n",
      "Epoch 224/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.7447e-04 - mae: 0.0039 - val_loss: 2.6892e-04 - val_mae: 0.0034\n",
      "Epoch 225/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.7134e-04 - mae: 0.0037 - val_loss: 2.6997e-04 - val_mae: 0.0037\n",
      "Epoch 226/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.7154e-04 - mae: 0.0038 - val_loss: 3.1220e-04 - val_mae: 0.0071\n",
      "Epoch 227/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.7009e-04 - mae: 0.0038 - val_loss: 2.6600e-04 - val_mae: 0.0036\n",
      "Epoch 228/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.7019e-04 - mae: 0.0039 - val_loss: 2.6259e-04 - val_mae: 0.0032\n",
      "Epoch 229/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.6826e-04 - mae: 0.0038 - val_loss: 2.7047e-04 - val_mae: 0.0040\n",
      "Epoch 230/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.6813e-04 - mae: 0.0038 - val_loss: 2.8636e-04 - val_mae: 0.0057\n",
      "Epoch 231/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.6513e-04 - mae: 0.0037 - val_loss: 2.6052e-04 - val_mae: 0.0034\n",
      "Epoch 232/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.6568e-04 - mae: 0.0038 - val_loss: 2.5972e-04 - val_mae: 0.0033\n",
      "Epoch 233/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.6549e-04 - mae: 0.0038 - val_loss: 2.6711e-04 - val_mae: 0.0041\n",
      "Epoch 234/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.6334e-04 - mae: 0.0037 - val_loss: 2.7735e-04 - val_mae: 0.0052\n",
      "Epoch 235/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.6453e-04 - mae: 0.0039 - val_loss: 2.6115e-04 - val_mae: 0.0037\n",
      "Epoch 236/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.6114e-04 - mae: 0.0037 - val_loss: 2.5845e-04 - val_mae: 0.0035\n",
      "Epoch 237/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.6080e-04 - mae: 0.0037 - val_loss: 2.5305e-04 - val_mae: 0.0030\n",
      "Epoch 238/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.6012e-04 - mae: 0.0037 - val_loss: 2.8142e-04 - val_mae: 0.0057\n",
      "Epoch 239/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.6135e-04 - mae: 0.0039 - val_loss: 2.5259e-04 - val_mae: 0.0031\n",
      "Epoch 240/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.5847e-04 - mae: 0.0037 - val_loss: 2.5165e-04 - val_mae: 0.0031\n",
      "Epoch 241/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.5836e-04 - mae: 0.0038 - val_loss: 2.7317e-04 - val_mae: 0.0049\n",
      "Epoch 242/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.5819e-04 - mae: 0.0038 - val_loss: 2.5784e-04 - val_mae: 0.0040\n",
      "Epoch 243/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.5660e-04 - mae: 0.0038 - val_loss: 2.5073e-04 - val_mae: 0.0032\n",
      "Epoch 244/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.5579e-04 - mae: 0.0038 - val_loss: 2.4902e-04 - val_mae: 0.0032\n",
      "Epoch 245/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.5360e-04 - mae: 0.0037 - val_loss: 2.5015e-04 - val_mae: 0.0034\n",
      "Epoch 246/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.5449e-04 - mae: 0.0038 - val_loss: 2.4675e-04 - val_mae: 0.0031\n",
      "Epoch 247/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.5260e-04 - mae: 0.0037 - val_loss: 2.5311e-04 - val_mae: 0.0039\n",
      "Epoch 248/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.5198e-04 - mae: 0.0037 - val_loss: 2.4427e-04 - val_mae: 0.0030\n",
      "Epoch 249/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 2.5157e-04 - mae: 0.0038 - val_loss: 2.5877e-04 - val_mae: 0.0048\n",
      "Epoch 250/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.5114e-04 - mae: 0.0038 - val_loss: 2.5955e-04 - val_mae: 0.0046\n",
      "Epoch 251/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.5035e-04 - mae: 0.0038 - val_loss: 2.5100e-04 - val_mae: 0.0041\n",
      "Epoch 252/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.4868e-04 - mae: 0.0037 - val_loss: 2.5578e-04 - val_mae: 0.0045\n",
      "Epoch 253/1000\n",
      "629/630 [============================>.] - ETA: 0s - loss: 2.4918e-04 - mae: 0.0038Restoring model weights from the end of the best epoch: 248.\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 2.4916e-04 - mae: 0.0038 - val_loss: 2.4741e-04 - val_mae: 0.0037\n",
      "Epoch 253: early stopping\n",
      "Training für Fold 4...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 5s 6ms/step - loss: 0.0409 - mae: 0.0543 - val_loss: 0.0302 - val_mae: 0.0258\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0279 - mae: 0.0181 - val_loss: 0.0262 - val_mae: 0.0136\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0251 - mae: 0.0126 - val_loss: 0.0242 - val_mae: 0.0111\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0234 - mae: 0.0101 - val_loss: 0.0227 - val_mae: 0.0106\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0221 - mae: 0.0095 - val_loss: 0.0214 - val_mae: 0.0081\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0209 - mae: 0.0096 - val_loss: 0.0203 - val_mae: 0.0084\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 0.0198 - mae: 0.0086 - val_loss: 0.0192 - val_mae: 0.0081\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0187 - mae: 0.0087 - val_loss: 0.0182 - val_mae: 0.0095\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0177 - mae: 0.0086 - val_loss: 0.0172 - val_mae: 0.0075\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0167 - mae: 0.0080 - val_loss: 0.0163 - val_mae: 0.0081\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0158 - mae: 0.0076 - val_loss: 0.0154 - val_mae: 0.0077\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0150 - mae: 0.0078 - val_loss: 0.0145 - val_mae: 0.0068\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0142 - mae: 0.0080 - val_loss: 0.0138 - val_mae: 0.0074\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0134 - mae: 0.0077 - val_loss: 0.0131 - val_mae: 0.0076\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0127 - mae: 0.0068 - val_loss: 0.0124 - val_mae: 0.0080\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0121 - mae: 0.0068 - val_loss: 0.0118 - val_mae: 0.0079\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0115 - mae: 0.0067 - val_loss: 0.0112 - val_mae: 0.0074\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0109 - mae: 0.0068 - val_loss: 0.0106 - val_mae: 0.0057\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0104 - mae: 0.0067 - val_loss: 0.0101 - val_mae: 0.0056\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 0.0099 - mae: 0.0065 - val_loss: 0.0097 - val_mae: 0.0066\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0094 - mae: 0.0062 - val_loss: 0.0092 - val_mae: 0.0050\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0090 - mae: 0.0064 - val_loss: 0.0088 - val_mae: 0.0056\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0086 - mae: 0.0060 - val_loss: 0.0084 - val_mae: 0.0070\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0082 - mae: 0.0059 - val_loss: 0.0080 - val_mae: 0.0064\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0078 - mae: 0.0058 - val_loss: 0.0076 - val_mae: 0.0057\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 4s 6ms/step - loss: 0.0075 - mae: 0.0059 - val_loss: 0.0073 - val_mae: 0.0053\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 3s 6ms/step - loss: 0.0071 - mae: 0.0059 - val_loss: 0.0070 - val_mae: 0.0049\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0068 - mae: 0.0055 - val_loss: 0.0067 - val_mae: 0.0061\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0065 - mae: 0.0056 - val_loss: 0.0064 - val_mae: 0.0049\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0063 - mae: 0.0059 - val_loss: 0.0061 - val_mae: 0.0059\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0060 - mae: 0.0053 - val_loss: 0.0059 - val_mae: 0.0067\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0057 - mae: 0.0057 - val_loss: 0.0056 - val_mae: 0.0073\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0055 - mae: 0.0054 - val_loss: 0.0054 - val_mae: 0.0045\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0053 - mae: 0.0056 - val_loss: 0.0051 - val_mae: 0.0049\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0050 - mae: 0.0054 - val_loss: 0.0049 - val_mae: 0.0048\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0048 - mae: 0.0053 - val_loss: 0.0047 - val_mae: 0.0056\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0046 - mae: 0.0054 - val_loss: 0.0045 - val_mae: 0.0042\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0045 - mae: 0.0051 - val_loss: 0.0043 - val_mae: 0.0043\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0043 - mae: 0.0053 - val_loss: 0.0042 - val_mae: 0.0050\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0041 - mae: 0.0052 - val_loss: 0.0040 - val_mae: 0.0047\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0039 - mae: 0.0052 - val_loss: 0.0039 - val_mae: 0.0061\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0038 - mae: 0.0052 - val_loss: 0.0037 - val_mae: 0.0043\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0036 - mae: 0.0050 - val_loss: 0.0036 - val_mae: 0.0049\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0035 - mae: 0.0050 - val_loss: 0.0034 - val_mae: 0.0042\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0034 - mae: 0.0049 - val_loss: 0.0033 - val_mae: 0.0052\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0032 - mae: 0.0050 - val_loss: 0.0032 - val_mae: 0.0048\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0031 - mae: 0.0049 - val_loss: 0.0031 - val_mae: 0.0044\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0030 - mae: 0.0049 - val_loss: 0.0030 - val_mae: 0.0069\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0029 - mae: 0.0047 - val_loss: 0.0028 - val_mae: 0.0057\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0028 - mae: 0.0049 - val_loss: 0.0027 - val_mae: 0.0057\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0027 - mae: 0.0049 - val_loss: 0.0026 - val_mae: 0.0049\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0026 - mae: 0.0050 - val_loss: 0.0026 - val_mae: 0.0064\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0025 - mae: 0.0046 - val_loss: 0.0025 - val_mae: 0.0045\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0024 - mae: 0.0048 - val_loss: 0.0024 - val_mae: 0.0050\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0023 - mae: 0.0047 - val_loss: 0.0023 - val_mae: 0.0048\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0023 - mae: 0.0046 - val_loss: 0.0022 - val_mae: 0.0039\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0022 - mae: 0.0046 - val_loss: 0.0022 - val_mae: 0.0072\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0021 - val_mae: 0.0053\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0021 - mae: 0.0045 - val_loss: 0.0020 - val_mae: 0.0039\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0020 - mae: 0.0047 - val_loss: 0.0020 - val_mae: 0.0063\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0043\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0066\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0018 - mae: 0.0044 - val_loss: 0.0018 - val_mae: 0.0037\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0018 - mae: 0.0045 - val_loss: 0.0017 - val_mae: 0.0039\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0017 - mae: 0.0048 - val_loss: 0.0017 - val_mae: 0.0055\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0047\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0016 - mae: 0.0043 - val_loss: 0.0016 - val_mae: 0.0044\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0037\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0015 - mae: 0.0044 - val_loss: 0.0015 - val_mae: 0.0059\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0054\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0082\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0037\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0048\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0039\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0054\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0034\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0042 - val_loss: 0.0012 - val_mae: 0.0039\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0036\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0042 - val_loss: 0.0012 - val_mae: 0.0039\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0040\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0041 - val_loss: 0.0011 - val_mae: 0.0048\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0047\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0069\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0050\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0010 - mae: 0.0043 - val_loss: 0.0010 - val_mae: 0.0035\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0010 - mae: 0.0041 - val_loss: 0.0010 - val_mae: 0.0043\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.9980e-04 - mae: 0.0043 - val_loss: 9.8248e-04 - val_mae: 0.0039\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.7679e-04 - mae: 0.0041 - val_loss: 9.6000e-04 - val_mae: 0.0036\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.5770e-04 - mae: 0.0042 - val_loss: 9.4008e-04 - val_mae: 0.0037\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.3954e-04 - mae: 0.0043 - val_loss: 9.1889e-04 - val_mae: 0.0034\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.2043e-04 - mae: 0.0042 - val_loss: 9.0310e-04 - val_mae: 0.0037\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.0368e-04 - mae: 0.0043 - val_loss: 8.8338e-04 - val_mae: 0.0034\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.8445e-04 - mae: 0.0042 - val_loss: 8.8637e-04 - val_mae: 0.0052\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.6783e-04 - mae: 0.0041 - val_loss: 8.5903e-04 - val_mae: 0.0042\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.5356e-04 - mae: 0.0043 - val_loss: 8.4528e-04 - val_mae: 0.0044\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.3634e-04 - mae: 0.0042 - val_loss: 8.2073e-04 - val_mae: 0.0036\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.2363e-04 - mae: 0.0043 - val_loss: 8.1245e-04 - val_mae: 0.0042\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.0503e-04 - mae: 0.0040 - val_loss: 8.1128e-04 - val_mae: 0.0053\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.9096e-04 - mae: 0.0040 - val_loss: 7.8137e-04 - val_mae: 0.0040\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.8031e-04 - mae: 0.0043 - val_loss: 7.6843e-04 - val_mae: 0.0040\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.6525e-04 - mae: 0.0042 - val_loss: 7.5802e-04 - val_mae: 0.0044\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.5243e-04 - mae: 0.0042 - val_loss: 7.4036e-04 - val_mae: 0.0039\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.3836e-04 - mae: 0.0041 - val_loss: 7.2340e-04 - val_mae: 0.0033\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.2840e-04 - mae: 0.0043 - val_loss: 7.2667e-04 - val_mae: 0.0047\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.1362e-04 - mae: 0.0040 - val_loss: 7.0544e-04 - val_mae: 0.0040\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.0343e-04 - mae: 0.0041 - val_loss: 7.5538e-04 - val_mae: 0.0083\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.9318e-04 - mae: 0.0042 - val_loss: 7.3376e-04 - val_mae: 0.0075\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.8199e-04 - mae: 0.0041 - val_loss: 6.7989e-04 - val_mae: 0.0045\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.7218e-04 - mae: 0.0041 - val_loss: 6.5972e-04 - val_mae: 0.0036\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.6160e-04 - mae: 0.0041 - val_loss: 6.6645e-04 - val_mae: 0.0049\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.5134e-04 - mae: 0.0041 - val_loss: 6.4501e-04 - val_mae: 0.0041\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.3963e-04 - mae: 0.0039 - val_loss: 6.3323e-04 - val_mae: 0.0038\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.3219e-04 - mae: 0.0041 - val_loss: 6.2638e-04 - val_mae: 0.0041\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.2466e-04 - mae: 0.0042 - val_loss: 6.1268e-04 - val_mae: 0.0036\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.1578e-04 - mae: 0.0042 - val_loss: 6.0058e-04 - val_mae: 0.0033\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.0676e-04 - mae: 0.0041 - val_loss: 5.9485e-04 - val_mae: 0.0035\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.9823e-04 - mae: 0.0040 - val_loss: 5.8601e-04 - val_mae: 0.0035\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.8992e-04 - mae: 0.0040 - val_loss: 6.0880e-04 - val_mae: 0.0060\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.8124e-04 - mae: 0.0039 - val_loss: 5.7069e-04 - val_mae: 0.0035\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.7761e-04 - mae: 0.0042 - val_loss: 5.6880e-04 - val_mae: 0.0040\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.6683e-04 - mae: 0.0040 - val_loss: 5.7074e-04 - val_mae: 0.0049\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.6053e-04 - mae: 0.0040 - val_loss: 5.6344e-04 - val_mae: 0.0048\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.5493e-04 - mae: 0.0041 - val_loss: 5.5324e-04 - val_mae: 0.0046\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.4767e-04 - mae: 0.0041 - val_loss: 5.3503e-04 - val_mae: 0.0034\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.4248e-04 - mae: 0.0042 - val_loss: 5.2858e-04 - val_mae: 0.0033\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.3270e-04 - mae: 0.0039 - val_loss: 5.2292e-04 - val_mae: 0.0034\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.3027e-04 - mae: 0.0042 - val_loss: 5.3610e-04 - val_mae: 0.0052\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.2133e-04 - mae: 0.0040 - val_loss: 5.1290e-04 - val_mae: 0.0036\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.1662e-04 - mae: 0.0040 - val_loss: 5.2812e-04 - val_mae: 0.0055\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.0958e-04 - mae: 0.0040 - val_loss: 5.0026e-04 - val_mae: 0.0034\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.0434e-04 - mae: 0.0040 - val_loss: 4.9765e-04 - val_mae: 0.0038\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.9822e-04 - mae: 0.0039 - val_loss: 4.8694e-04 - val_mae: 0.0033\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.9500e-04 - mae: 0.0041 - val_loss: 4.8095e-04 - val_mae: 0.0032\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.8646e-04 - mae: 0.0039 - val_loss: 4.9507e-04 - val_mae: 0.0047\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.8531e-04 - mae: 0.0042 - val_loss: 4.7822e-04 - val_mae: 0.0039\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.7685e-04 - mae: 0.0039 - val_loss: 4.7267e-04 - val_mae: 0.0039\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.7426e-04 - mae: 0.0041 - val_loss: 4.6458e-04 - val_mae: 0.0035\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.6883e-04 - mae: 0.0040 - val_loss: 4.6789e-04 - val_mae: 0.0043\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.6466e-04 - mae: 0.0041 - val_loss: 4.6036e-04 - val_mae: 0.0040\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.6065e-04 - mae: 0.0041 - val_loss: 4.4866e-04 - val_mae: 0.0033\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.5363e-04 - mae: 0.0039 - val_loss: 4.4278e-04 - val_mae: 0.0031\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.4989e-04 - mae: 0.0039 - val_loss: 4.4274e-04 - val_mae: 0.0036\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.4676e-04 - mae: 0.0040 - val_loss: 4.3685e-04 - val_mae: 0.0034\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.4027e-04 - mae: 0.0038 - val_loss: 4.2986e-04 - val_mae: 0.0031\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.3999e-04 - mae: 0.0041 - val_loss: 4.2905e-04 - val_mae: 0.0035\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.3448e-04 - mae: 0.0040 - val_loss: 4.3005e-04 - val_mae: 0.0040\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.2848e-04 - mae: 0.0039 - val_loss: 4.1753e-04 - val_mae: 0.0031\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.2450e-04 - mae: 0.0038 - val_loss: 4.2204e-04 - val_mae: 0.0040\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.2206e-04 - mae: 0.0040 - val_loss: 4.1368e-04 - val_mae: 0.0035\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.1796e-04 - mae: 0.0039 - val_loss: 4.0975e-04 - val_mae: 0.0034\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.1418e-04 - mae: 0.0039 - val_loss: 4.0735e-04 - val_mae: 0.0037\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.1186e-04 - mae: 0.0040 - val_loss: 4.0432e-04 - val_mae: 0.0036\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.0776e-04 - mae: 0.0040 - val_loss: 3.9936e-04 - val_mae: 0.0036\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.0421e-04 - mae: 0.0039 - val_loss: 3.9579e-04 - val_mae: 0.0035\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.0001e-04 - mae: 0.0039 - val_loss: 3.9366e-04 - val_mae: 0.0036\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9751e-04 - mae: 0.0039 - val_loss: 3.8777e-04 - val_mae: 0.0032\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9292e-04 - mae: 0.0038 - val_loss: 3.8586e-04 - val_mae: 0.0034\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9038e-04 - mae: 0.0039 - val_loss: 3.9660e-04 - val_mae: 0.0047\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.8799e-04 - mae: 0.0039 - val_loss: 3.7717e-04 - val_mae: 0.0032\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.8511e-04 - mae: 0.0039 - val_loss: 3.7957e-04 - val_mae: 0.0037\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.8265e-04 - mae: 0.0040 - val_loss: 3.8257e-04 - val_mae: 0.0041\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.7990e-04 - mae: 0.0040 - val_loss: 3.6736e-04 - val_mae: 0.0030\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.7380e-04 - mae: 0.0037 - val_loss: 3.8393e-04 - val_mae: 0.0050\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.7210e-04 - mae: 0.0038 - val_loss: 3.6389e-04 - val_mae: 0.0033\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.7156e-04 - mae: 0.0039 - val_loss: 3.8154e-04 - val_mae: 0.0049\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6829e-04 - mae: 0.0039 - val_loss: 3.6032e-04 - val_mae: 0.0034\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6530e-04 - mae: 0.0039 - val_loss: 3.5701e-04 - val_mae: 0.0033\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6099e-04 - mae: 0.0037 - val_loss: 3.5745e-04 - val_mae: 0.0037\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6192e-04 - mae: 0.0040 - val_loss: 3.5415e-04 - val_mae: 0.0036\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5662e-04 - mae: 0.0038 - val_loss: 3.7095e-04 - val_mae: 0.0054\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5484e-04 - mae: 0.0038 - val_loss: 3.5645e-04 - val_mae: 0.0043\n",
      "Epoch 172/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5363e-04 - mae: 0.0039 - val_loss: 3.4591e-04 - val_mae: 0.0034\n",
      "Epoch 173/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.4967e-04 - mae: 0.0038 - val_loss: 3.4330e-04 - val_mae: 0.0034\n",
      "Epoch 174/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.4609e-04 - mae: 0.0037 - val_loss: 3.4410e-04 - val_mae: 0.0037\n",
      "Epoch 175/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.4500e-04 - mae: 0.0038 - val_loss: 3.4416e-04 - val_mae: 0.0039\n",
      "Epoch 176/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.4545e-04 - mae: 0.0040 - val_loss: 3.4296e-04 - val_mae: 0.0042\n",
      "Epoch 177/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.4068e-04 - mae: 0.0038 - val_loss: 3.4618e-04 - val_mae: 0.0045\n",
      "Epoch 178/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.3846e-04 - mae: 0.0038 - val_loss: 3.3474e-04 - val_mae: 0.0037\n",
      "Epoch 179/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.3880e-04 - mae: 0.0040 - val_loss: 3.3205e-04 - val_mae: 0.0035\n",
      "Epoch 180/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.3578e-04 - mae: 0.0039 - val_loss: 3.3294e-04 - val_mae: 0.0038\n",
      "Epoch 181/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.3185e-04 - mae: 0.0037 - val_loss: 3.2655e-04 - val_mae: 0.0033\n",
      "Epoch 182/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.3181e-04 - mae: 0.0039 - val_loss: 3.3600e-04 - val_mae: 0.0045\n",
      "Epoch 183/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2774e-04 - mae: 0.0037 - val_loss: 3.3016e-04 - val_mae: 0.0043\n",
      "Epoch 184/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2675e-04 - mae: 0.0038 - val_loss: 3.3670e-04 - val_mae: 0.0050\n",
      "Epoch 185/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2616e-04 - mae: 0.0039 - val_loss: 3.5295e-04 - val_mae: 0.0065\n",
      "Epoch 186/1000\n",
      "624/630 [============================>.] - ETA: 0s - loss: 3.2268e-04 - mae: 0.0038Restoring model weights from the end of the best epoch: 181.\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2275e-04 - mae: 0.0038 - val_loss: 3.3967e-04 - val_mae: 0.0051\n",
      "Epoch 186: early stopping\n",
      "Training für Fold 5...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 4s 5ms/step - loss: 0.0345 - mae: 0.0362 - val_loss: 0.0288 - val_mae: 0.0192\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0266 - mae: 0.0168 - val_loss: 0.0248 - val_mae: 0.0125\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0236 - mae: 0.0118 - val_loss: 0.0226 - val_mae: 0.0117\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0217 - mae: 0.0101 - val_loss: 0.0210 - val_mae: 0.0096\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0204 - mae: 0.0101 - val_loss: 0.0198 - val_mae: 0.0117\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0192 - mae: 0.0094 - val_loss: 0.0185 - val_mae: 0.0074\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0181 - mae: 0.0085 - val_loss: 0.0175 - val_mae: 0.0078\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0171 - mae: 0.0082 - val_loss: 0.0166 - val_mae: 0.0106\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0161 - mae: 0.0082 - val_loss: 0.0157 - val_mae: 0.0081\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0153 - mae: 0.0076 - val_loss: 0.0148 - val_mae: 0.0067\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0145 - mae: 0.0080 - val_loss: 0.0141 - val_mae: 0.0088\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0137 - mae: 0.0073 - val_loss: 0.0134 - val_mae: 0.0070\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0131 - mae: 0.0075 - val_loss: 0.0128 - val_mae: 0.0077\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0124 - mae: 0.0070 - val_loss: 0.0121 - val_mae: 0.0065\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0118 - mae: 0.0068 - val_loss: 0.0117 - val_mae: 0.0099\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0113 - mae: 0.0064 - val_loss: 0.0110 - val_mae: 0.0055\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0108 - mae: 0.0068 - val_loss: 0.0105 - val_mae: 0.0065\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0103 - mae: 0.0067 - val_loss: 0.0100 - val_mae: 0.0053\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0098 - mae: 0.0065 - val_loss: 0.0096 - val_mae: 0.0086\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0094 - mae: 0.0063 - val_loss: 0.0091 - val_mae: 0.0057\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0089 - mae: 0.0060 - val_loss: 0.0088 - val_mae: 0.0073\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0085 - mae: 0.0060 - val_loss: 0.0083 - val_mae: 0.0054\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0081 - mae: 0.0060 - val_loss: 0.0080 - val_mae: 0.0066\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0078 - mae: 0.0060 - val_loss: 0.0076 - val_mae: 0.0051\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0074 - mae: 0.0059 - val_loss: 0.0073 - val_mae: 0.0059\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0071 - mae: 0.0057 - val_loss: 0.0070 - val_mae: 0.0092\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0068 - mae: 0.0059 - val_loss: 0.0067 - val_mae: 0.0058\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0065 - mae: 0.0058 - val_loss: 0.0064 - val_mae: 0.0059\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0062 - mae: 0.0056 - val_loss: 0.0062 - val_mae: 0.0094\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0060 - mae: 0.0057 - val_loss: 0.0058 - val_mae: 0.0051\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0057 - mae: 0.0059 - val_loss: 0.0056 - val_mae: 0.0061\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0055 - mae: 0.0056 - val_loss: 0.0054 - val_mae: 0.0046\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0053 - mae: 0.0053 - val_loss: 0.0052 - val_mae: 0.0047\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0051 - mae: 0.0055 - val_loss: 0.0049 - val_mae: 0.0054\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0049 - mae: 0.0056 - val_loss: 0.0048 - val_mae: 0.0051\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0047 - mae: 0.0055 - val_loss: 0.0046 - val_mae: 0.0049\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0045 - mae: 0.0054 - val_loss: 0.0044 - val_mae: 0.0063\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0043 - mae: 0.0052 - val_loss: 0.0042 - val_mae: 0.0051\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0041 - mae: 0.0055 - val_loss: 0.0041 - val_mae: 0.0076\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0040 - mae: 0.0053 - val_loss: 0.0039 - val_mae: 0.0056\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0038 - mae: 0.0053 - val_loss: 0.0038 - val_mae: 0.0056\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0037 - mae: 0.0052 - val_loss: 0.0036 - val_mae: 0.0050\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0036 - mae: 0.0053 - val_loss: 0.0035 - val_mae: 0.0043\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0034 - mae: 0.0052 - val_loss: 0.0034 - val_mae: 0.0045\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0033 - mae: 0.0051 - val_loss: 0.0032 - val_mae: 0.0044\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0032 - mae: 0.0051 - val_loss: 0.0031 - val_mae: 0.0045\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0031 - mae: 0.0052 - val_loss: 0.0030 - val_mae: 0.0049\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0030 - mae: 0.0049 - val_loss: 0.0029 - val_mae: 0.0051\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0029 - mae: 0.0051 - val_loss: 0.0028 - val_mae: 0.0046\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0028 - mae: 0.0050 - val_loss: 0.0027 - val_mae: 0.0041\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0027 - mae: 0.0050 - val_loss: 0.0026 - val_mae: 0.0038\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0026 - mae: 0.0049 - val_loss: 0.0026 - val_mae: 0.0043\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0025 - mae: 0.0049 - val_loss: 0.0025 - val_mae: 0.0046\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0024 - mae: 0.0050 - val_loss: 0.0024 - val_mae: 0.0044\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0024 - mae: 0.0051 - val_loss: 0.0023 - val_mae: 0.0043\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0023 - mae: 0.0049 - val_loss: 0.0023 - val_mae: 0.0052\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0022 - mae: 0.0048 - val_loss: 0.0022 - val_mae: 0.0050\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0022 - mae: 0.0050 - val_loss: 0.0021 - val_mae: 0.0053\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0021 - val_mae: 0.0045\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0020 - val_mae: 0.0046\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0020 - val_mae: 0.0050\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0019 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0043\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0050\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0018 - mae: 0.0049 - val_loss: 0.0018 - val_mae: 0.0046\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0018 - mae: 0.0048 - val_loss: 0.0017 - val_mae: 0.0041\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0017 - mae: 0.0047 - val_loss: 0.0017 - val_mae: 0.0045\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0017 - mae: 0.0045 - val_loss: 0.0016 - val_mae: 0.0040\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0016 - mae: 0.0047 - val_loss: 0.0017 - val_mae: 0.0079\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0016 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0045\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0039\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0047\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0015 - mae: 0.0047 - val_loss: 0.0014 - val_mae: 0.0046\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0014 - mae: 0.0047 - val_loss: 0.0014 - val_mae: 0.0038\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0054\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0014 - mae: 0.0047 - val_loss: 0.0013 - val_mae: 0.0041\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0044\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0040\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0041\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0057\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0036\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0041\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0060\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0046 - val_loss: 0.0011 - val_mae: 0.0045\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0040\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0056\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0010 - val_mae: 0.0037\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0010 - mae: 0.0042 - val_loss: 0.0010 - val_mae: 0.0045\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0010 - mae: 0.0044 - val_loss: 0.0010 - val_mae: 0.0046\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.9066e-04 - mae: 0.0044 - val_loss: 9.7065e-04 - val_mae: 0.0035\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.7037e-04 - mae: 0.0043 - val_loss: 9.7138e-04 - val_mae: 0.0053\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.4986e-04 - mae: 0.0043 - val_loss: 9.3223e-04 - val_mae: 0.0035\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.3343e-04 - mae: 0.0044 - val_loss: 9.2215e-04 - val_mae: 0.0042\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 9.1383e-04 - mae: 0.0043 - val_loss: 9.0099e-04 - val_mae: 0.0039\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.9500e-04 - mae: 0.0042 - val_loss: 8.8364e-04 - val_mae: 0.0039\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.7891e-04 - mae: 0.0043 - val_loss: 8.7735e-04 - val_mae: 0.0046\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.6130e-04 - mae: 0.0042 - val_loss: 8.5601e-04 - val_mae: 0.0044\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.4823e-04 - mae: 0.0044 - val_loss: 8.3201e-04 - val_mae: 0.0037\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.2930e-04 - mae: 0.0042 - val_loss: 8.2951e-04 - val_mae: 0.0049\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.1780e-04 - mae: 0.0044 - val_loss: 8.0401e-04 - val_mae: 0.0038\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 8.0207e-04 - mae: 0.0043 - val_loss: 7.9146e-04 - val_mae: 0.0038\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.8884e-04 - mae: 0.0043 - val_loss: 7.7731e-04 - val_mae: 0.0038\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.7310e-04 - mae: 0.0041 - val_loss: 7.6014e-04 - val_mae: 0.0035\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.6336e-04 - mae: 0.0044 - val_loss: 7.4751e-04 - val_mae: 0.0035\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.4814e-04 - mae: 0.0042 - val_loss: 7.4107e-04 - val_mae: 0.0041\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.3300e-04 - mae: 0.0040 - val_loss: 7.2723e-04 - val_mae: 0.0040\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.2404e-04 - mae: 0.0042 - val_loss: 7.2506e-04 - val_mae: 0.0048\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.1112e-04 - mae: 0.0041 - val_loss: 7.1593e-04 - val_mae: 0.0050\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 7.0052e-04 - mae: 0.0042 - val_loss: 6.9477e-04 - val_mae: 0.0041\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.9084e-04 - mae: 0.0043 - val_loss: 6.8149e-04 - val_mae: 0.0039\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.7729e-04 - mae: 0.0040 - val_loss: 6.6829e-04 - val_mae: 0.0037\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.6951e-04 - mae: 0.0042 - val_loss: 6.5722e-04 - val_mae: 0.0035\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.5841e-04 - mae: 0.0042 - val_loss: 6.4992e-04 - val_mae: 0.0037\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.4774e-04 - mae: 0.0041 - val_loss: 6.3943e-04 - val_mae: 0.0037\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.3930e-04 - mae: 0.0042 - val_loss: 6.2760e-04 - val_mae: 0.0034\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.2808e-04 - mae: 0.0040 - val_loss: 6.2134e-04 - val_mae: 0.0038\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.1994e-04 - mae: 0.0041 - val_loss: 6.1546e-04 - val_mae: 0.0041\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.1231e-04 - mae: 0.0042 - val_loss: 6.0469e-04 - val_mae: 0.0039\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.0593e-04 - mae: 0.0043 - val_loss: 5.9552e-04 - val_mae: 0.0037\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.9637e-04 - mae: 0.0042 - val_loss: 6.0588e-04 - val_mae: 0.0052\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.8688e-04 - mae: 0.0041 - val_loss: 5.8247e-04 - val_mae: 0.0040\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.8000e-04 - mae: 0.0041 - val_loss: 5.7435e-04 - val_mae: 0.0038\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.7325e-04 - mae: 0.0041 - val_loss: 5.6626e-04 - val_mae: 0.0038\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.6523e-04 - mae: 0.0041 - val_loss: 5.5522e-04 - val_mae: 0.0035\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.5864e-04 - mae: 0.0041 - val_loss: 5.5143e-04 - val_mae: 0.0037\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.5231e-04 - mae: 0.0041 - val_loss: 5.4019e-04 - val_mae: 0.0033\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.4417e-04 - mae: 0.0040 - val_loss: 5.5069e-04 - val_mae: 0.0050\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.4105e-04 - mae: 0.0043 - val_loss: 5.3194e-04 - val_mae: 0.0037\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.2972e-04 - mae: 0.0039 - val_loss: 5.2273e-04 - val_mae: 0.0033\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.2660e-04 - mae: 0.0041 - val_loss: 5.3117e-04 - val_mae: 0.0048\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.1979e-04 - mae: 0.0041 - val_loss: 5.1176e-04 - val_mae: 0.0035\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.1361e-04 - mae: 0.0041 - val_loss: 5.0537e-04 - val_mae: 0.0035\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.0782e-04 - mae: 0.0040 - val_loss: 5.0403e-04 - val_mae: 0.0040\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.0111e-04 - mae: 0.0040 - val_loss: 4.9987e-04 - val_mae: 0.0039\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.9812e-04 - mae: 0.0042 - val_loss: 4.8785e-04 - val_mae: 0.0034\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.9129e-04 - mae: 0.0040 - val_loss: 4.8318e-04 - val_mae: 0.0034\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.8703e-04 - mae: 0.0041 - val_loss: 4.7992e-04 - val_mae: 0.0035\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.7899e-04 - mae: 0.0039 - val_loss: 4.9630e-04 - val_mae: 0.0054\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.7624e-04 - mae: 0.0040 - val_loss: 4.9143e-04 - val_mae: 0.0055\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.7052e-04 - mae: 0.0040 - val_loss: 4.6943e-04 - val_mae: 0.0041\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.6775e-04 - mae: 0.0041 - val_loss: 4.5740e-04 - val_mae: 0.0033\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.6056e-04 - mae: 0.0039 - val_loss: 4.5786e-04 - val_mae: 0.0039\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.5759e-04 - mae: 0.0041 - val_loss: 4.5100e-04 - val_mae: 0.0036\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.5392e-04 - mae: 0.0041 - val_loss: 4.5500e-04 - val_mae: 0.0043\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.4738e-04 - mae: 0.0039 - val_loss: 4.6046e-04 - val_mae: 0.0054\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.4287e-04 - mae: 0.0039 - val_loss: 4.3872e-04 - val_mae: 0.0036\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.3915e-04 - mae: 0.0039 - val_loss: 4.4142e-04 - val_mae: 0.0043\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.3512e-04 - mae: 0.0039 - val_loss: 4.3280e-04 - val_mae: 0.0040\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.3091e-04 - mae: 0.0040 - val_loss: 4.2674e-04 - val_mae: 0.0037\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.2607e-04 - mae: 0.0039 - val_loss: 4.3002e-04 - val_mae: 0.0043\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.2472e-04 - mae: 0.0041 - val_loss: 4.1714e-04 - val_mae: 0.0034\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.2095e-04 - mae: 0.0041 - val_loss: 4.1249e-04 - val_mae: 0.0034\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.1495e-04 - mae: 0.0039 - val_loss: 4.0666e-04 - val_mae: 0.0031\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.1228e-04 - mae: 0.0039 - val_loss: 4.1004e-04 - val_mae: 0.0037\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.0833e-04 - mae: 0.0039 - val_loss: 4.0116e-04 - val_mae: 0.0033\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.0657e-04 - mae: 0.0040 - val_loss: 4.0266e-04 - val_mae: 0.0039\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.0310e-04 - mae: 0.0040 - val_loss: 3.9553e-04 - val_mae: 0.0034\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9855e-04 - mae: 0.0039 - val_loss: 3.9334e-04 - val_mae: 0.0035\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9628e-04 - mae: 0.0040 - val_loss: 3.8860e-04 - val_mae: 0.0032\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9372e-04 - mae: 0.0040 - val_loss: 3.8488e-04 - val_mae: 0.0032\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.8877e-04 - mae: 0.0039 - val_loss: 4.2285e-04 - val_mae: 0.0066\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.8704e-04 - mae: 0.0040 - val_loss: 3.8050e-04 - val_mae: 0.0034\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.8371e-04 - mae: 0.0039 - val_loss: 3.7662e-04 - val_mae: 0.0033\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.8283e-04 - mae: 0.0041 - val_loss: 3.8135e-04 - val_mae: 0.0042\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.7895e-04 - mae: 0.0040 - val_loss: 3.7541e-04 - val_mae: 0.0038\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.7495e-04 - mae: 0.0039 - val_loss: 3.8732e-04 - val_mae: 0.0051\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.7323e-04 - mae: 0.0040 - val_loss: 3.7325e-04 - val_mae: 0.0040\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6872e-04 - mae: 0.0038 - val_loss: 3.7189e-04 - val_mae: 0.0043\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6960e-04 - mae: 0.0041 - val_loss: 3.6422e-04 - val_mae: 0.0037\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6505e-04 - mae: 0.0039 - val_loss: 3.5782e-04 - val_mae: 0.0033\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6138e-04 - mae: 0.0038 - val_loss: 3.6826e-04 - val_mae: 0.0045\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6069e-04 - mae: 0.0040 - val_loss: 3.6159e-04 - val_mae: 0.0042\n",
      "Epoch 172/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5639e-04 - mae: 0.0038 - val_loss: 3.6346e-04 - val_mae: 0.0044\n",
      "Epoch 173/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5516e-04 - mae: 0.0039 - val_loss: 3.7546e-04 - val_mae: 0.0058\n",
      "Epoch 174/1000\n",
      "625/630 [============================>.] - ETA: 0s - loss: 3.5176e-04 - mae: 0.0038Restoring model weights from the end of the best epoch: 169.\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.5179e-04 - mae: 0.0039 - val_loss: 3.5878e-04 - val_mae: 0.0047\n",
      "Epoch 174: early stopping\n",
      "Durchschnittlicher Validation Loss: 0.0003099968133028597\n",
      "Durchschnittlicher Validation MAE: 0.0030166534706950188\n"
     ]
    }
   ],
   "source": [
    " # Initialisiere Listen, um Ergebnisse zu speichern\n",
    "val_loss_results = []\n",
    "val_mae_results = []\n",
    "\n",
    "# Funktion, um das Modell zu erstellen\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "\n",
    "                Dense(280, activation='relu', input_shape=(5,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "                Dense(136, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "                Dense(328, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "                Dense(328, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "                Dense(152, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "                Dense(104, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "                Dense(248, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "                Dense(152, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "                Dense(1 , activation = 'linear')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-Validation Konfiguration\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Leistungsüberwachung\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_scaled[train_index], y_train_scaled[val_index]\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "    print(f'Training für Fold {fold_no}...')\n",
    "    history = model.fit(X_train_fold, y_train_fold, batch_size=200, epochs=1000, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    # Speichere die Ergebnisse des aktuellen Folds\n",
    "    val_loss_results.append(min(history.history['val_loss']))\n",
    "    val_mae_results.append(min(history.history['val_mae']))\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "# Berechne den Durchschnitt über alle Folds\n",
    "average_val_loss = np.mean(val_loss_results)\n",
    "average_val_mae = np.mean(val_mae_results)\n",
    "\n",
    "# Umwandeln der Listen in Pandas DataFrames\n",
    "val_loss_df = pd.DataFrame(val_loss_results, columns=['Validation Loss'])\n",
    "val_mae_df = pd.DataFrame(val_mae_results, columns=['Validation MAE'])\n",
    "\n",
    "# Speichern der DataFrames in CSV-Dateien\n",
    "val_loss_df.to_csv('val_loss_results_D4_t_21_I_F_3.csv', index=False)\n",
    "val_mae_df.to_csv('val_mae_results_D4_t_21_I_F_3.csv', index=False)\n",
    "\n",
    "# Gib die durchschnittlichen Ergebnisse aus\n",
    "print(f'Durchschnittlicher Validation Loss: {average_val_loss}')\n",
    "print(f'Durchschnittlicher Validation MAE: {average_val_mae}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T23:33:54.066004900Z",
     "start_time": "2024-04-02T22:36:35.506083300Z"
    }
   },
   "id": "929336b1a7ac475d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703/703 - 1s - loss: 4.8648e-04 - mae: 0.0099 - 1s/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.00048648283700458705, 0.009938209317624569]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test_scaled, y_test_scaled, verbose=2)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:54:18.208481600Z",
     "start_time": "2024-04-03T08:54:17.104576800Z"
    }
   },
   "id": "4b02697cbcecd185"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bsp. Predicted: [465.5966] Actual: [452.59] \n",
      "Durchschnittliche Abweichung (MAE): [22.04214984]\n",
      "1.9741126350047247\n"
     ]
    }
   ],
   "source": [
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Führen Sie die Rücktransformation der skalierten Werte durch\n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[100]} Actual: {original_actual_values[100]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "\n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "\n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")\n",
    "\n",
    "errors = np.abs((original_actual_values - original_predicted_values) / original_actual_values)\n",
    "mape = np.mean(errors) * 100\n",
    "print(mape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:54:24.242567400Z",
     "start_time": "2024-04-03T08:54:23.036659800Z"
    }
   },
   "id": "a402d28abbd82f60"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Werte die kleiner sind: 99\n"
     ]
    },
    {
     "data": {
      "text/plain": "             Echt  Vorhergesagt  X-Koordinate  Y-Koordinate  Zeitpunkt  \\\n464   1203.249268       1243.30          0.45          0.10        1.0   \n465   1285.497681       1324.40          0.45          0.12        1.0   \n413   1209.738770       1248.50          0.40          0.10        1.0   \n515   1198.858154       1237.40          0.50          0.10        1.0   \n414   1291.585083       1330.00          0.40          0.12        1.0   \n...           ...           ...           ...           ...        ...   \n253    779.971313        752.88          0.20          0.98        1.0   \n304    779.003296        751.80          0.25          0.98        1.0   \n406    776.215698        749.00          0.35          0.98        1.0   \n1069   735.995361        708.11          1.00          0.98        1.0   \n355    778.476257        750.51          0.30          0.98        1.0   \n\n         Strom  Kraft  Differenz  \n464   0.333333   0.25 -40.050732  \n465   0.333333   0.25 -38.902319  \n413   0.333333   0.25 -38.761230  \n515   0.333333   0.25 -38.541846  \n414   0.333333   0.25 -38.414917  \n...        ...    ...        ...  \n253   0.333333   0.25  27.091313  \n304   0.333333   0.25  27.203296  \n406   0.333333   0.25  27.215698  \n1069  0.333333   0.25  27.885361  \n355   0.333333   0.25  27.966257  \n\n[1071 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Echt</th>\n      <th>Vorhergesagt</th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Differenz</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>464</th>\n      <td>1203.249268</td>\n      <td>1243.30</td>\n      <td>0.45</td>\n      <td>0.10</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>-40.050732</td>\n    </tr>\n    <tr>\n      <th>465</th>\n      <td>1285.497681</td>\n      <td>1324.40</td>\n      <td>0.45</td>\n      <td>0.12</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>-38.902319</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>1209.738770</td>\n      <td>1248.50</td>\n      <td>0.40</td>\n      <td>0.10</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>-38.761230</td>\n    </tr>\n    <tr>\n      <th>515</th>\n      <td>1198.858154</td>\n      <td>1237.40</td>\n      <td>0.50</td>\n      <td>0.10</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>-38.541846</td>\n    </tr>\n    <tr>\n      <th>414</th>\n      <td>1291.585083</td>\n      <td>1330.00</td>\n      <td>0.40</td>\n      <td>0.12</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>-38.414917</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>253</th>\n      <td>779.971313</td>\n      <td>752.88</td>\n      <td>0.20</td>\n      <td>0.98</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>27.091313</td>\n    </tr>\n    <tr>\n      <th>304</th>\n      <td>779.003296</td>\n      <td>751.80</td>\n      <td>0.25</td>\n      <td>0.98</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>27.203296</td>\n    </tr>\n    <tr>\n      <th>406</th>\n      <td>776.215698</td>\n      <td>749.00</td>\n      <td>0.35</td>\n      <td>0.98</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>27.215698</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>735.995361</td>\n      <td>708.11</td>\n      <td>1.00</td>\n      <td>0.98</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>27.885361</td>\n    </tr>\n    <tr>\n      <th>355</th>\n      <td>778.476257</td>\n      <td>750.51</td>\n      <td>0.30</td>\n      <td>0.98</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.25</td>\n      <td>27.966257</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'Echt': [val[0] for val in list2], 'Vorhergesagt': [val[0] for val in list1]})\n",
    "df_result['X-Koordinate'] = X_test_scaled[:, 0]\n",
    "df_result['Y-Koordinate'] = X_test_scaled[:, 1]\n",
    "df_result['Zeitpunkt'] = X_test_scaled[:, 2]\n",
    "df_result['Strom'] = X_test_scaled[:, 3]\n",
    "df_result['Kraft'] = X_test_scaled[:, 4]\n",
    "\n",
    "df_result['Differenz'] = abs(df_result['Echt'] - df_result['Vorhergesagt'])\n",
    "df_result['Differenz'].sort_values()\n",
    "sorted_df = df_result.sort_values(by= 'Differenz')\n",
    "Anzahl_Punkte = (sorted_df['Differenz'] < -20).sum()\n",
    "print(\"Anzahl der Werte die kleiner sind:\", Anzahl_Punkte)\n",
    "\n",
    "sorted_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T09:03:45.521822600Z",
     "start_time": "2024-04-02T09:03:45.434442Z"
    }
   },
   "id": "7ffe8ddf2200f429"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: [0.99905697]\n"
     ]
    }
   ],
   "source": [
    "#Berechnung der Auswertungsgröße R^2\n",
    "\n",
    "def calculate_r_squared(predicted, actual):\n",
    "    # Berechnung des Mittelwerts der tatsächlichen Werte\n",
    "    mean_actual = sum(actual) / len(actual)\n",
    "    \n",
    "    # Berechnung der totalen Summe der Quadrate (SST)\n",
    "    sst = sum((x - mean_actual) ** 2 for x in actual)\n",
    "    \n",
    "    # Berechnung der Summe der Quadrate der Residuen (SSE)\n",
    "    sse = sum((actual[i] - predicted[i]) ** 2 for i in range(len(actual)))\n",
    "    \n",
    "    # Berechnung des R^2-Wertes\n",
    "    r_squared = 1 - (sse / sst)\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "# Berechnung von R^2 mit den bereitgestellten Listen\n",
    "r_squared = calculate_r_squared(list1, list2)\n",
    "\n",
    "print(f\"R^2: {r_squared}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T10:25:07.467733900Z",
     "start_time": "2024-04-02T10:25:07.409070500Z"
    }
   },
   "id": "4c350477801f0961"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse = history.history['loss']\n",
    "val_mse = history.history['val_loss']\n",
    "epochs = range(1, len(mse) + 1)\n",
    "\n",
    "# MAE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mse, 'r', label='Training MSE')\n",
    "plt.plot(epochs, val_mse, 'b', label='Validation MSE')\n",
    "plt.title('Training and Validation MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T13:19:36.804139100Z",
     "start_time": "2024-03-20T13:19:36.515474400Z"
    }
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001B625802E50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 47\u001B[0m\n\u001B[0;32m     45\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(estimator\u001B[38;5;241m=\u001B[39mmodel, param_grid\u001B[38;5;241m=\u001B[39mparam_grid, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# Hinweis: Stellen Sie sicher, dass Ihre Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m grid_result \u001B[38;5;241m=\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_scaled\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# Beste Parameter und Score ausgeben\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBeste Parameter:\u001B[39m\u001B[38;5;124m\"\u001B[39m, grid_search\u001B[38;5;241m.\u001B[39mbest_params_)\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\base.py:1351\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1344\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1346\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1347\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1348\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1349\u001B[0m     )\n\u001B[0;32m   1350\u001B[0m ):\n\u001B[1;32m-> 1351\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    964\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    965\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    966\u001B[0m     )\n\u001B[0;32m    968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 970\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    974\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1525\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1526\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1527\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    908\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    909\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    910\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    911\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    912\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    913\u001B[0m         )\n\u001B[0;32m    914\u001B[0m     )\n\u001B[1;32m--> 916\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    920\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    922\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    924\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    930\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    931\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    932\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    934\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    935\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    936\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    937\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    938\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    939\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\joblib\\parallel.py:1952\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1946\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   1947\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   1948\u001B[0m \u001B[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001B[39;00m\n\u001B[0;32m   1949\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   1950\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1952\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\joblib\\parallel.py:1595\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1592\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1594\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1595\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1597\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1598\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1599\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1600\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1601\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\joblib\\parallel.py:1707\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1702\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1703\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1705\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1706\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1707\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1708\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1710\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1711\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1712\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def build_model(learning_rate=0.00001, activation='relu', regularization=0.00001, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(280, activation=activation, input_shape=(3,), kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(136, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(328, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(328, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(152, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(104, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(248, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(152, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Verwenden Sie eine Funktion, um das Modell zu instanziieren, für scikit-learn Wrapper\n",
    "model = KerasRegressor(model=build_model, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# Anpassung der Parameter im param_grid\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.00001],\n",
    "    'model__regularization': [0.00001],\n",
    "    'fit__batch_size': [16, 32, 64, 100, 200],\n",
    "    'fit__epochs': [100],\n",
    "    'model__dropout_rate' : [0.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "# Hinweis: Stellen Sie sicher, dass Ihre Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "# Beste Parameter und Score ausgeben\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "print(\"Beste Genauigkeit:\", grid_search.best_score_)\n",
    "\n",
    "with open(\"Gridsearch_D4_t_21_I_F_3.txt\", \"w\") as f:\n",
    "    f.write(f\"Beste Parameter: {grid_search.best_params_}\\n\")\n",
    "    f.write(f\"Beste Genauigkeit: {grid_search.best_score_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:15:54.960082300Z",
     "start_time": "2024-03-18T11:56:34.729533600Z"
    }
   },
   "id": "7464a951f44a07ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T10:03:46.650224Z"
    }
   },
   "id": "3e35d5ebef369658"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

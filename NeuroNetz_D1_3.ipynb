{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b0518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:19.951381300Z",
     "start_time": "2024-04-03T13:59:09.198935700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Einlesen der Pickle-Dateien und Vorverarbeitung des Inhaltes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42e897a1c7eeed"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ff61b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:19.976172200Z",
     "start_time": "2024-04-03T13:59:19.951381300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n0             0.0000      -0.00200        500   7000   9000      669.05\n1             0.0000      -0.00199        500   7000   9000      675.83\n2             0.0000      -0.00198        500   7000   9000      682.81\n3             0.0000      -0.00197        500   7000   9000      689.82\n4             0.0000      -0.00196        500   7000   9000      696.80\n...              ...           ...        ...    ...    ...         ...\n100646        0.0025       0.00196        500   7000   9000      578.47\n100647        0.0025       0.00197        500   7000   9000      576.89\n100648        0.0025       0.00198        500   7000   9000      575.32\n100649        0.0025       0.00199        500   7000   9000      573.76\n100650        0.0025       0.00200        500   7000   9000      572.20\n\n[100651 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00199</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>675.83</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00198</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>682.81</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00197</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>689.82</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>696.80</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>100646</th>\n      <td>0.0025</td>\n      <td>0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>578.47</td>\n    </tr>\n    <tr>\n      <th>100647</th>\n      <td>0.0025</td>\n      <td>0.00197</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>576.89</td>\n    </tr>\n    <tr>\n      <th>100648</th>\n      <td>0.0025</td>\n      <td>0.00198</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>575.32</td>\n    </tr>\n    <tr>\n      <th>100649</th>\n      <td>0.0025</td>\n      <td>0.00199</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>573.76</td>\n    </tr>\n    <tr>\n      <th>100650</th>\n      <td>0.0025</td>\n      <td>0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>572.20</td>\n    </tr>\n  </tbody>\n</table>\n<p>100651 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/Finish_D1_I7000_F9000/TPath_500_finish_data_D1.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966e3c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:19.984161700Z",
     "start_time": "2024-04-03T13:59:19.972171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "count    100651.000000\nmean       1144.030064\nstd         264.135723\nmin         572.200000\n25%         937.330000\n50%        1201.100000\n75%        1368.700000\nmax        1520.000000\nName: Temperatur, dtype: float64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop(data.columns[2:5], axis = 1)\n",
    "df['Temperatur'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8783d1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:20.047542Z",
     "start_time": "2024-04-03T13:59:19.983162300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       X-Koordinate  Y-Koordinate  Temperatur\n",
      "83145       0.00207      -0.00062      1282.2\n",
      "66701       0.00166      -0.00065      1347.6\n",
      "91325       0.00227       0.00098      1094.6\n",
      "46593       0.00116      -0.00123      1175.7\n",
      "49518       0.00123      -0.00005      1459.2\n",
      "...             ...           ...         ...\n",
      "6265        0.00015       0.00050      1439.1\n",
      "54886       0.00136       0.00150       876.7\n",
      "76820       0.00191       0.00029      1335.8\n",
      "860         0.00002      -0.00142      1071.2\n",
      "15795       0.00039      -0.00044      1488.1\n",
      "\n",
      "[100651 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Temperatur\n0            0.00207      -0.00062      1282.2\n1            0.00166      -0.00065      1347.6\n2            0.00227       0.00098      1094.6\n3            0.00116      -0.00123      1175.7\n4            0.00123      -0.00005      1459.2\n...              ...           ...         ...\n100646       0.00015       0.00050      1439.1\n100647       0.00136       0.00150       876.7\n100648       0.00191       0.00029      1335.8\n100649       0.00002      -0.00142      1071.2\n100650       0.00039      -0.00044      1488.1\n\n[100651 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00207</td>\n      <td>-0.00062</td>\n      <td>1282.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00166</td>\n      <td>-0.00065</td>\n      <td>1347.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00227</td>\n      <td>0.00098</td>\n      <td>1094.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00116</td>\n      <td>-0.00123</td>\n      <td>1175.7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00123</td>\n      <td>-0.00005</td>\n      <td>1459.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>100646</th>\n      <td>0.00015</td>\n      <td>0.00050</td>\n      <td>1439.1</td>\n    </tr>\n    <tr>\n      <th>100647</th>\n      <td>0.00136</td>\n      <td>0.00150</td>\n      <td>876.7</td>\n    </tr>\n    <tr>\n      <th>100648</th>\n      <td>0.00191</td>\n      <td>0.00029</td>\n      <td>1335.8</td>\n    </tr>\n    <tr>\n      <th>100649</th>\n      <td>0.00002</td>\n      <td>-0.00142</td>\n      <td>1071.2</td>\n    </tr>\n    <tr>\n      <th>100650</th>\n      <td>0.00039</td>\n      <td>-0.00044</td>\n      <td>1488.1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100651 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "print(df1)\n",
    "df_reset = df1.reset_index(drop=True)\n",
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e72a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:20.048047900Z",
     "start_time": "2024-04-03T13:59:19.998978300Z"
    }
   },
   "outputs": [],
   "source": [
    "label = df_reset[\"Temperatur\"]\n",
    "\n",
    "df1 = df_reset.drop(\"Temperatur\", axis=1)\n",
    "X = df1\n",
    "y = label"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7fa289a50d87423"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e694a236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:20.083059Z",
     "start_time": "2024-04-03T13:59:20.002569500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate\ncount  100651.000000  1.006510e+05\nmean        0.001250  1.103042e-20\nstd         0.000725  1.157589e-03\nmin         0.000000 -2.000000e-03\n25%         0.000620 -1.000000e-03\n50%         0.001250  4.529900e-18\n75%         0.001880  1.000000e-03\nmax         0.002500  2.000000e-03",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100651.000000</td>\n      <td>1.006510e+05</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.001250</td>\n      <td>1.103042e-20</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.000725</td>\n      <td>1.157589e-03</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-2.000000e-03</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000620</td>\n      <td>-1.000000e-03</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.001250</td>\n      <td>4.529900e-18</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.001880</td>\n      <td>1.000000e-03</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.002500</td>\n      <td>2.000000e-03</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3303b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:20.127059300Z",
     "start_time": "2024-04-03T13:59:20.018604900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "count    100651.000000\nmean       1144.030064\nstd         264.135723\nmin         572.200000\n25%         937.330000\n50%        1201.100000\n75%        1368.700000\nmax        1520.000000\nName: Temperatur, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ad8da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:20.135059200Z",
     "start_time": "2024-04-03T13:59:20.028394300Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c705edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:20.136059100Z",
     "start_time": "2024-04-03T13:59:20.039278700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_features2 = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "X_test_scaled = scaler_features.transform(X_test)  # Nutze unterschiedliche Skalierungsparameter\n",
    "\n",
    "scaler_target = MinMaxScaler()\n",
    "y_train_scaled = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbefe631e495b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T13:59:20.171059Z",
     "start_time": "2024-04-03T13:59:20.045923700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.416 , 0.5725],\n       [0.812 , 0.7325],\n       [0.628 , 0.5075],\n       ...,\n       [0.604 , 0.3875],\n       [0.748 , 0.65  ],\n       [0.028 , 0.6225]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ende der Datenvorverarbeitung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbcdf8ff6114c7a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning Modell mit Konfiguration je nach Dateiname"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca42b21ba64183fa"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "162/162 [==============================] - 3s 9ms/step - loss: 0.1623 - mae: 0.1721 - val_loss: 0.0295 - val_mae: 0.0249\n",
      "Epoch 2/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0298 - mae: 0.0303 - val_loss: 0.0286 - val_mae: 0.0303\n",
      "Epoch 3/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0276 - mae: 0.0227 - val_loss: 0.0259 - val_mae: 0.0094\n",
      "Epoch 4/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0267 - mae: 0.0214 - val_loss: 0.0250 - val_mae: 0.0045\n",
      "Epoch 5/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0248 - mae: 0.0074 - val_loss: 0.0245 - val_mae: 0.0056\n",
      "Epoch 6/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0247 - mae: 0.0143 - val_loss: 0.0248 - val_mae: 0.0213\n",
      "Epoch 7/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0241 - mae: 0.0108 - val_loss: 0.0235 - val_mae: 0.0036\n",
      "Epoch 8/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0240 - mae: 0.0150 - val_loss: 0.0231 - val_mae: 0.0047\n",
      "Epoch 9/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0230 - mae: 0.0081 - val_loss: 0.0227 - val_mae: 0.0045\n",
      "Epoch 10/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0232 - mae: 0.0162 - val_loss: 0.0224 - val_mae: 0.0091\n",
      "Epoch 11/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0221 - mae: 0.0039 - val_loss: 0.0219 - val_mae: 0.0030\n",
      "Epoch 12/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0218 - mae: 0.0049 - val_loss: 0.0216 - val_mae: 0.0034\n",
      "Epoch 13/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0217 - mae: 0.0108 - val_loss: 0.0231 - val_mae: 0.0289\n",
      "Epoch 14/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0212 - mae: 0.0068 - val_loss: 0.0210 - val_mae: 0.0094\n",
      "Epoch 15/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0209 - mae: 0.0083 - val_loss: 0.0208 - val_mae: 0.0111\n",
      "Epoch 16/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0206 - mae: 0.0102 - val_loss: 0.0203 - val_mae: 0.0041\n",
      "Epoch 17/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0203 - mae: 0.0103 - val_loss: 0.0200 - val_mae: 0.0065\n",
      "Epoch 18/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0200 - mae: 0.0103 - val_loss: 0.0197 - val_mae: 0.0083\n",
      "Epoch 19/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0197 - mae: 0.0093 - val_loss: 0.0194 - val_mae: 0.0065\n",
      "Epoch 20/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0192 - mae: 0.0048 - val_loss: 0.0209 - val_mae: 0.0291\n",
      "Epoch 21/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0191 - mae: 0.0082 - val_loss: 0.0188 - val_mae: 0.0079\n",
      "Epoch 22/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0187 - mae: 0.0090 - val_loss: 0.0184 - val_mae: 0.0046\n",
      "Epoch 23/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0187 - mae: 0.0138 - val_loss: 0.0183 - val_mae: 0.0113\n",
      "Epoch 24/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0180 - mae: 0.0037 - val_loss: 0.0178 - val_mae: 0.0041\n",
      "Epoch 25/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0177 - mae: 0.0058 - val_loss: 0.0175 - val_mae: 0.0021\n",
      "Epoch 26/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0174 - mae: 0.0051 - val_loss: 0.0172 - val_mae: 0.0040\n",
      "Epoch 27/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0171 - mae: 0.0082 - val_loss: 0.0192 - val_mae: 0.0346\n",
      "Epoch 28/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0170 - mae: 0.0096 - val_loss: 0.0165 - val_mae: 0.0032\n",
      "Epoch 29/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0165 - mae: 0.0073 - val_loss: 0.0163 - val_mae: 0.0060\n",
      "Epoch 30/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0162 - mae: 0.0086 - val_loss: 0.0161 - val_mae: 0.0098\n",
      "Epoch 31/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0159 - mae: 0.0084 - val_loss: 0.0156 - val_mae: 0.0044\n",
      "Epoch 32/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0156 - mae: 0.0095 - val_loss: 0.0152 - val_mae: 0.0026\n",
      "Epoch 33/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0151 - mae: 0.0044 - val_loss: 0.0149 - val_mae: 0.0025\n",
      "Epoch 34/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0149 - mae: 0.0088 - val_loss: 0.0149 - val_mae: 0.0112\n",
      "Epoch 35/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0148 - mae: 0.0130 - val_loss: 0.0143 - val_mae: 0.0071\n",
      "Epoch 36/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0141 - mae: 0.0050 - val_loss: 0.0139 - val_mae: 0.0028\n",
      "Epoch 37/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0139 - mae: 0.0087 - val_loss: 0.0136 - val_mae: 0.0084\n",
      "Epoch 38/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0135 - mae: 0.0058 - val_loss: 0.0144 - val_mae: 0.0238\n",
      "Epoch 39/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0132 - mae: 0.0083 - val_loss: 0.0130 - val_mae: 0.0066\n",
      "Epoch 40/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0129 - mae: 0.0085 - val_loss: 0.0125 - val_mae: 0.0017\n",
      "Epoch 41/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0124 - mae: 0.0051 - val_loss: 0.0122 - val_mae: 0.0040\n",
      "Epoch 42/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0124 - mae: 0.0108 - val_loss: 0.0120 - val_mae: 0.0098\n",
      "Epoch 43/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0118 - mae: 0.0054 - val_loss: 0.0115 - val_mae: 0.0032\n",
      "Epoch 44/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0114 - mae: 0.0057 - val_loss: 0.0112 - val_mae: 0.0056\n",
      "Epoch 45/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0114 - mae: 0.0119 - val_loss: 0.0109 - val_mae: 0.0023\n",
      "Epoch 46/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0107 - mae: 0.0038 - val_loss: 0.0113 - val_mae: 0.0196\n",
      "Epoch 47/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0104 - mae: 0.0065 - val_loss: 0.0108 - val_mae: 0.0205\n",
      "Epoch 48/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0105 - mae: 0.0124 - val_loss: 0.0099 - val_mae: 0.0029\n",
      "Epoch 49/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0097 - mae: 0.0033 - val_loss: 0.0095 - val_mae: 0.0025\n",
      "Epoch 50/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0094 - mae: 0.0033 - val_loss: 0.0092 - val_mae: 0.0030\n",
      "Epoch 51/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0092 - mae: 0.0079 - val_loss: 0.0089 - val_mae: 0.0066\n",
      "Epoch 52/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0088 - mae: 0.0060 - val_loss: 0.0085 - val_mae: 0.0016\n",
      "Epoch 53/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0085 - mae: 0.0071 - val_loss: 0.0083 - val_mae: 0.0094\n",
      "Epoch 54/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0082 - mae: 0.0085 - val_loss: 0.0080 - val_mae: 0.0078\n",
      "Epoch 55/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0079 - mae: 0.0076 - val_loss: 0.0076 - val_mae: 0.0050\n",
      "Epoch 56/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0076 - mae: 0.0081 - val_loss: 0.0088 - val_mae: 0.0280\n",
      "Epoch 57/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0072 - mae: 0.0060 - val_loss: 0.0070 - val_mae: 0.0055\n",
      "Epoch 58/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0070 - mae: 0.0083 - val_loss: 0.0069 - val_mae: 0.0132\n",
      "Epoch 59/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0066 - mae: 0.0060 - val_loss: 0.0065 - val_mae: 0.0070\n",
      "Epoch 60/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0063 - mae: 0.0050 - val_loss: 0.0063 - val_mae: 0.0090\n",
      "Epoch 61/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0063 - mae: 0.0097 - val_loss: 0.0059 - val_mae: 0.0063\n",
      "Epoch 62/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0058 - mae: 0.0053 - val_loss: 0.0059 - val_mae: 0.0116\n",
      "Epoch 63/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0055 - mae: 0.0065 - val_loss: 0.0053 - val_mae: 0.0044\n",
      "Epoch 64/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0052 - mae: 0.0047 - val_loss: 0.0051 - val_mae: 0.0028\n",
      "Epoch 65/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0051 - mae: 0.0085 - val_loss: 0.0048 - val_mae: 0.0028\n",
      "Epoch 66/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0047 - mae: 0.0048 - val_loss: 0.0046 - val_mae: 0.0020\n",
      "Epoch 67/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0046 - mae: 0.0074 - val_loss: 0.0044 - val_mae: 0.0023\n",
      "Epoch 68/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0043 - mae: 0.0054 - val_loss: 0.0041 - val_mae: 0.0026\n",
      "Epoch 69/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0041 - mae: 0.0065 - val_loss: 0.0040 - val_mae: 0.0081\n",
      "Epoch 70/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0039 - mae: 0.0068 - val_loss: 0.0044 - val_mae: 0.0177\n",
      "Epoch 71/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0037 - mae: 0.0068 - val_loss: 0.0035 - val_mae: 0.0021\n",
      "Epoch 72/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0036 - mae: 0.0073 - val_loss: 0.0034 - val_mae: 0.0045\n",
      "Epoch 73/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0034 - mae: 0.0067 - val_loss: 0.0032 - val_mae: 0.0054\n",
      "Epoch 74/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0031 - mae: 0.0046 - val_loss: 0.0030 - val_mae: 0.0036\n",
      "Epoch 75/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0031 - mae: 0.0093 - val_loss: 0.0035 - val_mae: 0.0177\n",
      "Epoch 76/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0029 - mae: 0.0048 - val_loss: 0.0027 - val_mae: 0.0042\n",
      "Epoch 77/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0028 - mae: 0.0071 - val_loss: 0.0027 - val_mae: 0.0063\n",
      "Epoch 78/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0026 - mae: 0.0042 - val_loss: 0.0026 - val_mae: 0.0092\n",
      "Epoch 79/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0024 - mae: 0.0045 - val_loss: 0.0024 - val_mae: 0.0054\n",
      "Epoch 80/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0065 - val_loss: 0.0023 - val_mae: 0.0086\n",
      "Epoch 81/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0023 - mae: 0.0065 - val_loss: 0.0021 - val_mae: 0.0026\n",
      "Epoch 82/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0021 - mae: 0.0032 - val_loss: 0.0039 - val_mae: 0.0357\n",
      "Epoch 83/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0021 - mae: 0.0058 - val_loss: 0.0020 - val_mae: 0.0053\n",
      "Epoch 84/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0019 - mae: 0.0059 - val_loss: 0.0020 - val_mae: 0.0118\n",
      "Epoch 85/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0018 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0076\n",
      "Epoch 86/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0019 - mae: 0.0092 - val_loss: 0.0021 - val_mae: 0.0187\n",
      "Epoch 87/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0016 - mae: 0.0026 - val_loss: 0.0016 - val_mae: 0.0021\n",
      "Epoch 88/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0016 - mae: 0.0057 - val_loss: 0.0016 - val_mae: 0.0055\n",
      "Epoch 89/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0015 - mae: 0.0055 - val_loss: 0.0015 - val_mae: 0.0034\n",
      "Epoch 90/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0016 - mae: 0.0068 - val_loss: 0.0014 - val_mae: 0.0023\n",
      "Epoch 91/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0014 - mae: 0.0033 - val_loss: 0.0014 - val_mae: 0.0057\n",
      "Epoch 92/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0014 - mae: 0.0076 - val_loss: 0.0013 - val_mae: 0.0028\n",
      "Epoch 93/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 0.0013 - val_mae: 0.0033\n",
      "Epoch 94/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0013 - mae: 0.0062 - val_loss: 0.0012 - val_mae: 0.0048\n",
      "Epoch 95/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 0.0013 - mae: 0.0062 - val_loss: 0.0012 - val_mae: 0.0030\n",
      "Epoch 96/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0037 - val_loss: 0.0012 - val_mae: 0.0085\n",
      "Epoch 97/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0065 - val_loss: 0.0012 - val_mae: 0.0103\n",
      "Epoch 98/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0029 - val_loss: 0.0011 - val_mae: 0.0021\n",
      "Epoch 99/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0041 - val_loss: 0.0011 - val_mae: 0.0072\n",
      "Epoch 100/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0062 - val_loss: 0.0011 - val_mae: 0.0082\n",
      "Epoch 101/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 0.0010 - mae: 0.0053 - val_loss: 0.0010 - val_mae: 0.0091\n",
      "Epoch 102/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 9.9676e-04 - mae: 0.0054 - val_loss: 9.4944e-04 - val_mae: 0.0040\n",
      "Epoch 103/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 9.6799e-04 - mae: 0.0055 - val_loss: 9.5874e-04 - val_mae: 0.0070\n",
      "Epoch 104/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 9.2556e-04 - mae: 0.0046 - val_loss: 9.1816e-04 - val_mae: 0.0060\n",
      "Epoch 105/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 9.3255e-04 - mae: 0.0062 - val_loss: 8.8871e-04 - val_mae: 0.0045\n",
      "Epoch 106/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 8.7752e-04 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0210\n",
      "Epoch 107/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 8.9999e-04 - mae: 0.0059 - val_loss: 8.1608e-04 - val_mae: 0.0021\n",
      "Epoch 108/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 8.6018e-04 - mae: 0.0054 - val_loss: 8.1066e-04 - val_mae: 0.0032\n",
      "Epoch 109/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 8.0864e-04 - mae: 0.0039 - val_loss: 8.0915e-04 - val_mae: 0.0052\n",
      "Epoch 110/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 8.1296e-04 - mae: 0.0051 - val_loss: 7.7041e-04 - val_mae: 0.0030\n",
      "Epoch 111/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 8.1620e-04 - mae: 0.0061 - val_loss: 8.8544e-04 - val_mae: 0.0086\n",
      "Epoch 112/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 7.7252e-04 - mae: 0.0040 - val_loss: 8.0595e-04 - val_mae: 0.0067\n",
      "Epoch 113/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 7.7736e-04 - mae: 0.0061 - val_loss: 8.7278e-04 - val_mae: 0.0087\n",
      "Epoch 114/400\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 7.2566e-04 - mae: 0.0037 - val_loss: 7.8950e-04 - val_mae: 0.0071\n",
      "Epoch 115/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 7.3436e-04 - mae: 0.0054 - val_loss: 7.0489e-04 - val_mae: 0.0046\n",
      "Epoch 116/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 7.4381e-04 - mae: 0.0058 - val_loss: 0.0013 - val_mae: 0.0195\n",
      "Epoch 117/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 7.4825e-04 - mae: 0.0048 - val_loss: 6.5803e-04 - val_mae: 0.0023\n",
      "Epoch 118/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 6.5190e-04 - mae: 0.0023 - val_loss: 6.4157e-04 - val_mae: 0.0019\n",
      "Epoch 119/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 6.6410e-04 - mae: 0.0038 - val_loss: 6.5634e-04 - val_mae: 0.0051\n",
      "Epoch 120/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 6.4457e-04 - mae: 0.0041 - val_loss: 6.2095e-04 - val_mae: 0.0030\n",
      "Epoch 121/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 6.6993e-04 - mae: 0.0056 - val_loss: 6.1441e-04 - val_mae: 0.0031\n",
      "Epoch 122/400\n",
      "162/162 [==============================] - 2s 10ms/step - loss: 6.3066e-04 - mae: 0.0042 - val_loss: 9.6960e-04 - val_mae: 0.0147\n",
      "Epoch 123/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 6.3958e-04 - mae: 0.0050 - val_loss: 5.7951e-04 - val_mae: 0.0015\n",
      "Epoch 124/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 5.8763e-04 - mae: 0.0033 - val_loss: 6.6237e-04 - val_mae: 0.0084\n",
      "Epoch 125/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 6.0394e-04 - mae: 0.0046 - val_loss: 0.0012 - val_mae: 0.0167\n",
      "Epoch 126/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 6.1246e-04 - mae: 0.0049 - val_loss: 6.4716e-04 - val_mae: 0.0089\n",
      "Epoch 127/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 5.7654e-04 - mae: 0.0044 - val_loss: 5.7470e-04 - val_mae: 0.0045\n",
      "Epoch 128/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.6377e-04 - mae: 0.0043 - val_loss: 5.3521e-04 - val_mae: 0.0027\n",
      "Epoch 129/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.7569e-04 - mae: 0.0051 - val_loss: 5.6132e-04 - val_mae: 0.0053\n",
      "Epoch 130/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.4615e-04 - mae: 0.0045 - val_loss: 5.4108e-04 - val_mae: 0.0048\n",
      "Epoch 131/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.5390e-04 - mae: 0.0048 - val_loss: 6.0042e-04 - val_mae: 0.0084\n",
      "Epoch 132/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.2301e-04 - mae: 0.0037 - val_loss: 5.0145e-04 - val_mae: 0.0029\n",
      "Epoch 133/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.4171e-04 - mae: 0.0048 - val_loss: 4.8892e-04 - val_mae: 0.0018\n",
      "Epoch 134/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.2633e-04 - mae: 0.0047 - val_loss: 4.8775e-04 - val_mae: 0.0026\n",
      "Epoch 135/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.4160e-04 - mae: 0.0059 - val_loss: 4.7964e-04 - val_mae: 0.0024\n",
      "Epoch 136/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 4.8114e-04 - mae: 0.0029 - val_loss: 7.6663e-04 - val_mae: 0.0156\n",
      "Epoch 137/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 5.2463e-04 - mae: 0.0055 - val_loss: 6.9885e-04 - val_mae: 0.0126\n",
      "Epoch 138/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.7350e-04 - mae: 0.0030 - val_loss: 5.0825e-04 - val_mae: 0.0062\n",
      "Epoch 139/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.7806e-04 - mae: 0.0044 - val_loss: 5.2209e-04 - val_mae: 0.0069\n",
      "Epoch 140/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.8226e-04 - mae: 0.0047 - val_loss: 4.6963e-04 - val_mae: 0.0044\n",
      "Epoch 141/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.6570e-04 - mae: 0.0039 - val_loss: 5.8134e-04 - val_mae: 0.0104\n",
      "Epoch 142/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.9398e-04 - mae: 0.0052 - val_loss: 4.3182e-04 - val_mae: 0.0021\n",
      "Epoch 143/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.4036e-04 - mae: 0.0032 - val_loss: 4.5737e-04 - val_mae: 0.0054\n",
      "Epoch 144/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.5323e-04 - mae: 0.0044 - val_loss: 4.6690e-04 - val_mae: 0.0049\n",
      "Epoch 145/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.7749e-04 - mae: 0.0058 - val_loss: 4.4421e-04 - val_mae: 0.0043\n",
      "Epoch 146/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.2137e-04 - mae: 0.0028 - val_loss: 4.3664e-04 - val_mae: 0.0040\n",
      "Epoch 147/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.3202e-04 - mae: 0.0039 - val_loss: 4.0819e-04 - val_mae: 0.0029\n",
      "Epoch 148/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.3598e-04 - mae: 0.0049 - val_loss: 4.0387e-04 - val_mae: 0.0028\n",
      "Epoch 149/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.1751e-04 - mae: 0.0042 - val_loss: 3.9490e-04 - val_mae: 0.0024\n",
      "Epoch 150/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.3452e-04 - mae: 0.0055 - val_loss: 4.0466e-04 - val_mae: 0.0041\n",
      "Epoch 151/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.1614e-04 - mae: 0.0041 - val_loss: 4.1504e-04 - val_mae: 0.0047\n",
      "Epoch 152/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.1157e-04 - mae: 0.0046 - val_loss: 3.8683e-04 - val_mae: 0.0027\n",
      "Epoch 153/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.1578e-04 - mae: 0.0047 - val_loss: 4.3268e-04 - val_mae: 0.0055\n",
      "Epoch 154/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.1225e-04 - mae: 0.0043 - val_loss: 0.0014 - val_mae: 0.0229\n",
      "Epoch 155/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.7510e-04 - mae: 0.0052 - val_loss: 3.6853e-04 - val_mae: 0.0017\n",
      "Epoch 156/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.6521e-04 - mae: 0.0015 - val_loss: 3.9466e-04 - val_mae: 0.0044\n",
      "Epoch 157/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.7877e-04 - mae: 0.0036 - val_loss: 3.5805e-04 - val_mae: 0.0014\n",
      "Epoch 158/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.7892e-04 - mae: 0.0041 - val_loss: 3.9396e-04 - val_mae: 0.0056\n",
      "Epoch 159/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 4.0489e-04 - mae: 0.0056 - val_loss: 4.5449e-04 - val_mae: 0.0067\n",
      "Epoch 160/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.6140e-04 - mae: 0.0029 - val_loss: 3.9389e-04 - val_mae: 0.0068\n",
      "Epoch 161/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.8302e-04 - mae: 0.0048 - val_loss: 3.5165e-04 - val_mae: 0.0027\n",
      "Epoch 162/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.5892e-04 - mae: 0.0035 - val_loss: 5.8240e-04 - val_mae: 0.0128\n",
      "Epoch 163/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.8112e-04 - mae: 0.0049 - val_loss: 3.3896e-04 - val_mae: 0.0018\n",
      "Epoch 164/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.5771e-04 - mae: 0.0039 - val_loss: 3.3528e-04 - val_mae: 0.0020\n",
      "Epoch 165/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.7109e-04 - mae: 0.0049 - val_loss: 4.9798e-04 - val_mae: 0.0118\n",
      "Epoch 166/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.5194e-04 - mae: 0.0031 - val_loss: 3.2636e-04 - val_mae: 0.0013\n",
      "Epoch 167/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.5880e-04 - mae: 0.0046 - val_loss: 3.3046e-04 - val_mae: 0.0021\n",
      "Epoch 168/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.7777e-04 - mae: 0.0053 - val_loss: 3.4428e-04 - val_mae: 0.0035\n",
      "Epoch 169/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 3.3313e-04 - mae: 0.0033 - val_loss: 3.4087e-04 - val_mae: 0.0038\n",
      "Epoch 170/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.6683e-04 - mae: 0.0048 - val_loss: 3.1961e-04 - val_mae: 0.0020\n",
      "Epoch 171/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.3025e-04 - mae: 0.0032 - val_loss: 3.1568e-04 - val_mae: 0.0021\n",
      "Epoch 172/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.3290e-04 - mae: 0.0037 - val_loss: 3.0942e-04 - val_mae: 0.0014\n",
      "Epoch 173/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.4637e-04 - mae: 0.0048 - val_loss: 3.6941e-04 - val_mae: 0.0059\n",
      "Epoch 174/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.3834e-04 - mae: 0.0044 - val_loss: 3.4400e-04 - val_mae: 0.0050\n",
      "Epoch 175/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.2621e-04 - mae: 0.0039 - val_loss: 3.0664e-04 - val_mae: 0.0020\n",
      "Epoch 176/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.3111e-04 - mae: 0.0044 - val_loss: 3.5006e-04 - val_mae: 0.0056\n",
      "Epoch 177/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.2052e-04 - mae: 0.0035 - val_loss: 2.9927e-04 - val_mae: 0.0016\n",
      "Epoch 178/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.3376e-04 - mae: 0.0047 - val_loss: 3.0014e-04 - val_mae: 0.0024\n",
      "Epoch 179/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.3956e-04 - mae: 0.0045 - val_loss: 3.4133e-04 - val_mae: 0.0067\n",
      "Epoch 180/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.9864e-04 - mae: 0.0024 - val_loss: 3.1648e-04 - val_mae: 0.0047\n",
      "Epoch 181/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.3334e-04 - mae: 0.0046 - val_loss: 2.9142e-04 - val_mae: 0.0019\n",
      "Epoch 182/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.2447e-04 - mae: 0.0041 - val_loss: 2.9174e-04 - val_mae: 0.0021\n",
      "Epoch 183/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.0360e-04 - mae: 0.0033 - val_loss: 3.3257e-04 - val_mae: 0.0059\n",
      "Epoch 184/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.2830e-04 - mae: 0.0052 - val_loss: 3.3540e-04 - val_mae: 0.0070\n",
      "Epoch 185/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.1066e-04 - mae: 0.0037 - val_loss: 4.4325e-04 - val_mae: 0.0112\n",
      "Epoch 186/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.0578e-04 - mae: 0.0034 - val_loss: 2.7933e-04 - val_mae: 0.0014\n",
      "Epoch 187/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.9193e-04 - mae: 0.0032 - val_loss: 2.7867e-04 - val_mae: 0.0020\n",
      "Epoch 188/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.0102e-04 - mae: 0.0042 - val_loss: 2.8182e-04 - val_mae: 0.0030\n",
      "Epoch 189/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 3.0756e-04 - mae: 0.0043 - val_loss: 2.7490e-04 - val_mae: 0.0019\n",
      "Epoch 190/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.9369e-04 - mae: 0.0035 - val_loss: 5.6163e-04 - val_mae: 0.0150\n",
      "Epoch 191/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.9424e-04 - mae: 0.0036 - val_loss: 5.0530e-04 - val_mae: 0.0121\n",
      "Epoch 192/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 3.1239e-04 - mae: 0.0049 - val_loss: 3.2187e-04 - val_mae: 0.0064\n",
      "Epoch 193/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.8237e-04 - mae: 0.0032 - val_loss: 2.6651e-04 - val_mae: 0.0014\n",
      "Epoch 194/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.9121e-04 - mae: 0.0041 - val_loss: 3.7832e-04 - val_mae: 0.0071\n",
      "Epoch 195/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.9905e-04 - mae: 0.0042 - val_loss: 2.8104e-04 - val_mae: 0.0042\n",
      "Epoch 196/400\n",
      "162/162 [==============================] - 1s 7ms/step - loss: 2.8955e-04 - mae: 0.0043 - val_loss: 2.6375e-04 - val_mae: 0.0019\n",
      "Epoch 197/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.7762e-04 - mae: 0.0034 - val_loss: 3.2461e-04 - val_mae: 0.0065\n",
      "Epoch 198/400\n",
      "162/162 [==============================] - 1s 8ms/step - loss: 2.9159e-04 - mae: 0.0043 - val_loss: 2.6277e-04 - val_mae: 0.0023\n",
      "Epoch 199/400\n",
      "162/162 [==============================] - 1s 9ms/step - loss: 2.8873e-04 - mae: 0.0043 - val_loss: 3.8007e-04 - val_mae: 0.0078\n",
      "Epoch 200/400\n",
      "162/162 [==============================] - 2s 15ms/step - loss: 2.9302e-04 - mae: 0.0045 - val_loss: 3.3364e-04 - val_mae: 0.0071\n",
      "Epoch 201/400\n",
      "162/162 [==============================] - 3s 18ms/step - loss: 2.7647e-04 - mae: 0.0040 - val_loss: 3.0219e-04 - val_mae: 0.0066\n",
      "Epoch 202/400\n",
      "162/162 [==============================] - 2s 15ms/step - loss: 2.9695e-04 - mae: 0.0047 - val_loss: 2.6431e-04 - val_mae: 0.0033\n",
      "Epoch 203/400\n",
      "160/162 [============================>.] - ETA: 0s - loss: 2.7508e-04 - mae: 0.0034Restoring model weights from the end of the best epoch: 198.\n",
      "162/162 [==============================] - 3s 21ms/step - loss: 2.7521e-04 - mae: 0.0034 - val_loss: 2.8233e-04 - val_mae: 0.0039\n",
      "Epoch 203: early stopping\n",
      "Die Ausführungszeit betrug 264.44533681869507 Sekunden.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "    # Eingabeschicht\n",
    "\n",
    "    Dense(320, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(176, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(288, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(192, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(208, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "\n",
    "    Dense(224, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(80, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(304, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(240, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(48, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=2, mode='min', restore_best_weights=True)#, min_delta = 0.00005)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 400\n",
    "epochs = 400\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose = 1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Berechne die Dauer\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"Die Ausführungszeit betrug {duration} Sekunden.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T14:05:05.757853600Z",
     "start_time": "2024-04-03T14:00:41.200708600Z"
    }
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# k-Fold Crossvalidation \n",
    "Für die Performancebestimmung der unterschiedlichen Netzarchitekturen auf den Trainingsdaten"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7928df8ec1031de"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "Training für Fold 1...\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "81/81 [==============================] - 3s 15ms/step - loss: 0.4990 - mae: 0.2470 - val_loss: 0.2954 - val_mae: 0.0856\n",
      "Epoch 2/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2695 - mae: 0.0496 - val_loss: 0.2576 - val_mae: 0.0666\n",
      "Epoch 3/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.2433 - mae: 0.0282 - val_loss: 0.2351 - val_mae: 0.0083\n",
      "Epoch 4/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.2319 - mae: 0.0243 - val_loss: 0.2263 - val_mae: 0.0159\n",
      "Epoch 5/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.2239 - mae: 0.0268 - val_loss: 0.2185 - val_mae: 0.0085\n",
      "Epoch 6/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.2166 - mae: 0.0224 - val_loss: 0.2122 - val_mae: 0.0156\n",
      "Epoch 7/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.2089 - mae: 0.0102 - val_loss: 0.2057 - val_mae: 0.0036\n",
      "Epoch 8/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.2031 - mae: 0.0119 - val_loss: 0.2002 - val_mae: 0.0132\n",
      "Epoch 9/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1980 - mae: 0.0190 - val_loss: 0.1949 - val_mae: 0.0146\n",
      "Epoch 10/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1922 - mae: 0.0092 - val_loss: 0.1971 - val_mae: 0.0668\n",
      "Epoch 11/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1878 - mae: 0.0181 - val_loss: 0.1846 - val_mae: 0.0035\n",
      "Epoch 12/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1828 - mae: 0.0134 - val_loss: 0.1799 - val_mae: 0.0042\n",
      "Epoch 13/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1779 - mae: 0.0085 - val_loss: 0.1756 - val_mae: 0.0098\n",
      "Epoch 14/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1740 - mae: 0.0147 - val_loss: 0.1712 - val_mae: 0.0050\n",
      "Epoch 15/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1692 - mae: 0.0062 - val_loss: 0.1671 - val_mae: 0.0074\n",
      "Epoch 16/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1656 - mae: 0.0131 - val_loss: 0.1631 - val_mae: 0.0065\n",
      "Epoch 17/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1614 - mae: 0.0091 - val_loss: 0.1602 - val_mae: 0.0240\n",
      "Epoch 18/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1576 - mae: 0.0110 - val_loss: 0.1555 - val_mae: 0.0060\n",
      "Epoch 19/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1538 - mae: 0.0096 - val_loss: 0.1518 - val_mae: 0.0029\n",
      "Epoch 20/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1502 - mae: 0.0102 - val_loss: 0.1483 - val_mae: 0.0074\n",
      "Epoch 21/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1467 - mae: 0.0112 - val_loss: 0.1449 - val_mae: 0.0105\n",
      "Epoch 22/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1432 - mae: 0.0091 - val_loss: 0.1415 - val_mae: 0.0110\n",
      "Epoch 23/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.1398 - mae: 0.0075 - val_loss: 0.1380 - val_mae: 0.0035\n",
      "Epoch 24/1000\n",
      "81/81 [==============================] - 1s 15ms/step - loss: 0.1366 - mae: 0.0112 - val_loss: 0.1347 - val_mae: 0.0038\n",
      "Epoch 25/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1333 - mae: 0.0086 - val_loss: 0.1328 - val_mae: 0.0290\n",
      "Epoch 26/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1302 - mae: 0.0099 - val_loss: 0.1283 - val_mae: 0.0023\n",
      "Epoch 27/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1269 - mae: 0.0069 - val_loss: 0.1254 - val_mae: 0.0113\n",
      "Epoch 28/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1240 - mae: 0.0106 - val_loss: 0.1222 - val_mae: 0.0034\n",
      "Epoch 29/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1210 - mae: 0.0106 - val_loss: 0.1196 - val_mae: 0.0179\n",
      "Epoch 30/1000\n",
      "81/81 [==============================] - 1s 15ms/step - loss: 0.1178 - mae: 0.0061 - val_loss: 0.1164 - val_mae: 0.0096\n",
      "Epoch 31/1000\n",
      "81/81 [==============================] - 1s 15ms/step - loss: 0.1150 - mae: 0.0092 - val_loss: 0.1134 - val_mae: 0.0069\n",
      "Epoch 32/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.1120 - mae: 0.0070 - val_loss: 0.1105 - val_mae: 0.0055\n",
      "Epoch 33/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1093 - mae: 0.0104 - val_loss: 0.1077 - val_mae: 0.0094\n",
      "Epoch 34/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1063 - mae: 0.0061 - val_loss: 0.1056 - val_mae: 0.0218\n",
      "Epoch 35/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.1037 - mae: 0.0085 - val_loss: 0.1021 - val_mae: 0.0052\n",
      "Epoch 36/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1008 - mae: 0.0066 - val_loss: 0.0998 - val_mae: 0.0178\n",
      "Epoch 37/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0982 - mae: 0.0111 - val_loss: 0.0985 - val_mae: 0.0389\n",
      "Epoch 38/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0955 - mae: 0.0087 - val_loss: 0.0940 - val_mae: 0.0046\n",
      "Epoch 39/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0928 - mae: 0.0064 - val_loss: 0.0917 - val_mae: 0.0150\n",
      "Epoch 40/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0903 - mae: 0.0099 - val_loss: 0.0889 - val_mae: 0.0085\n",
      "Epoch 41/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0876 - mae: 0.0074 - val_loss: 0.0863 - val_mae: 0.0073\n",
      "Epoch 42/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0854 - mae: 0.0120 - val_loss: 0.0838 - val_mae: 0.0073\n",
      "Epoch 43/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0825 - mae: 0.0051 - val_loss: 0.0813 - val_mae: 0.0046\n",
      "Epoch 44/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0801 - mae: 0.0053 - val_loss: 0.0788 - val_mae: 0.0032\n",
      "Epoch 45/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0778 - mae: 0.0088 - val_loss: 0.0765 - val_mae: 0.0095\n",
      "Epoch 46/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0753 - mae: 0.0067 - val_loss: 0.0740 - val_mae: 0.0035\n",
      "Epoch 47/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0730 - mae: 0.0089 - val_loss: 0.0718 - val_mae: 0.0091\n",
      "Epoch 48/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0708 - mae: 0.0095 - val_loss: 0.0695 - val_mae: 0.0105\n",
      "Epoch 49/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0684 - mae: 0.0067 - val_loss: 0.0672 - val_mae: 0.0042\n",
      "Epoch 50/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0663 - mae: 0.0119 - val_loss: 0.0650 - val_mae: 0.0050\n",
      "Epoch 51/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0639 - mae: 0.0059 - val_loss: 0.0628 - val_mae: 0.0045\n",
      "Epoch 52/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0618 - mae: 0.0083 - val_loss: 0.0607 - val_mae: 0.0086\n",
      "Epoch 53/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0598 - mae: 0.0104 - val_loss: 0.0586 - val_mae: 0.0072\n",
      "Epoch 54/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0576 - mae: 0.0077 - val_loss: 0.0565 - val_mae: 0.0027\n",
      "Epoch 55/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0556 - mae: 0.0084 - val_loss: 0.0545 - val_mae: 0.0047\n",
      "Epoch 56/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0537 - mae: 0.0083 - val_loss: 0.0530 - val_mae: 0.0180\n",
      "Epoch 57/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0516 - mae: 0.0065 - val_loss: 0.0506 - val_mae: 0.0045\n",
      "Epoch 58/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0498 - mae: 0.0105 - val_loss: 0.0495 - val_mae: 0.0224\n",
      "Epoch 59/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0480 - mae: 0.0113 - val_loss: 0.0468 - val_mae: 0.0033\n",
      "Epoch 60/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0460 - mae: 0.0060 - val_loss: 0.0450 - val_mae: 0.0039\n",
      "Epoch 61/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0443 - mae: 0.0095 - val_loss: 0.0433 - val_mae: 0.0036\n",
      "Epoch 62/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0427 - mae: 0.0107 - val_loss: 0.0416 - val_mae: 0.0056\n",
      "Epoch 63/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0408 - mae: 0.0050 - val_loss: 0.0399 - val_mae: 0.0058\n",
      "Epoch 64/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0392 - mae: 0.0089 - val_loss: 0.0383 - val_mae: 0.0035\n",
      "Epoch 65/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0376 - mae: 0.0083 - val_loss: 0.0368 - val_mae: 0.0106\n",
      "Epoch 66/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0360 - mae: 0.0086 - val_loss: 0.0353 - val_mae: 0.0098\n",
      "Epoch 67/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0346 - mae: 0.0092 - val_loss: 0.0338 - val_mae: 0.0103\n",
      "Epoch 68/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0332 - mae: 0.0109 - val_loss: 0.0322 - val_mae: 0.0034\n",
      "Epoch 69/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0316 - mae: 0.0047 - val_loss: 0.0309 - val_mae: 0.0065\n",
      "Epoch 70/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0303 - mae: 0.0094 - val_loss: 0.0305 - val_mae: 0.0272\n",
      "Epoch 71/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0290 - mae: 0.0089 - val_loss: 0.0284 - val_mae: 0.0146\n",
      "Epoch 72/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0278 - mae: 0.0118 - val_loss: 0.0270 - val_mae: 0.0089\n",
      "Epoch 73/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0264 - mae: 0.0072 - val_loss: 0.0260 - val_mae: 0.0149\n",
      "Epoch 74/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0253 - mae: 0.0108 - val_loss: 0.0246 - val_mae: 0.0116\n",
      "Epoch 75/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0240 - mae: 0.0078 - val_loss: 0.0268 - val_mae: 0.0511\n",
      "Epoch 76/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0230 - mae: 0.0090 - val_loss: 0.0223 - val_mae: 0.0032\n",
      "Epoch 77/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0218 - mae: 0.0065 - val_loss: 0.0212 - val_mae: 0.0057\n",
      "Epoch 78/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0210 - mae: 0.0115 - val_loss: 0.0215 - val_mae: 0.0306\n",
      "Epoch 79/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0199 - mae: 0.0096 - val_loss: 0.0192 - val_mae: 0.0038\n",
      "Epoch 80/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0189 - mae: 0.0079 - val_loss: 0.0183 - val_mae: 0.0049\n",
      "Epoch 81/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0180 - mae: 0.0087 - val_loss: 0.0174 - val_mae: 0.0036\n",
      "Epoch 82/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0172 - mae: 0.0104 - val_loss: 0.0168 - val_mae: 0.0107\n",
      "Epoch 83/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0163 - mae: 0.0103 - val_loss: 0.0158 - val_mae: 0.0039\n",
      "Epoch 84/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0155 - mae: 0.0086 - val_loss: 0.0151 - val_mae: 0.0124\n",
      "Epoch 85/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0147 - mae: 0.0091 - val_loss: 0.0143 - val_mae: 0.0074\n",
      "Epoch 86/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0140 - mae: 0.0089 - val_loss: 0.0136 - val_mae: 0.0077\n",
      "Epoch 87/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0134 - mae: 0.0114 - val_loss: 0.0132 - val_mae: 0.0156\n",
      "Epoch 88/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0127 - mae: 0.0085 - val_loss: 0.0126 - val_mae: 0.0156\n",
      "Epoch 89/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0120 - mae: 0.0090 - val_loss: 0.0116 - val_mae: 0.0055\n",
      "Epoch 90/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0116 - mae: 0.0116 - val_loss: 0.0113 - val_mae: 0.0143\n",
      "Epoch 91/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0109 - mae: 0.0099 - val_loss: 0.0106 - val_mae: 0.0104\n",
      "Epoch 92/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0104 - mae: 0.0092 - val_loss: 0.0100 - val_mae: 0.0035\n",
      "Epoch 93/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0099 - mae: 0.0096 - val_loss: 0.0097 - val_mae: 0.0132\n",
      "Epoch 94/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0095 - mae: 0.0106 - val_loss: 0.0091 - val_mae: 0.0039\n",
      "Epoch 95/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0089 - mae: 0.0087 - val_loss: 0.0091 - val_mae: 0.0184\n",
      "Epoch 96/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0086 - mae: 0.0115 - val_loss: 0.0082 - val_mae: 0.0054\n",
      "Epoch 97/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0084 - mae: 0.0136 - val_loss: 0.0084 - val_mae: 0.0196\n",
      "Epoch 98/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0080 - mae: 0.0119 - val_loss: 0.0075 - val_mae: 0.0064\n",
      "Epoch 99/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0073 - mae: 0.0056 - val_loss: 0.0072 - val_mae: 0.0061\n",
      "Epoch 100/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0072 - mae: 0.0110 - val_loss: 0.0070 - val_mae: 0.0127\n",
      "Epoch 101/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0067 - mae: 0.0065 - val_loss: 0.0067 - val_mae: 0.0141\n",
      "Epoch 102/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0065 - mae: 0.0110 - val_loss: 0.0065 - val_mae: 0.0135\n",
      "Epoch 103/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0064 - mae: 0.0116 - val_loss: 0.0061 - val_mae: 0.0112\n",
      "Epoch 104/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0059 - mae: 0.0056 - val_loss: 0.0057 - val_mae: 0.0043\n",
      "Epoch 105/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0058 - mae: 0.0112 - val_loss: 0.0059 - val_mae: 0.0177\n",
      "Epoch 106/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0055 - mae: 0.0092 - val_loss: 0.0053 - val_mae: 0.0048\n",
      "Epoch 107/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0053 - mae: 0.0104 - val_loss: 0.0053 - val_mae: 0.0131\n",
      "Epoch 108/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0052 - mae: 0.0108 - val_loss: 0.0051 - val_mae: 0.0123\n",
      "Epoch 109/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0049 - mae: 0.0092 - val_loss: 0.0047 - val_mae: 0.0066\n",
      "Epoch 110/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0047 - mae: 0.0102 - val_loss: 0.0049 - val_mae: 0.0169\n",
      "Epoch 111/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0046 - mae: 0.0104 - val_loss: 0.0044 - val_mae: 0.0046\n",
      "Epoch 112/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0044 - mae: 0.0100 - val_loss: 0.0042 - val_mae: 0.0054\n",
      "Epoch 113/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0043 - mae: 0.0096 - val_loss: 0.0041 - val_mae: 0.0077\n",
      "Epoch 114/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0043 - mae: 0.0119 - val_loss: 0.0039 - val_mae: 0.0040\n",
      "Epoch 115/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0040 - mae: 0.0086 - val_loss: 0.0038 - val_mae: 0.0054\n",
      "Epoch 116/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0040 - mae: 0.0129 - val_loss: 0.0039 - val_mae: 0.0125\n",
      "Epoch 117/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0038 - mae: 0.0094 - val_loss: 0.0036 - val_mae: 0.0042\n",
      "Epoch 118/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0037 - mae: 0.0101 - val_loss: 0.0037 - val_mae: 0.0122\n",
      "Epoch 119/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0036 - mae: 0.0097 - val_loss: 0.0034 - val_mae: 0.0046\n",
      "Epoch 120/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0035 - mae: 0.0088 - val_loss: 0.0033 - val_mae: 0.0055\n",
      "Epoch 121/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0038 - mae: 0.0146 - val_loss: 0.0033 - val_mae: 0.0046\n",
      "Epoch 122/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0032 - mae: 0.0042 - val_loss: 0.0032 - val_mae: 0.0056\n",
      "Epoch 123/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0033 - mae: 0.0091 - val_loss: 0.0037 - val_mae: 0.0218\n",
      "Epoch 124/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0032 - mae: 0.0096 - val_loss: 0.0032 - val_mae: 0.0105\n",
      "Epoch 125/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0033 - mae: 0.0128 - val_loss: 0.0031 - val_mae: 0.0090\n",
      "Epoch 126/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0030 - mae: 0.0073 - val_loss: 0.0030 - val_mae: 0.0070\n",
      "Epoch 127/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0031 - mae: 0.0108 - val_loss: 0.0029 - val_mae: 0.0063\n",
      "Epoch 128/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0030 - mae: 0.0109 - val_loss: 0.0028 - val_mae: 0.0043\n",
      "Epoch 129/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0034 - mae: 0.0172 - val_loss: 0.0029 - val_mae: 0.0068\n",
      "Epoch 130/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0029 - mae: 0.0089 - val_loss: 0.0028 - val_mae: 0.0070\n",
      "Epoch 131/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0029 - mae: 0.0098 - val_loss: 0.0028 - val_mae: 0.0066\n",
      "Epoch 132/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0082 - val_loss: 0.0027 - val_mae: 0.0072\n",
      "Epoch 133/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0094 - val_loss: 0.0027 - val_mae: 0.0095\n",
      "Epoch 134/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0028 - mae: 0.0100 - val_loss: 0.0027 - val_mae: 0.0074\n",
      "Epoch 135/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0095 - val_loss: 0.0026 - val_mae: 0.0060\n",
      "Epoch 136/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0027 - mae: 0.0108 - val_loss: 0.0025 - val_mae: 0.0041\n",
      "Epoch 137/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0026 - mae: 0.0094 - val_loss: 0.0026 - val_mae: 0.0082\n",
      "Epoch 138/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0026 - mae: 0.0090 - val_loss: 0.0028 - val_mae: 0.0150\n",
      "Epoch 139/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0026 - mae: 0.0103 - val_loss: 0.0025 - val_mae: 0.0048\n",
      "Epoch 140/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0026 - mae: 0.0100 - val_loss: 0.0025 - val_mae: 0.0075\n",
      "Epoch 141/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0026 - mae: 0.0103 - val_loss: 0.0026 - val_mae: 0.0112\n",
      "Epoch 142/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0026 - mae: 0.0105 - val_loss: 0.0024 - val_mae: 0.0054\n",
      "Epoch 143/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0126 - val_loss: 0.0028 - val_mae: 0.0159\n",
      "Epoch 144/1000\n",
      "81/81 [==============================] - 1s 15ms/step - loss: 0.0024 - mae: 0.0075 - val_loss: 0.0024 - val_mae: 0.0053\n",
      "Epoch 145/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0024 - mae: 0.0073 - val_loss: 0.0023 - val_mae: 0.0056\n",
      "Epoch 146/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0109 - val_loss: 0.0025 - val_mae: 0.0133\n",
      "Epoch 147/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0098 - val_loss: 0.0023 - val_mae: 0.0063\n",
      "Epoch 148/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0024 - mae: 0.0093 - val_loss: 0.0024 - val_mae: 0.0078\n",
      "Epoch 149/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0024 - mae: 0.0089 - val_loss: 0.0023 - val_mae: 0.0081\n",
      "Epoch 150/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0024 - mae: 0.0095 - val_loss: 0.0023 - val_mae: 0.0060\n",
      "Epoch 151/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0114 - val_loss: 0.0039 - val_mae: 0.0333\n",
      "Epoch 152/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0024 - mae: 0.0090 - val_loss: 0.0022 - val_mae: 0.0042\n",
      "Epoch 153/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0024 - mae: 0.0094 - val_loss: 0.0022 - val_mae: 0.0048\n",
      "Epoch 154/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0024 - mae: 0.0104 - val_loss: 0.0024 - val_mae: 0.0114\n",
      "Epoch 155/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0086 - val_loss: 0.0028 - val_mae: 0.0239\n",
      "Epoch 156/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0086 - val_loss: 0.0023 - val_mae: 0.0097\n",
      "Epoch 157/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0023 - mae: 0.0084 - val_loss: 0.0022 - val_mae: 0.0080\n",
      "Epoch 158/1000\n",
      "77/81 [===========================>..] - ETA: 0s - loss: 0.0023 - mae: 0.0109Restoring model weights from the end of the best epoch: 153.\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0110 - val_loss: 0.0023 - val_mae: 0.0131\n",
      "Epoch 158: early stopping\n",
      "Training für Fold 2...\n",
      "Epoch 1/1000\n",
      "81/81 [==============================] - 3s 15ms/step - loss: 0.4257 - mae: 0.2141 - val_loss: 0.3302 - val_mae: 0.1740\n",
      "Epoch 2/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2616 - mae: 0.0472 - val_loss: 0.2369 - val_mae: 0.0094\n",
      "Epoch 3/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2278 - mae: 0.0164 - val_loss: 0.2193 - val_mae: 0.0245\n",
      "Epoch 4/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.2120 - mae: 0.0113 - val_loss: 0.2057 - val_mae: 0.0098\n",
      "Epoch 5/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2006 - mae: 0.0133 - val_loss: 0.1953 - val_mae: 0.0142\n",
      "Epoch 6/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1903 - mae: 0.0113 - val_loss: 0.1953 - val_mae: 0.0724\n",
      "Epoch 7/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1830 - mae: 0.0258 - val_loss: 0.1766 - val_mae: 0.0064\n",
      "Epoch 8/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1725 - mae: 0.0042 - val_loss: 0.1684 - val_mae: 0.0034\n",
      "Epoch 9/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1646 - mae: 0.0041 - val_loss: 0.1608 - val_mae: 0.0038\n",
      "Epoch 10/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1573 - mae: 0.0064 - val_loss: 0.1536 - val_mae: 0.0033\n",
      "Epoch 11/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1507 - mae: 0.0114 - val_loss: 0.1485 - val_mae: 0.0318\n",
      "Epoch 12/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1441 - mae: 0.0124 - val_loss: 0.1405 - val_mae: 0.0039\n",
      "Epoch 13/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1375 - mae: 0.0042 - val_loss: 0.1345 - val_mae: 0.0055\n",
      "Epoch 14/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1316 - mae: 0.0052 - val_loss: 0.1287 - val_mae: 0.0048\n",
      "Epoch 15/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1260 - mae: 0.0074 - val_loss: 0.1233 - val_mae: 0.0106\n",
      "Epoch 16/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.1207 - mae: 0.0083 - val_loss: 0.1179 - val_mae: 0.0080\n",
      "Epoch 17/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1155 - mae: 0.0077 - val_loss: 0.1154 - val_mae: 0.0385\n",
      "Epoch 18/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1107 - mae: 0.0092 - val_loss: 0.1080 - val_mae: 0.0038\n",
      "Epoch 19/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1058 - mae: 0.0064 - val_loss: 0.1037 - val_mae: 0.0149\n",
      "Epoch 20/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1016 - mae: 0.0132 - val_loss: 0.0990 - val_mae: 0.0040\n",
      "Epoch 21/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0970 - mae: 0.0050 - val_loss: 0.0948 - val_mae: 0.0049\n",
      "Epoch 22/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0929 - mae: 0.0068 - val_loss: 0.0907 - val_mae: 0.0036\n",
      "Epoch 23/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0889 - mae: 0.0083 - val_loss: 0.0869 - val_mae: 0.0067\n",
      "Epoch 24/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0851 - mae: 0.0092 - val_loss: 0.0831 - val_mae: 0.0034\n",
      "Epoch 25/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0815 - mae: 0.0112 - val_loss: 0.0795 - val_mae: 0.0088\n",
      "Epoch 26/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0778 - mae: 0.0073 - val_loss: 0.0760 - val_mae: 0.0029\n",
      "Epoch 27/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0745 - mae: 0.0098 - val_loss: 0.0727 - val_mae: 0.0052\n",
      "Epoch 28/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0711 - mae: 0.0082 - val_loss: 0.0694 - val_mae: 0.0053\n",
      "Epoch 29/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0680 - mae: 0.0094 - val_loss: 0.0663 - val_mae: 0.0045\n",
      "Epoch 30/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0649 - mae: 0.0071 - val_loss: 0.0634 - val_mae: 0.0100\n",
      "Epoch 31/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0621 - mae: 0.0090 - val_loss: 0.0606 - val_mae: 0.0132\n",
      "Epoch 32/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0593 - mae: 0.0093 - val_loss: 0.0601 - val_mae: 0.0350\n",
      "Epoch 33/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0566 - mae: 0.0094 - val_loss: 0.0551 - val_mae: 0.0080\n",
      "Epoch 34/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0538 - mae: 0.0067 - val_loss: 0.0529 - val_mae: 0.0167\n",
      "Epoch 35/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0514 - mae: 0.0091 - val_loss: 0.0501 - val_mae: 0.0073\n",
      "Epoch 36/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0490 - mae: 0.0092 - val_loss: 0.0483 - val_mae: 0.0209\n",
      "Epoch 37/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0467 - mae: 0.0100 - val_loss: 0.0454 - val_mae: 0.0062\n",
      "Epoch 38/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0445 - mae: 0.0098 - val_loss: 0.0433 - val_mae: 0.0070\n",
      "Epoch 39/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0423 - mae: 0.0088 - val_loss: 0.0413 - val_mae: 0.0119\n",
      "Epoch 40/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0402 - mae: 0.0087 - val_loss: 0.0392 - val_mae: 0.0063\n",
      "Epoch 41/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0383 - mae: 0.0099 - val_loss: 0.0373 - val_mae: 0.0083\n",
      "Epoch 42/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0364 - mae: 0.0096 - val_loss: 0.0356 - val_mae: 0.0125\n",
      "Epoch 43/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0347 - mae: 0.0112 - val_loss: 0.0338 - val_mae: 0.0124\n",
      "Epoch 44/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0328 - mae: 0.0063 - val_loss: 0.0319 - val_mae: 0.0036\n",
      "Epoch 45/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0312 - mae: 0.0087 - val_loss: 0.0303 - val_mae: 0.0031\n",
      "Epoch 46/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0299 - mae: 0.0124 - val_loss: 0.0289 - val_mae: 0.0111\n",
      "Epoch 47/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0281 - mae: 0.0058 - val_loss: 0.0273 - val_mae: 0.0069\n",
      "Epoch 48/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0268 - mae: 0.0105 - val_loss: 0.0259 - val_mae: 0.0041\n",
      "Epoch 49/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0253 - mae: 0.0075 - val_loss: 0.0247 - val_mae: 0.0099\n",
      "Epoch 50/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0241 - mae: 0.0111 - val_loss: 0.0233 - val_mae: 0.0056\n",
      "Epoch 51/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0228 - mae: 0.0086 - val_loss: 0.0231 - val_mae: 0.0288\n",
      "Epoch 52/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0216 - mae: 0.0083 - val_loss: 0.0209 - val_mae: 0.0045\n",
      "Epoch 53/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0204 - mae: 0.0078 - val_loss: 0.0203 - val_mae: 0.0177\n",
      "Epoch 54/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0195 - mae: 0.0097 - val_loss: 0.0188 - val_mae: 0.0069\n",
      "Epoch 55/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0184 - mae: 0.0096 - val_loss: 0.0178 - val_mae: 0.0042\n",
      "Epoch 56/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0174 - mae: 0.0084 - val_loss: 0.0169 - val_mae: 0.0079\n",
      "Epoch 57/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0165 - mae: 0.0093 - val_loss: 0.0160 - val_mae: 0.0072\n",
      "Epoch 58/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0157 - mae: 0.0107 - val_loss: 0.0151 - val_mae: 0.0060\n",
      "Epoch 59/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0149 - mae: 0.0103 - val_loss: 0.0144 - val_mae: 0.0075\n",
      "Epoch 60/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0141 - mae: 0.0097 - val_loss: 0.0135 - val_mae: 0.0032\n",
      "Epoch 61/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0133 - mae: 0.0093 - val_loss: 0.0134 - val_mae: 0.0231\n",
      "Epoch 62/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0126 - mae: 0.0091 - val_loss: 0.0125 - val_mae: 0.0174\n",
      "Epoch 63/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0119 - mae: 0.0085 - val_loss: 0.0115 - val_mae: 0.0046\n",
      "Epoch 64/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0114 - mae: 0.0092 - val_loss: 0.0109 - val_mae: 0.0042\n",
      "Epoch 65/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0109 - mae: 0.0094 - val_loss: 0.0104 - val_mae: 0.0035\n",
      "Epoch 66/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0102 - mae: 0.0085 - val_loss: 0.0098 - val_mae: 0.0050\n",
      "Epoch 67/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0096 - mae: 0.0074 - val_loss: 0.0095 - val_mae: 0.0139\n",
      "Epoch 68/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0092 - mae: 0.0100 - val_loss: 0.0089 - val_mae: 0.0054\n",
      "Epoch 69/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0087 - mae: 0.0079 - val_loss: 0.0084 - val_mae: 0.0048\n",
      "Epoch 70/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0084 - mae: 0.0104 - val_loss: 0.0084 - val_mae: 0.0167\n",
      "Epoch 71/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0079 - mae: 0.0090 - val_loss: 0.0079 - val_mae: 0.0151\n",
      "Epoch 72/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0076 - mae: 0.0109 - val_loss: 0.0081 - val_mae: 0.0272\n",
      "Epoch 73/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0071 - mae: 0.0064 - val_loss: 0.0069 - val_mae: 0.0037\n",
      "Epoch 74/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0072 - mae: 0.0138 - val_loss: 0.0066 - val_mae: 0.0053\n",
      "Epoch 75/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0064 - mae: 0.0039 - val_loss: 0.0063 - val_mae: 0.0053\n",
      "Epoch 76/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0063 - mae: 0.0097 - val_loss: 0.0060 - val_mae: 0.0047\n",
      "Epoch 77/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0059 - mae: 0.0056 - val_loss: 0.0058 - val_mae: 0.0076\n",
      "Epoch 78/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0057 - mae: 0.0084 - val_loss: 0.0058 - val_mae: 0.0162\n",
      "Epoch 79/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0055 - mae: 0.0100 - val_loss: 0.0053 - val_mae: 0.0041\n",
      "Epoch 80/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0053 - mae: 0.0099 - val_loss: 0.0051 - val_mae: 0.0074\n",
      "Epoch 81/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0050 - mae: 0.0077 - val_loss: 0.0048 - val_mae: 0.0051\n",
      "Epoch 82/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0050 - mae: 0.0117 - val_loss: 0.0047 - val_mae: 0.0065\n",
      "Epoch 83/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0046 - mae: 0.0051 - val_loss: 0.0045 - val_mae: 0.0043\n",
      "Epoch 84/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0045 - mae: 0.0108 - val_loss: 0.0043 - val_mae: 0.0049\n",
      "Epoch 85/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0043 - mae: 0.0094 - val_loss: 0.0042 - val_mae: 0.0077\n",
      "Epoch 86/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0042 - mae: 0.0087 - val_loss: 0.0040 - val_mae: 0.0054\n",
      "Epoch 87/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0042 - mae: 0.0117 - val_loss: 0.0039 - val_mae: 0.0041\n",
      "Epoch 88/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0038 - mae: 0.0065 - val_loss: 0.0037 - val_mae: 0.0045\n",
      "Epoch 89/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0038 - mae: 0.0085 - val_loss: 0.0040 - val_mae: 0.0176\n",
      "Epoch 90/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0036 - mae: 0.0074 - val_loss: 0.0037 - val_mae: 0.0140\n",
      "Epoch 91/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0036 - mae: 0.0091 - val_loss: 0.0034 - val_mae: 0.0047\n",
      "Epoch 92/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0035 - mae: 0.0091 - val_loss: 0.0034 - val_mae: 0.0088\n",
      "Epoch 93/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0033 - mae: 0.0079 - val_loss: 0.0033 - val_mae: 0.0086\n",
      "Epoch 94/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0036 - mae: 0.0136 - val_loss: 0.0031 - val_mae: 0.0050\n",
      "Epoch 95/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0031 - mae: 0.0043 - val_loss: 0.0031 - val_mae: 0.0073\n",
      "Epoch 96/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0032 - mae: 0.0095 - val_loss: 0.0032 - val_mae: 0.0144\n",
      "Epoch 97/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0030 - mae: 0.0059 - val_loss: 0.0029 - val_mae: 0.0057\n",
      "Epoch 98/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0029 - mae: 0.0058 - val_loss: 0.0030 - val_mae: 0.0125\n",
      "Epoch 99/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0031 - mae: 0.0121 - val_loss: 0.0028 - val_mae: 0.0040\n",
      "Epoch 100/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0054 - val_loss: 0.0028 - val_mae: 0.0084\n",
      "Epoch 101/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0029 - mae: 0.0108 - val_loss: 0.0027 - val_mae: 0.0035\n",
      "Epoch 102/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0084 - val_loss: 0.0029 - val_mae: 0.0131\n",
      "Epoch 103/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0027 - mae: 0.0081 - val_loss: 0.0026 - val_mae: 0.0077\n",
      "Epoch 104/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0091 - val_loss: 0.0025 - val_mae: 0.0038\n",
      "Epoch 105/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0096 - val_loss: 0.0025 - val_mae: 0.0041\n",
      "Epoch 106/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0082 - val_loss: 0.0025 - val_mae: 0.0079\n",
      "Epoch 107/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0092 - val_loss: 0.0024 - val_mae: 0.0050\n",
      "Epoch 108/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0101 - val_loss: 0.0024 - val_mae: 0.0044\n",
      "Epoch 109/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0132 - val_loss: 0.0024 - val_mae: 0.0091\n",
      "Epoch 110/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0023 - mae: 0.0046 - val_loss: 0.0023 - val_mae: 0.0033\n",
      "Epoch 111/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0073 - val_loss: 0.0030 - val_mae: 0.0249\n",
      "Epoch 112/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0024 - mae: 0.0089 - val_loss: 0.0024 - val_mae: 0.0110\n",
      "Epoch 113/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0024 - mae: 0.0102 - val_loss: 0.0023 - val_mae: 0.0076\n",
      "Epoch 114/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0024 - mae: 0.0097 - val_loss: 0.0022 - val_mae: 0.0062\n",
      "Epoch 115/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0077 - val_loss: 0.0023 - val_mae: 0.0083\n",
      "Epoch 116/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0097 - val_loss: 0.0024 - val_mae: 0.0131\n",
      "Epoch 117/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0022 - mae: 0.0084 - val_loss: 0.0021 - val_mae: 0.0036\n",
      "Epoch 118/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0022 - mae: 0.0089 - val_loss: 0.0022 - val_mae: 0.0081\n",
      "Epoch 119/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0024 - mae: 0.0116 - val_loss: 0.0021 - val_mae: 0.0039\n",
      "Epoch 120/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0042 - val_loss: 0.0021 - val_mae: 0.0036\n",
      "Epoch 121/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0072 - val_loss: 0.0022 - val_mae: 0.0108\n",
      "Epoch 122/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0022 - mae: 0.0099 - val_loss: 0.0020 - val_mae: 0.0032\n",
      "Epoch 123/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0022 - mae: 0.0103 - val_loss: 0.0034 - val_mae: 0.0321\n",
      "Epoch 124/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0072 - val_loss: 0.0020 - val_mae: 0.0034\n",
      "Epoch 125/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0066 - val_loss: 0.0020 - val_mae: 0.0077\n",
      "Epoch 126/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0022 - mae: 0.0111 - val_loss: 0.0020 - val_mae: 0.0053\n",
      "Epoch 127/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0020 - mae: 0.0065 - val_loss: 0.0020 - val_mae: 0.0064\n",
      "Epoch 128/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0075 - val_loss: 0.0019 - val_mae: 0.0038\n",
      "Epoch 129/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0022 - mae: 0.0108 - val_loss: 0.0020 - val_mae: 0.0061\n",
      "Epoch 130/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0022 - mae: 0.0099 - val_loss: 0.0021 - val_mae: 0.0113\n",
      "Epoch 131/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0058 - val_loss: 0.0019 - val_mae: 0.0036\n",
      "Epoch 132/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0075 - val_loss: 0.0019 - val_mae: 0.0044\n",
      "Epoch 133/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0081 - val_loss: 0.0021 - val_mae: 0.0132\n",
      "Epoch 134/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0072 - val_loss: 0.0019 - val_mae: 0.0071\n",
      "Epoch 135/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0097 - val_loss: 0.0021 - val_mae: 0.0145\n",
      "Epoch 136/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0085 - val_loss: 0.0019 - val_mae: 0.0038\n",
      "Epoch 137/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0114 - val_loss: 0.0023 - val_mae: 0.0183\n",
      "Epoch 138/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0070 - val_loss: 0.0018 - val_mae: 0.0037\n",
      "Epoch 139/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0019 - mae: 0.0072 - val_loss: 0.0020 - val_mae: 0.0097\n",
      "Epoch 140/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0071 - val_loss: 0.0019 - val_mae: 0.0102\n",
      "Epoch 141/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0089 - val_loss: 0.0018 - val_mae: 0.0059\n",
      "Epoch 142/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0089 - val_loss: 0.0018 - val_mae: 0.0042\n",
      "Epoch 143/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0100 - val_loss: 0.0018 - val_mae: 0.0052\n",
      "Epoch 144/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0018 - mae: 0.0049 - val_loss: 0.0018 - val_mae: 0.0049\n",
      "Epoch 145/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0093 - val_loss: 0.0018 - val_mae: 0.0057\n",
      "Epoch 146/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0018 - mae: 0.0075 - val_loss: 0.0018 - val_mae: 0.0081\n",
      "Epoch 147/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0019 - mae: 0.0086 - val_loss: 0.0019 - val_mae: 0.0096\n",
      "Epoch 148/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0088 - val_loss: 0.0018 - val_mae: 0.0050\n",
      "Epoch 149/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0097 - val_loss: 0.0020 - val_mae: 0.0129\n",
      "Epoch 150/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0019 - mae: 0.0088 - val_loss: 0.0018 - val_mae: 0.0057\n",
      "Epoch 151/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0018 - mae: 0.0069 - val_loss: 0.0019 - val_mae: 0.0131\n",
      "Epoch 152/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0097 - val_loss: 0.0017 - val_mae: 0.0046\n",
      "Epoch 153/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0086 - val_loss: 0.0020 - val_mae: 0.0156\n",
      "Epoch 154/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0018 - mae: 0.0078 - val_loss: 0.0018 - val_mae: 0.0065\n",
      "Epoch 155/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0018 - mae: 0.0099 - val_loss: 0.0019 - val_mae: 0.0117\n",
      "Epoch 156/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0018 - mae: 0.0069 - val_loss: 0.0018 - val_mae: 0.0075\n",
      "Epoch 157/1000\n",
      "80/81 [============================>.] - ETA: 0s - loss: 0.0018 - mae: 0.0084Restoring model weights from the end of the best epoch: 152.\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0018 - mae: 0.0084 - val_loss: 0.0018 - val_mae: 0.0070\n",
      "Epoch 157: early stopping\n",
      "Training für Fold 3...\n",
      "Epoch 1/1000\n",
      "81/81 [==============================] - 3s 15ms/step - loss: 0.4240 - mae: 0.2176 - val_loss: 0.2918 - val_mae: 0.0944\n",
      "Epoch 2/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2503 - mae: 0.0334 - val_loss: 0.2320 - val_mae: 0.0343\n",
      "Epoch 3/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2218 - mae: 0.0203 - val_loss: 0.2137 - val_mae: 0.0188\n",
      "Epoch 4/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2097 - mae: 0.0297 - val_loss: 0.2017 - val_mae: 0.0083\n",
      "Epoch 5/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1969 - mae: 0.0060 - val_loss: 0.1922 - val_mae: 0.0040\n",
      "Epoch 6/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1881 - mae: 0.0074 - val_loss: 0.1838 - val_mae: 0.0065\n",
      "Epoch 7/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1804 - mae: 0.0162 - val_loss: 0.1768 - val_mae: 0.0212\n",
      "Epoch 8/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1727 - mae: 0.0125 - val_loss: 0.1690 - val_mae: 0.0138\n",
      "Epoch 9/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1657 - mae: 0.0131 - val_loss: 0.1619 - val_mae: 0.0031\n",
      "Epoch 10/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1589 - mae: 0.0095 - val_loss: 0.1562 - val_mae: 0.0202\n",
      "Epoch 11/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1529 - mae: 0.0124 - val_loss: 0.1496 - val_mae: 0.0083\n",
      "Epoch 12/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1468 - mae: 0.0099 - val_loss: 0.1441 - val_mae: 0.0139\n",
      "Epoch 13/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1414 - mae: 0.0132 - val_loss: 0.1385 - val_mae: 0.0108\n",
      "Epoch 14/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1360 - mae: 0.0113 - val_loss: 0.1333 - val_mae: 0.0128\n",
      "Epoch 15/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1309 - mae: 0.0110 - val_loss: 0.1286 - val_mae: 0.0170\n",
      "Epoch 16/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1260 - mae: 0.0111 - val_loss: 0.1234 - val_mae: 0.0033\n",
      "Epoch 17/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1212 - mae: 0.0072 - val_loss: 0.1191 - val_mae: 0.0160\n",
      "Epoch 18/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1168 - mae: 0.0099 - val_loss: 0.1144 - val_mae: 0.0030\n",
      "Epoch 19/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1126 - mae: 0.0117 - val_loss: 0.1105 - val_mae: 0.0127\n",
      "Epoch 20/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1082 - mae: 0.0083 - val_loss: 0.1061 - val_mae: 0.0050\n",
      "Epoch 21/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1042 - mae: 0.0070 - val_loss: 0.1023 - val_mae: 0.0117\n",
      "Epoch 22/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1003 - mae: 0.0079 - val_loss: 0.0986 - val_mae: 0.0151\n",
      "Epoch 23/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0966 - mae: 0.0094 - val_loss: 0.0946 - val_mae: 0.0057\n",
      "Epoch 24/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0930 - mae: 0.0087 - val_loss: 0.0910 - val_mae: 0.0034\n",
      "Epoch 25/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0895 - mae: 0.0084 - val_loss: 0.0876 - val_mae: 0.0028\n",
      "Epoch 26/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0860 - mae: 0.0084 - val_loss: 0.0843 - val_mae: 0.0074\n",
      "Epoch 27/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0828 - mae: 0.0100 - val_loss: 0.0810 - val_mae: 0.0063\n",
      "Epoch 28/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0795 - mae: 0.0073 - val_loss: 0.0780 - val_mae: 0.0102\n",
      "Epoch 29/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0766 - mae: 0.0138 - val_loss: 0.0750 - val_mae: 0.0129\n",
      "Epoch 30/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0734 - mae: 0.0069 - val_loss: 0.0720 - val_mae: 0.0113\n",
      "Epoch 31/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0705 - mae: 0.0075 - val_loss: 0.0690 - val_mae: 0.0047\n",
      "Epoch 32/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0677 - mae: 0.0079 - val_loss: 0.0662 - val_mae: 0.0059\n",
      "Epoch 33/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0649 - mae: 0.0076 - val_loss: 0.0634 - val_mae: 0.0025\n",
      "Epoch 34/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0625 - mae: 0.0134 - val_loss: 0.0609 - val_mae: 0.0073\n",
      "Epoch 35/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0596 - mae: 0.0062 - val_loss: 0.0583 - val_mae: 0.0046\n",
      "Epoch 36/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0573 - mae: 0.0101 - val_loss: 0.0558 - val_mae: 0.0029\n",
      "Epoch 37/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0547 - mae: 0.0046 - val_loss: 0.0537 - val_mae: 0.0158\n",
      "Epoch 38/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0524 - mae: 0.0084 - val_loss: 0.0511 - val_mae: 0.0034\n",
      "Epoch 39/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0502 - mae: 0.0099 - val_loss: 0.0490 - val_mae: 0.0079\n",
      "Epoch 40/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0479 - mae: 0.0080 - val_loss: 0.0467 - val_mae: 0.0040\n",
      "Epoch 41/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0458 - mae: 0.0067 - val_loss: 0.0447 - val_mae: 0.0030\n",
      "Epoch 42/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0438 - mae: 0.0096 - val_loss: 0.0426 - val_mae: 0.0041\n",
      "Epoch 43/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0417 - mae: 0.0067 - val_loss: 0.0407 - val_mae: 0.0041\n",
      "Epoch 44/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0399 - mae: 0.0083 - val_loss: 0.0388 - val_mae: 0.0042\n",
      "Epoch 45/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0381 - mae: 0.0100 - val_loss: 0.0370 - val_mae: 0.0055\n",
      "Epoch 46/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0363 - mae: 0.0115 - val_loss: 0.0355 - val_mae: 0.0141\n",
      "Epoch 47/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0345 - mae: 0.0079 - val_loss: 0.0336 - val_mae: 0.0071\n",
      "Epoch 48/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0329 - mae: 0.0086 - val_loss: 0.0320 - val_mae: 0.0046\n",
      "Epoch 49/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0313 - mae: 0.0095 - val_loss: 0.0304 - val_mae: 0.0063\n",
      "Epoch 50/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0298 - mae: 0.0077 - val_loss: 0.0289 - val_mae: 0.0051\n",
      "Epoch 51/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0284 - mae: 0.0103 - val_loss: 0.0276 - val_mae: 0.0077\n",
      "Epoch 52/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0270 - mae: 0.0098 - val_loss: 0.0262 - val_mae: 0.0102\n",
      "Epoch 53/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0255 - mae: 0.0067 - val_loss: 0.0250 - val_mae: 0.0134\n",
      "Epoch 54/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0243 - mae: 0.0095 - val_loss: 0.0236 - val_mae: 0.0055\n",
      "Epoch 55/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0231 - mae: 0.0093 - val_loss: 0.0224 - val_mae: 0.0034\n",
      "Epoch 56/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0220 - mae: 0.0099 - val_loss: 0.0212 - val_mae: 0.0032\n",
      "Epoch 57/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0208 - mae: 0.0087 - val_loss: 0.0207 - val_mae: 0.0173\n",
      "Epoch 58/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0197 - mae: 0.0072 - val_loss: 0.0192 - val_mae: 0.0095\n",
      "Epoch 59/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0187 - mae: 0.0096 - val_loss: 0.0187 - val_mae: 0.0235\n",
      "Epoch 60/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0178 - mae: 0.0092 - val_loss: 0.0172 - val_mae: 0.0043\n",
      "Epoch 61/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0170 - mae: 0.0117 - val_loss: 0.0164 - val_mae: 0.0077\n",
      "Epoch 62/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0159 - mae: 0.0070 - val_loss: 0.0156 - val_mae: 0.0133\n",
      "Epoch 63/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0153 - mae: 0.0111 - val_loss: 0.0147 - val_mae: 0.0061\n",
      "Epoch 64/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0144 - mae: 0.0075 - val_loss: 0.0140 - val_mae: 0.0096\n",
      "Epoch 65/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0136 - mae: 0.0081 - val_loss: 0.0132 - val_mae: 0.0039\n",
      "Epoch 66/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0131 - mae: 0.0098 - val_loss: 0.0125 - val_mae: 0.0060\n",
      "Epoch 67/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0123 - mae: 0.0092 - val_loss: 0.0119 - val_mae: 0.0044\n",
      "Epoch 68/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0117 - mae: 0.0104 - val_loss: 0.0127 - val_mae: 0.0339\n",
      "Epoch 69/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0111 - mae: 0.0080 - val_loss: 0.0107 - val_mae: 0.0051\n",
      "Epoch 70/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0106 - mae: 0.0102 - val_loss: 0.0104 - val_mae: 0.0121\n",
      "Epoch 71/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0100 - mae: 0.0086 - val_loss: 0.0098 - val_mae: 0.0123\n",
      "Epoch 72/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0097 - mae: 0.0112 - val_loss: 0.0092 - val_mae: 0.0086\n",
      "Epoch 73/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0090 - mae: 0.0060 - val_loss: 0.0088 - val_mae: 0.0072\n",
      "Epoch 74/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0086 - mae: 0.0097 - val_loss: 0.0084 - val_mae: 0.0090\n",
      "Epoch 75/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0082 - mae: 0.0098 - val_loss: 0.0081 - val_mae: 0.0129\n",
      "Epoch 76/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0079 - mae: 0.0107 - val_loss: 0.0078 - val_mae: 0.0144\n",
      "Epoch 77/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0074 - mae: 0.0084 - val_loss: 0.0072 - val_mae: 0.0052\n",
      "Epoch 78/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0073 - mae: 0.0122 - val_loss: 0.0068 - val_mae: 0.0034\n",
      "Epoch 79/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0068 - mae: 0.0080 - val_loss: 0.0068 - val_mae: 0.0141\n",
      "Epoch 80/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0064 - mae: 0.0078 - val_loss: 0.0069 - val_mae: 0.0257\n",
      "Epoch 81/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0062 - mae: 0.0095 - val_loss: 0.0074 - val_mae: 0.0343\n",
      "Epoch 82/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0060 - mae: 0.0092 - val_loss: 0.0070 - val_mae: 0.0296\n",
      "Epoch 83/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0057 - mae: 0.0086 - val_loss: 0.0055 - val_mae: 0.0085\n",
      "Epoch 84/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0055 - mae: 0.0097 - val_loss: 0.0052 - val_mae: 0.0044\n",
      "Epoch 85/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0052 - mae: 0.0099 - val_loss: 0.0052 - val_mae: 0.0134\n",
      "Epoch 86/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0050 - mae: 0.0099 - val_loss: 0.0048 - val_mae: 0.0042\n",
      "Epoch 87/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0050 - mae: 0.0111 - val_loss: 0.0046 - val_mae: 0.0063\n",
      "Epoch 88/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0045 - mae: 0.0042 - val_loss: 0.0044 - val_mae: 0.0031\n",
      "Epoch 89/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0045 - mae: 0.0089 - val_loss: 0.0042 - val_mae: 0.0045\n",
      "Epoch 90/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0044 - mae: 0.0103 - val_loss: 0.0042 - val_mae: 0.0115\n",
      "Epoch 91/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0041 - mae: 0.0079 - val_loss: 0.0040 - val_mae: 0.0101\n",
      "Epoch 92/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0041 - mae: 0.0108 - val_loss: 0.0039 - val_mae: 0.0075\n",
      "Epoch 93/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0038 - mae: 0.0069 - val_loss: 0.0043 - val_mae: 0.0191\n",
      "Epoch 94/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0038 - mae: 0.0104 - val_loss: 0.0036 - val_mae: 0.0040\n",
      "Epoch 95/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0036 - mae: 0.0068 - val_loss: 0.0035 - val_mae: 0.0077\n",
      "Epoch 96/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0035 - mae: 0.0096 - val_loss: 0.0034 - val_mae: 0.0033\n",
      "Epoch 97/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0034 - mae: 0.0083 - val_loss: 0.0033 - val_mae: 0.0046\n",
      "Epoch 98/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0033 - mae: 0.0086 - val_loss: 0.0032 - val_mae: 0.0030\n",
      "Epoch 99/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0032 - mae: 0.0079 - val_loss: 0.0031 - val_mae: 0.0045\n",
      "Epoch 100/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0034 - mae: 0.0114 - val_loss: 0.0030 - val_mae: 0.0041\n",
      "Epoch 101/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0030 - mae: 0.0035 - val_loss: 0.0030 - val_mae: 0.0054\n",
      "Epoch 102/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0031 - mae: 0.0100 - val_loss: 0.0036 - val_mae: 0.0258\n",
      "Epoch 103/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0029 - mae: 0.0082 - val_loss: 0.0028 - val_mae: 0.0040\n",
      "Epoch 104/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0029 - mae: 0.0097 - val_loss: 0.0032 - val_mae: 0.0149\n",
      "Epoch 105/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0081 - val_loss: 0.0027 - val_mae: 0.0043\n",
      "Epoch 106/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0091 - val_loss: 0.0031 - val_mae: 0.0177\n",
      "Epoch 107/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0089 - val_loss: 0.0026 - val_mae: 0.0030\n",
      "Epoch 108/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0104 - val_loss: 0.0025 - val_mae: 0.0033\n",
      "Epoch 109/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0085 - val_loss: 0.0030 - val_mae: 0.0199\n",
      "Epoch 110/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0064 - val_loss: 0.0025 - val_mae: 0.0048\n",
      "Epoch 111/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0084 - val_loss: 0.0024 - val_mae: 0.0037\n",
      "Epoch 112/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0090 - val_loss: 0.0024 - val_mae: 0.0032\n",
      "Epoch 113/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0024 - mae: 0.0072 - val_loss: 0.0025 - val_mae: 0.0127\n",
      "Epoch 114/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0134 - val_loss: 0.0023 - val_mae: 0.0045\n",
      "Epoch 115/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0040 - val_loss: 0.0023 - val_mae: 0.0039\n",
      "Epoch 116/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0024 - mae: 0.0092 - val_loss: 0.0024 - val_mae: 0.0118\n",
      "Epoch 117/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0062 - val_loss: 0.0027 - val_mae: 0.0209\n",
      "Epoch 118/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0023 - mae: 0.0078 - val_loss: 0.0041 - val_mae: 0.0416\n",
      "Epoch 119/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0076 - val_loss: 0.0022 - val_mae: 0.0029\n",
      "Epoch 120/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0086 - val_loss: 0.0022 - val_mae: 0.0040\n",
      "Epoch 121/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0101 - val_loss: 0.0022 - val_mae: 0.0055\n",
      "Epoch 122/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0052 - val_loss: 0.0021 - val_mae: 0.0044\n",
      "Epoch 123/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0103 - val_loss: 0.0032 - val_mae: 0.0302\n",
      "Epoch 124/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0099 - val_loss: 0.0021 - val_mae: 0.0031\n",
      "Epoch 125/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0040 - val_loss: 0.0021 - val_mae: 0.0067\n",
      "Epoch 126/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0083 - val_loss: 0.0023 - val_mae: 0.0173\n",
      "Epoch 127/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0021 - mae: 0.0077 - val_loss: 0.0021 - val_mae: 0.0062\n",
      "Epoch 128/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0081 - val_loss: 0.0021 - val_mae: 0.0071\n",
      "Epoch 129/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0076 - val_loss: 0.0020 - val_mae: 0.0080\n",
      "Epoch 130/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0076 - val_loss: 0.0020 - val_mae: 0.0070\n",
      "Epoch 131/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0085 - val_loss: 0.0020 - val_mae: 0.0044\n",
      "Epoch 132/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0083 - val_loss: 0.0020 - val_mae: 0.0073\n",
      "Epoch 133/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0086 - val_loss: 0.0020 - val_mae: 0.0077\n",
      "Epoch 134/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0091 - val_loss: 0.0019 - val_mae: 0.0053\n",
      "Epoch 135/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0084 - val_loss: 0.0020 - val_mae: 0.0083\n",
      "Epoch 136/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0095 - val_loss: 0.0020 - val_mae: 0.0099\n",
      "Epoch 137/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0069 - val_loss: 0.0021 - val_mae: 0.0150\n",
      "Epoch 138/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0108 - val_loss: 0.0020 - val_mae: 0.0114\n",
      "Epoch 139/1000\n",
      "81/81 [==============================] - ETA: 0s - loss: 0.0019 - mae: 0.0065Restoring model weights from the end of the best epoch: 134.\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0065 - val_loss: 0.0023 - val_mae: 0.0205\n",
      "Epoch 139: early stopping\n",
      "Training für Fold 4...\n",
      "Epoch 1/1000\n",
      "81/81 [==============================] - 3s 16ms/step - loss: 0.4995 - mae: 0.2567 - val_loss: 0.3388 - val_mae: 0.1736\n",
      "Epoch 2/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2641 - mae: 0.0494 - val_loss: 0.2425 - val_mae: 0.0261\n",
      "Epoch 3/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.2340 - mae: 0.0236 - val_loss: 0.2275 - val_mae: 0.0346\n",
      "Epoch 4/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2216 - mae: 0.0236 - val_loss: 0.2153 - val_mae: 0.0071\n",
      "Epoch 5/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2121 - mae: 0.0220 - val_loss: 0.2072 - val_mae: 0.0160\n",
      "Epoch 6/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.2036 - mae: 0.0141 - val_loss: 0.1996 - val_mae: 0.0092\n",
      "Epoch 7/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1968 - mae: 0.0179 - val_loss: 0.1935 - val_mae: 0.0204\n",
      "Epoch 8/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1901 - mae: 0.0148 - val_loss: 0.1867 - val_mae: 0.0105\n",
      "Epoch 9/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1842 - mae: 0.0162 - val_loss: 0.1819 - val_mae: 0.0254\n",
      "Epoch 10/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1782 - mae: 0.0114 - val_loss: 0.1753 - val_mae: 0.0070\n",
      "Epoch 11/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1730 - mae: 0.0132 - val_loss: 0.1703 - val_mae: 0.0144\n",
      "Epoch 12/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1680 - mae: 0.0131 - val_loss: 0.1651 - val_mae: 0.0051\n",
      "Epoch 13/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1629 - mae: 0.0102 - val_loss: 0.1608 - val_mae: 0.0181\n",
      "Epoch 14/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1585 - mae: 0.0133 - val_loss: 0.1559 - val_mae: 0.0062\n",
      "Epoch 15/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1539 - mae: 0.0082 - val_loss: 0.1516 - val_mae: 0.0078\n",
      "Epoch 16/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1498 - mae: 0.0117 - val_loss: 0.1475 - val_mae: 0.0078\n",
      "Epoch 17/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1460 - mae: 0.0158 - val_loss: 0.1434 - val_mae: 0.0060\n",
      "Epoch 18/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1416 - mae: 0.0068 - val_loss: 0.1396 - val_mae: 0.0055\n",
      "Epoch 19/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1378 - mae: 0.0078 - val_loss: 0.1358 - val_mae: 0.0045\n",
      "Epoch 20/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1342 - mae: 0.0105 - val_loss: 0.1322 - val_mae: 0.0036\n",
      "Epoch 21/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1307 - mae: 0.0116 - val_loss: 0.1287 - val_mae: 0.0045\n",
      "Epoch 22/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1270 - mae: 0.0068 - val_loss: 0.1252 - val_mae: 0.0031\n",
      "Epoch 23/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1239 - mae: 0.0109 - val_loss: 0.1220 - val_mae: 0.0083\n",
      "Epoch 24/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1204 - mae: 0.0079 - val_loss: 0.1188 - val_mae: 0.0097\n",
      "Epoch 25/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1172 - mae: 0.0084 - val_loss: 0.1155 - val_mae: 0.0051\n",
      "Epoch 26/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1141 - mae: 0.0108 - val_loss: 0.1124 - val_mae: 0.0069\n",
      "Epoch 27/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1111 - mae: 0.0109 - val_loss: 0.1095 - val_mae: 0.0100\n",
      "Epoch 28/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1081 - mae: 0.0124 - val_loss: 0.1063 - val_mae: 0.0030\n",
      "Epoch 29/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1050 - mae: 0.0077 - val_loss: 0.1038 - val_mae: 0.0166\n",
      "Epoch 30/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1021 - mae: 0.0058 - val_loss: 0.1009 - val_mae: 0.0149\n",
      "Epoch 31/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0993 - mae: 0.0084 - val_loss: 0.0980 - val_mae: 0.0141\n",
      "Epoch 32/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0965 - mae: 0.0089 - val_loss: 0.0952 - val_mae: 0.0120\n",
      "Epoch 33/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0938 - mae: 0.0097 - val_loss: 0.0924 - val_mae: 0.0093\n",
      "Epoch 34/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0911 - mae: 0.0081 - val_loss: 0.0897 - val_mae: 0.0060\n",
      "Epoch 35/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0885 - mae: 0.0094 - val_loss: 0.0871 - val_mae: 0.0074\n",
      "Epoch 36/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0859 - mae: 0.0083 - val_loss: 0.0846 - val_mae: 0.0088\n",
      "Epoch 37/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0833 - mae: 0.0080 - val_loss: 0.0820 - val_mae: 0.0059\n",
      "Epoch 38/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0808 - mae: 0.0086 - val_loss: 0.0795 - val_mae: 0.0035\n",
      "Epoch 39/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0785 - mae: 0.0088 - val_loss: 0.0771 - val_mae: 0.0054\n",
      "Epoch 40/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0762 - mae: 0.0108 - val_loss: 0.0747 - val_mae: 0.0030\n",
      "Epoch 41/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0736 - mae: 0.0059 - val_loss: 0.0726 - val_mae: 0.0148\n",
      "Epoch 42/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0714 - mae: 0.0089 - val_loss: 0.0701 - val_mae: 0.0026\n",
      "Epoch 43/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0691 - mae: 0.0090 - val_loss: 0.0679 - val_mae: 0.0059\n",
      "Epoch 44/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0670 - mae: 0.0098 - val_loss: 0.0665 - val_mae: 0.0243\n",
      "Epoch 45/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0647 - mae: 0.0086 - val_loss: 0.0635 - val_mae: 0.0024\n",
      "Epoch 46/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0625 - mae: 0.0077 - val_loss: 0.0619 - val_mae: 0.0220\n",
      "Epoch 47/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0606 - mae: 0.0115 - val_loss: 0.0596 - val_mae: 0.0165\n",
      "Epoch 48/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0584 - mae: 0.0083 - val_loss: 0.0574 - val_mae: 0.0121\n",
      "Epoch 49/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0564 - mae: 0.0079 - val_loss: 0.0553 - val_mae: 0.0080\n",
      "Epoch 50/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0547 - mae: 0.0118 - val_loss: 0.0535 - val_mae: 0.0100\n",
      "Epoch 51/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0525 - mae: 0.0062 - val_loss: 0.0515 - val_mae: 0.0062\n",
      "Epoch 52/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0506 - mae: 0.0076 - val_loss: 0.0496 - val_mae: 0.0035\n",
      "Epoch 53/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0488 - mae: 0.0089 - val_loss: 0.0478 - val_mae: 0.0066\n",
      "Epoch 54/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0471 - mae: 0.0080 - val_loss: 0.0460 - val_mae: 0.0047\n",
      "Epoch 55/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0453 - mae: 0.0100 - val_loss: 0.0444 - val_mae: 0.0076\n",
      "Epoch 56/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0439 - mae: 0.0131 - val_loss: 0.0429 - val_mae: 0.0136\n",
      "Epoch 57/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0419 - mae: 0.0058 - val_loss: 0.0412 - val_mae: 0.0142\n",
      "Epoch 58/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0403 - mae: 0.0081 - val_loss: 0.0394 - val_mae: 0.0056\n",
      "Epoch 59/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0389 - mae: 0.0108 - val_loss: 0.0379 - val_mae: 0.0074\n",
      "Epoch 60/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0372 - mae: 0.0066 - val_loss: 0.0364 - val_mae: 0.0062\n",
      "Epoch 61/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0358 - mae: 0.0098 - val_loss: 0.0362 - val_mae: 0.0293\n",
      "Epoch 62/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0343 - mae: 0.0085 - val_loss: 0.0335 - val_mae: 0.0042\n",
      "Epoch 63/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0329 - mae: 0.0078 - val_loss: 0.0323 - val_mae: 0.0135\n",
      "Epoch 64/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0316 - mae: 0.0100 - val_loss: 0.0314 - val_mae: 0.0222\n",
      "Epoch 65/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0303 - mae: 0.0103 - val_loss: 0.0295 - val_mae: 0.0061\n",
      "Epoch 66/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0292 - mae: 0.0119 - val_loss: 0.0294 - val_mae: 0.0251\n",
      "Epoch 67/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0278 - mae: 0.0083 - val_loss: 0.0270 - val_mae: 0.0030\n",
      "Epoch 68/1000\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0264 - mae: 0.0044 - val_loss: 0.0258 - val_mae: 0.0028\n",
      "Epoch 69/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0254 - mae: 0.0083 - val_loss: 0.0252 - val_mae: 0.0193\n",
      "Epoch 70/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0243 - mae: 0.0091 - val_loss: 0.0236 - val_mae: 0.0026\n",
      "Epoch 71/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0233 - mae: 0.0091 - val_loss: 0.0226 - val_mae: 0.0041\n",
      "Epoch 72/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0223 - mae: 0.0098 - val_loss: 0.0216 - val_mae: 0.0052\n",
      "Epoch 73/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0213 - mae: 0.0109 - val_loss: 0.0206 - val_mae: 0.0025\n",
      "Epoch 74/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0201 - mae: 0.0047 - val_loss: 0.0196 - val_mae: 0.0028\n",
      "Epoch 75/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0193 - mae: 0.0086 - val_loss: 0.0187 - val_mae: 0.0031\n",
      "Epoch 76/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0184 - mae: 0.0084 - val_loss: 0.0178 - val_mae: 0.0034\n",
      "Epoch 77/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0177 - mae: 0.0114 - val_loss: 0.0172 - val_mae: 0.0120\n",
      "Epoch 78/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0167 - mae: 0.0065 - val_loss: 0.0164 - val_mae: 0.0144\n",
      "Epoch 79/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0160 - mae: 0.0103 - val_loss: 0.0158 - val_mae: 0.0159\n",
      "Epoch 80/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0152 - mae: 0.0085 - val_loss: 0.0150 - val_mae: 0.0170\n",
      "Epoch 81/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0145 - mae: 0.0103 - val_loss: 0.0142 - val_mae: 0.0124\n",
      "Epoch 82/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0138 - mae: 0.0092 - val_loss: 0.0151 - val_mae: 0.0359\n",
      "Epoch 83/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0131 - mae: 0.0086 - val_loss: 0.0128 - val_mae: 0.0117\n",
      "Epoch 84/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0126 - mae: 0.0115 - val_loss: 0.0122 - val_mae: 0.0120\n",
      "Epoch 85/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0119 - mae: 0.0073 - val_loss: 0.0116 - val_mae: 0.0102\n",
      "Epoch 86/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0114 - mae: 0.0095 - val_loss: 0.0110 - val_mae: 0.0091\n",
      "Epoch 87/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0109 - mae: 0.0102 - val_loss: 0.0109 - val_mae: 0.0202\n",
      "Epoch 88/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0103 - mae: 0.0084 - val_loss: 0.0099 - val_mae: 0.0047\n",
      "Epoch 89/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0097 - mae: 0.0082 - val_loss: 0.0095 - val_mae: 0.0086\n",
      "Epoch 90/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0094 - mae: 0.0107 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 91/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0088 - mae: 0.0075 - val_loss: 0.0085 - val_mae: 0.0054\n",
      "Epoch 92/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0085 - mae: 0.0090 - val_loss: 0.0081 - val_mae: 0.0036\n",
      "Epoch 93/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0081 - mae: 0.0086 - val_loss: 0.0078 - val_mae: 0.0085\n",
      "Epoch 94/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0077 - mae: 0.0089 - val_loss: 0.0085 - val_mae: 0.0317\n",
      "Epoch 95/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0073 - mae: 0.0094 - val_loss: 0.0071 - val_mae: 0.0085\n",
      "Epoch 96/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0070 - mae: 0.0099 - val_loss: 0.0069 - val_mae: 0.0123\n",
      "Epoch 97/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0067 - mae: 0.0098 - val_loss: 0.0066 - val_mae: 0.0123\n",
      "Epoch 98/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0065 - mae: 0.0125 - val_loss: 0.0063 - val_mae: 0.0122\n",
      "Epoch 99/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0060 - mae: 0.0064 - val_loss: 0.0059 - val_mae: 0.0082\n",
      "Epoch 100/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0059 - mae: 0.0102 - val_loss: 0.0057 - val_mae: 0.0080\n",
      "Epoch 101/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0055 - mae: 0.0077 - val_loss: 0.0059 - val_mae: 0.0238\n",
      "Epoch 102/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0054 - mae: 0.0099 - val_loss: 0.0060 - val_mae: 0.0286\n",
      "Epoch 103/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0052 - mae: 0.0097 - val_loss: 0.0049 - val_mae: 0.0036\n",
      "Epoch 104/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0048 - mae: 0.0065 - val_loss: 0.0047 - val_mae: 0.0070\n",
      "Epoch 105/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0047 - mae: 0.0104 - val_loss: 0.0045 - val_mae: 0.0071\n",
      "Epoch 106/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0045 - mae: 0.0074 - val_loss: 0.0043 - val_mae: 0.0037\n",
      "Epoch 107/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0046 - mae: 0.0122 - val_loss: 0.0042 - val_mae: 0.0050\n",
      "Epoch 108/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0041 - mae: 0.0042 - val_loss: 0.0040 - val_mae: 0.0034\n",
      "Epoch 109/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0041 - mae: 0.0099 - val_loss: 0.0039 - val_mae: 0.0058\n",
      "Epoch 110/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0040 - mae: 0.0100 - val_loss: 0.0039 - val_mae: 0.0112\n",
      "Epoch 111/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0037 - mae: 0.0076 - val_loss: 0.0036 - val_mae: 0.0037\n",
      "Epoch 112/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0036 - mae: 0.0076 - val_loss: 0.0035 - val_mae: 0.0039\n",
      "Epoch 113/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0037 - mae: 0.0115 - val_loss: 0.0034 - val_mae: 0.0065\n",
      "Epoch 114/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0033 - mae: 0.0051 - val_loss: 0.0033 - val_mae: 0.0085\n",
      "Epoch 115/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0034 - mae: 0.0102 - val_loss: 0.0035 - val_mae: 0.0186\n",
      "Epoch 116/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0032 - mae: 0.0089 - val_loss: 0.0031 - val_mae: 0.0075\n",
      "Epoch 117/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0031 - mae: 0.0092 - val_loss: 0.0030 - val_mae: 0.0052\n",
      "Epoch 118/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0030 - mae: 0.0088 - val_loss: 0.0030 - val_mae: 0.0098\n",
      "Epoch 119/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0030 - mae: 0.0085 - val_loss: 0.0032 - val_mae: 0.0187\n",
      "Epoch 120/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0030 - mae: 0.0110 - val_loss: 0.0028 - val_mae: 0.0084\n",
      "Epoch 121/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0080 - val_loss: 0.0027 - val_mae: 0.0062\n",
      "Epoch 122/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0028 - mae: 0.0097 - val_loss: 0.0027 - val_mae: 0.0069\n",
      "Epoch 123/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0085 - val_loss: 0.0026 - val_mae: 0.0067\n",
      "Epoch 124/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0105 - val_loss: 0.0025 - val_mae: 0.0037\n",
      "Epoch 125/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0026 - mae: 0.0077 - val_loss: 0.0025 - val_mae: 0.0071\n",
      "Epoch 126/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0064 - val_loss: 0.0024 - val_mae: 0.0046\n",
      "Epoch 127/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0094 - val_loss: 0.0025 - val_mae: 0.0086\n",
      "Epoch 128/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0025 - mae: 0.0109 - val_loss: 0.0023 - val_mae: 0.0032\n",
      "Epoch 129/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0095 - val_loss: 0.0023 - val_mae: 0.0040\n",
      "Epoch 130/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0024 - mae: 0.0082 - val_loss: 0.0023 - val_mae: 0.0090\n",
      "Epoch 131/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0024 - mae: 0.0093 - val_loss: 0.0022 - val_mae: 0.0043\n",
      "Epoch 132/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0024 - mae: 0.0099 - val_loss: 0.0022 - val_mae: 0.0057\n",
      "Epoch 133/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0022 - mae: 0.0071 - val_loss: 0.0022 - val_mae: 0.0050\n",
      "Epoch 134/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0102 - val_loss: 0.0022 - val_mae: 0.0084\n",
      "Epoch 135/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0022 - mae: 0.0080 - val_loss: 0.0022 - val_mae: 0.0095\n",
      "Epoch 136/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0023 - mae: 0.0102 - val_loss: 0.0021 - val_mae: 0.0055\n",
      "Epoch 137/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0023 - mae: 0.0107 - val_loss: 0.0022 - val_mae: 0.0102\n",
      "Epoch 138/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0022 - mae: 0.0083 - val_loss: 0.0023 - val_mae: 0.0136\n",
      "Epoch 139/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0086 - val_loss: 0.0021 - val_mae: 0.0069\n",
      "Epoch 140/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0086 - val_loss: 0.0021 - val_mae: 0.0108\n",
      "Epoch 141/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0022 - mae: 0.0110 - val_loss: 0.0020 - val_mae: 0.0063\n",
      "Epoch 142/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0072 - val_loss: 0.0022 - val_mae: 0.0135\n",
      "Epoch 143/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0084 - val_loss: 0.0020 - val_mae: 0.0065\n",
      "Epoch 144/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0096 - val_loss: 0.0020 - val_mae: 0.0076\n",
      "Epoch 145/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0062 - val_loss: 0.0019 - val_mae: 0.0036\n",
      "Epoch 146/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0101 - val_loss: 0.0019 - val_mae: 0.0038\n",
      "Epoch 147/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0057 - val_loss: 0.0022 - val_mae: 0.0163\n",
      "Epoch 148/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0098 - val_loss: 0.0019 - val_mae: 0.0056\n",
      "Epoch 149/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0075 - val_loss: 0.0020 - val_mae: 0.0102\n",
      "Epoch 150/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0117 - val_loss: 0.0019 - val_mae: 0.0040\n",
      "Epoch 151/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0049\n",
      "Epoch 152/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0021 - mae: 0.0109 - val_loss: 0.0019 - val_mae: 0.0040\n",
      "Epoch 153/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0066 - val_loss: 0.0019 - val_mae: 0.0073\n",
      "Epoch 154/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0096 - val_loss: 0.0023 - val_mae: 0.0192\n",
      "Epoch 155/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0020 - mae: 0.0095 - val_loss: 0.0019 - val_mae: 0.0102\n",
      "Epoch 156/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0019 - mae: 0.0078 - val_loss: 0.0019 - val_mae: 0.0073\n",
      "Epoch 157/1000\n",
      "79/81 [============================>.] - ETA: 0s - loss: 0.0020 - mae: 0.0104Restoring model weights from the end of the best epoch: 152.\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0020 - mae: 0.0103 - val_loss: 0.0019 - val_mae: 0.0080\n",
      "Epoch 157: early stopping\n",
      "Training für Fold 5...\n",
      "Epoch 1/1000\n",
      "81/81 [==============================] - 3s 15ms/step - loss: 0.4844 - mae: 0.2373 - val_loss: 0.3007 - val_mae: 0.0951\n",
      "Epoch 2/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2578 - mae: 0.0346 - val_loss: 0.2380 - val_mae: 0.0231\n",
      "Epoch 3/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2293 - mae: 0.0231 - val_loss: 0.2205 - val_mae: 0.0147\n",
      "Epoch 4/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.2153 - mae: 0.0187 - val_loss: 0.2093 - val_mae: 0.0071\n",
      "Epoch 5/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.2055 - mae: 0.0172 - val_loss: 0.2004 - val_mae: 0.0077\n",
      "Epoch 6/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1969 - mae: 0.0151 - val_loss: 0.1930 - val_mae: 0.0184\n",
      "Epoch 7/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1892 - mae: 0.0123 - val_loss: 0.1879 - val_mae: 0.0402\n",
      "Epoch 8/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1825 - mae: 0.0146 - val_loss: 0.1792 - val_mae: 0.0177\n",
      "Epoch 9/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1761 - mae: 0.0137 - val_loss: 0.1726 - val_mae: 0.0054\n",
      "Epoch 10/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1701 - mae: 0.0124 - val_loss: 0.1669 - val_mae: 0.0048\n",
      "Epoch 11/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1645 - mae: 0.0122 - val_loss: 0.1616 - val_mae: 0.0114\n",
      "Epoch 12/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1591 - mae: 0.0104 - val_loss: 0.1565 - val_mae: 0.0134\n",
      "Epoch 13/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1542 - mae: 0.0127 - val_loss: 0.1518 - val_mae: 0.0153\n",
      "Epoch 14/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1493 - mae: 0.0106 - val_loss: 0.1469 - val_mae: 0.0066\n",
      "Epoch 15/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1452 - mae: 0.0155 - val_loss: 0.1424 - val_mae: 0.0052\n",
      "Epoch 16/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1403 - mae: 0.0057 - val_loss: 0.1383 - val_mae: 0.0104\n",
      "Epoch 17/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1364 - mae: 0.0099 - val_loss: 0.1344 - val_mae: 0.0151\n",
      "Epoch 18/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1326 - mae: 0.0131 - val_loss: 0.1302 - val_mae: 0.0060\n",
      "Epoch 19/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1284 - mae: 0.0079 - val_loss: 0.1265 - val_mae: 0.0066\n",
      "Epoch 20/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1248 - mae: 0.0087 - val_loss: 0.1233 - val_mae: 0.0176\n",
      "Epoch 21/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.1213 - mae: 0.0094 - val_loss: 0.1193 - val_mae: 0.0073\n",
      "Epoch 22/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1178 - mae: 0.0092 - val_loss: 0.1159 - val_mae: 0.0047\n",
      "Epoch 23/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1143 - mae: 0.0072 - val_loss: 0.1127 - val_mae: 0.0102\n",
      "Epoch 24/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1111 - mae: 0.0086 - val_loss: 0.1094 - val_mae: 0.0062\n",
      "Epoch 25/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1081 - mae: 0.0119 - val_loss: 0.1062 - val_mae: 0.0062\n",
      "Epoch 26/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1048 - mae: 0.0072 - val_loss: 0.1032 - val_mae: 0.0081\n",
      "Epoch 27/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.1019 - mae: 0.0094 - val_loss: 0.1002 - val_mae: 0.0033\n",
      "Epoch 28/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0990 - mae: 0.0105 - val_loss: 0.0978 - val_mae: 0.0216\n",
      "Epoch 29/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0960 - mae: 0.0079 - val_loss: 0.0944 - val_mae: 0.0036\n",
      "Epoch 30/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0931 - mae: 0.0062 - val_loss: 0.0917 - val_mae: 0.0028\n",
      "Epoch 31/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0905 - mae: 0.0102 - val_loss: 0.0891 - val_mae: 0.0083\n",
      "Epoch 32/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0879 - mae: 0.0102 - val_loss: 0.0864 - val_mae: 0.0064\n",
      "Epoch 33/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0851 - mae: 0.0064 - val_loss: 0.0839 - val_mae: 0.0112\n",
      "Epoch 34/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0826 - mae: 0.0089 - val_loss: 0.0813 - val_mae: 0.0059\n",
      "Epoch 35/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0801 - mae: 0.0076 - val_loss: 0.0788 - val_mae: 0.0089\n",
      "Epoch 36/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0780 - mae: 0.0128 - val_loss: 0.0763 - val_mae: 0.0057\n",
      "Epoch 37/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0752 - mae: 0.0050 - val_loss: 0.0739 - val_mae: 0.0042\n",
      "Epoch 38/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0731 - mae: 0.0112 - val_loss: 0.0717 - val_mae: 0.0079\n",
      "Epoch 39/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0705 - mae: 0.0061 - val_loss: 0.0694 - val_mae: 0.0048\n",
      "Epoch 40/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0684 - mae: 0.0095 - val_loss: 0.0672 - val_mae: 0.0068\n",
      "Epoch 41/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0661 - mae: 0.0077 - val_loss: 0.0650 - val_mae: 0.0083\n",
      "Epoch 42/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0641 - mae: 0.0125 - val_loss: 0.0633 - val_mae: 0.0184\n",
      "Epoch 43/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0619 - mae: 0.0080 - val_loss: 0.0607 - val_mae: 0.0047\n",
      "Epoch 44/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0599 - mae: 0.0100 - val_loss: 0.0588 - val_mae: 0.0085\n",
      "Epoch 45/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0578 - mae: 0.0082 - val_loss: 0.0575 - val_mae: 0.0222\n",
      "Epoch 46/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0559 - mae: 0.0112 - val_loss: 0.0548 - val_mae: 0.0088\n",
      "Epoch 47/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0539 - mae: 0.0084 - val_loss: 0.0534 - val_mae: 0.0176\n",
      "Epoch 48/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0520 - mae: 0.0079 - val_loss: 0.0510 - val_mae: 0.0046\n",
      "Epoch 49/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0502 - mae: 0.0099 - val_loss: 0.0494 - val_mae: 0.0150\n",
      "Epoch 50/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0484 - mae: 0.0088 - val_loss: 0.0475 - val_mae: 0.0108\n",
      "Epoch 51/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0469 - mae: 0.0127 - val_loss: 0.0456 - val_mae: 0.0039\n",
      "Epoch 52/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0448 - mae: 0.0053 - val_loss: 0.0439 - val_mae: 0.0037\n",
      "Epoch 53/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0433 - mae: 0.0100 - val_loss: 0.0424 - val_mae: 0.0088\n",
      "Epoch 54/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0416 - mae: 0.0071 - val_loss: 0.0408 - val_mae: 0.0070\n",
      "Epoch 55/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0401 - mae: 0.0105 - val_loss: 0.0392 - val_mae: 0.0075\n",
      "Epoch 56/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0385 - mae: 0.0090 - val_loss: 0.0404 - val_mae: 0.0489\n",
      "Epoch 57/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0371 - mae: 0.0096 - val_loss: 0.0363 - val_mae: 0.0113\n",
      "Epoch 58/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0356 - mae: 0.0099 - val_loss: 0.0352 - val_mae: 0.0192\n",
      "Epoch 59/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0341 - mae: 0.0071 - val_loss: 0.0333 - val_mae: 0.0048\n",
      "Epoch 60/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0329 - mae: 0.0119 - val_loss: 0.0320 - val_mae: 0.0055\n",
      "Epoch 61/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0314 - mae: 0.0069 - val_loss: 0.0309 - val_mae: 0.0130\n",
      "Epoch 62/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0301 - mae: 0.0080 - val_loss: 0.0294 - val_mae: 0.0045\n",
      "Epoch 63/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0289 - mae: 0.0093 - val_loss: 0.0283 - val_mae: 0.0094\n",
      "Epoch 64/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0277 - mae: 0.0094 - val_loss: 0.0272 - val_mae: 0.0148\n",
      "Epoch 65/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0265 - mae: 0.0086 - val_loss: 0.0258 - val_mae: 0.0050\n",
      "Epoch 66/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0255 - mae: 0.0100 - val_loss: 0.0247 - val_mae: 0.0034\n",
      "Epoch 67/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0242 - mae: 0.0061 - val_loss: 0.0236 - val_mae: 0.0037\n",
      "Epoch 68/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0233 - mae: 0.0115 - val_loss: 0.0228 - val_mae: 0.0144\n",
      "Epoch 69/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0222 - mae: 0.0082 - val_loss: 0.0219 - val_mae: 0.0142\n",
      "Epoch 70/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0212 - mae: 0.0079 - val_loss: 0.0213 - val_mae: 0.0206\n",
      "Epoch 71/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0204 - mae: 0.0111 - val_loss: 0.0201 - val_mae: 0.0165\n",
      "Epoch 72/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0194 - mae: 0.0085 - val_loss: 0.0189 - val_mae: 0.0075\n",
      "Epoch 73/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0185 - mae: 0.0074 - val_loss: 0.0179 - val_mae: 0.0036\n",
      "Epoch 74/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0177 - mae: 0.0097 - val_loss: 0.0171 - val_mae: 0.0037\n",
      "Epoch 75/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0170 - mae: 0.0109 - val_loss: 0.0164 - val_mae: 0.0093\n",
      "Epoch 76/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0160 - mae: 0.0073 - val_loss: 0.0157 - val_mae: 0.0102\n",
      "Epoch 77/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0154 - mae: 0.0099 - val_loss: 0.0150 - val_mae: 0.0113\n",
      "Epoch 78/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0146 - mae: 0.0075 - val_loss: 0.0143 - val_mae: 0.0092\n",
      "Epoch 79/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0145 - mae: 0.0165 - val_loss: 0.0138 - val_mae: 0.0156\n",
      "Epoch 80/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0133 - mae: 0.0062 - val_loss: 0.0129 - val_mae: 0.0040\n",
      "Epoch 81/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0126 - mae: 0.0061 - val_loss: 0.0123 - val_mae: 0.0042\n",
      "Epoch 82/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0121 - mae: 0.0079 - val_loss: 0.0117 - val_mae: 0.0037\n",
      "Epoch 83/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0116 - mae: 0.0086 - val_loss: 0.0112 - val_mae: 0.0040\n",
      "Epoch 84/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0111 - mae: 0.0101 - val_loss: 0.0107 - val_mae: 0.0054\n",
      "Epoch 85/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0105 - mae: 0.0088 - val_loss: 0.0107 - val_mae: 0.0173\n",
      "Epoch 86/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0100 - mae: 0.0073 - val_loss: 0.0098 - val_mae: 0.0095\n",
      "Epoch 87/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0097 - mae: 0.0112 - val_loss: 0.0092 - val_mae: 0.0039\n",
      "Epoch 88/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0091 - mae: 0.0077 - val_loss: 0.0089 - val_mae: 0.0092\n",
      "Epoch 89/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0088 - mae: 0.0112 - val_loss: 0.0084 - val_mae: 0.0059\n",
      "Epoch 90/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0083 - mae: 0.0082 - val_loss: 0.0082 - val_mae: 0.0126\n",
      "Epoch 91/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0080 - mae: 0.0096 - val_loss: 0.0079 - val_mae: 0.0141\n",
      "Epoch 92/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0076 - mae: 0.0096 - val_loss: 0.0073 - val_mae: 0.0050\n",
      "Epoch 93/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0073 - mae: 0.0098 - val_loss: 0.0073 - val_mae: 0.0179\n",
      "Epoch 94/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0069 - mae: 0.0084 - val_loss: 0.0067 - val_mae: 0.0050\n",
      "Epoch 95/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0068 - mae: 0.0109 - val_loss: 0.0064 - val_mae: 0.0038\n",
      "Epoch 96/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0063 - mae: 0.0079 - val_loss: 0.0063 - val_mae: 0.0121\n",
      "Epoch 97/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0061 - mae: 0.0088 - val_loss: 0.0059 - val_mae: 0.0106\n",
      "Epoch 98/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0061 - mae: 0.0122 - val_loss: 0.0056 - val_mae: 0.0057\n",
      "Epoch 99/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0056 - mae: 0.0076 - val_loss: 0.0060 - val_mae: 0.0188\n",
      "Epoch 100/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0054 - mae: 0.0096 - val_loss: 0.0052 - val_mae: 0.0081\n",
      "Epoch 101/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0052 - mae: 0.0093 - val_loss: 0.0056 - val_mae: 0.0227\n",
      "Epoch 102/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0050 - mae: 0.0094 - val_loss: 0.0048 - val_mae: 0.0054\n",
      "Epoch 103/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0048 - mae: 0.0087 - val_loss: 0.0049 - val_mae: 0.0157\n",
      "Epoch 104/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0047 - mae: 0.0102 - val_loss: 0.0045 - val_mae: 0.0094\n",
      "Epoch 105/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0045 - mae: 0.0091 - val_loss: 0.0044 - val_mae: 0.0099\n",
      "Epoch 106/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0044 - mae: 0.0104 - val_loss: 0.0041 - val_mae: 0.0059\n",
      "Epoch 107/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0042 - mae: 0.0094 - val_loss: 0.0040 - val_mae: 0.0034\n",
      "Epoch 108/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0042 - mae: 0.0125 - val_loss: 0.0039 - val_mae: 0.0038\n",
      "Epoch 109/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0039 - mae: 0.0083 - val_loss: 0.0037 - val_mae: 0.0042\n",
      "Epoch 110/1000\n",
      "81/81 [==============================] - 1s 14ms/step - loss: 0.0038 - mae: 0.0099 - val_loss: 0.0037 - val_mae: 0.0053\n",
      "Epoch 111/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0037 - mae: 0.0089 - val_loss: 0.0037 - val_mae: 0.0116\n",
      "Epoch 112/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0038 - mae: 0.0137 - val_loss: 0.0034 - val_mae: 0.0039\n",
      "Epoch 113/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0034 - mae: 0.0058 - val_loss: 0.0046 - val_mae: 0.0288\n",
      "Epoch 114/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0034 - mae: 0.0086 - val_loss: 0.0033 - val_mae: 0.0057\n",
      "Epoch 115/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0034 - mae: 0.0106 - val_loss: 0.0032 - val_mae: 0.0069\n",
      "Epoch 116/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0032 - mae: 0.0091 - val_loss: 0.0031 - val_mae: 0.0037\n",
      "Epoch 117/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0033 - mae: 0.0115 - val_loss: 0.0032 - val_mae: 0.0107\n",
      "Epoch 118/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0031 - mae: 0.0072 - val_loss: 0.0031 - val_mae: 0.0087\n",
      "Epoch 119/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0031 - mae: 0.0099 - val_loss: 0.0029 - val_mae: 0.0043\n",
      "Epoch 120/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0030 - mae: 0.0090 - val_loss: 0.0031 - val_mae: 0.0155\n",
      "Epoch 121/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0031 - mae: 0.0108 - val_loss: 0.0028 - val_mae: 0.0041\n",
      "Epoch 122/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0030 - mae: 0.0105 - val_loss: 0.0029 - val_mae: 0.0097\n",
      "Epoch 123/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0061 - val_loss: 0.0028 - val_mae: 0.0075\n",
      "Epoch 124/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0029 - mae: 0.0105 - val_loss: 0.0029 - val_mae: 0.0132\n",
      "Epoch 125/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0099 - val_loss: 0.0027 - val_mae: 0.0064\n",
      "Epoch 126/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0102 - val_loss: 0.0026 - val_mae: 0.0038\n",
      "Epoch 127/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0027 - mae: 0.0075 - val_loss: 0.0026 - val_mae: 0.0061\n",
      "Epoch 128/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0110 - val_loss: 0.0026 - val_mae: 0.0086\n",
      "Epoch 129/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0086 - val_loss: 0.0025 - val_mae: 0.0056\n",
      "Epoch 130/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0097 - val_loss: 0.0027 - val_mae: 0.0123\n",
      "Epoch 131/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0097 - val_loss: 0.0025 - val_mae: 0.0050\n",
      "Epoch 132/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0093 - val_loss: 0.0025 - val_mae: 0.0050\n",
      "Epoch 133/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0094 - val_loss: 0.0024 - val_mae: 0.0039\n",
      "Epoch 134/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0026 - mae: 0.0105 - val_loss: 0.0024 - val_mae: 0.0059\n",
      "Epoch 135/1000\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0025 - mae: 0.0079 - val_loss: 0.0025 - val_mae: 0.0084\n",
      "Epoch 136/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0025 - mae: 0.0107 - val_loss: 0.0028 - val_mae: 0.0181\n",
      "Epoch 137/1000\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0025 - mae: 0.0098 - val_loss: 0.0025 - val_mae: 0.0110\n",
      "Epoch 138/1000\n",
      "81/81 [==============================] - ETA: 0s - loss: 0.0027 - mae: 0.0126Restoring model weights from the end of the best epoch: 133.\n",
      "81/81 [==============================] - 1s 13ms/step - loss: 0.0027 - mae: 0.0126 - val_loss: 0.0025 - val_mae: 0.0123\n",
      "Epoch 138: early stopping\n",
      "Durchschnittlicher Validation Loss: 0.002037907624617219\n",
      "Durchschnittlicher Validation MAE: 0.0025905787479132414\n"
     ]
    }
   ],
   "source": [
    "# Initialisiere Listen, um Ergebnisse zu speichern\n",
    "val_loss_results = []\n",
    "val_mae_results = []\n",
    "\n",
    "# Funktion, um das Modell zu erstellen\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "            Dense(320, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "        \n",
    "            Dense(176, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "        \n",
    "            Dense(288, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "        \n",
    "            Dense(192, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "        \n",
    "            Dense(208, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "        \n",
    "            Dense(224, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(80, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(304, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(240, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(48, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(1 , activation = 'linear')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-Validation Konfiguration\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Leistungsüberwachung\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_scaled[train_index], y_train_scaled[val_index]\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "    print(f'Training für Fold {fold_no}...')\n",
    "    history = model.fit(X_train_fold, y_train_fold, batch_size=800, epochs=1000, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    # Speichere die Ergebnisse des aktuellen Folds\n",
    "    val_loss_results.append(min(history.history['val_loss']))\n",
    "    val_mae_results.append(min(history.history['val_mae']))\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "# Berechne den Durchschnitt über alle Folds\n",
    "average_val_loss = np.mean(val_loss_results)\n",
    "average_val_mae = np.mean(val_mae_results)\n",
    "\n",
    "# Gib die durchschnittlichen Ergebnisse aus\n",
    "print(f'Durchschnittlicher Validation Loss: {average_val_loss}')\n",
    "print(f'Durchschnittlicher Validation MAE: {average_val_mae}')\n",
    "\n",
    "# Umwandeln der Listen in Pandas DataFrames\n",
    "val_loss_df = pd.DataFrame(val_loss_results, columns=['Validation Loss'])\n",
    "val_mae_df = pd.DataFrame(val_mae_results, columns=['Validation MAE'])\n",
    "\n",
    "# Speichern der DataFrames in CSV-Dateien\n",
    "val_loss_df.to_csv('val_loss_results_D1_3.csv', index=False)\n",
    "val_mae_df.to_csv('val_mae_results_D1_3.csv', index=False)\n",
    "\n",
    "# Gib die durchschnittlichen Ergebnisse aus\n",
    "print(f'Durchschnittlicher Validation Loss: {average_val_loss}')\n",
    "print(f'Durchschnittlicher Validation MAE: {average_val_mae}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T09:46:23.150788600Z",
     "start_time": "2024-03-18T09:33:24.348341700Z"
    }
   },
   "id": "29c288dd786bd1cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Auswertung des NeuroNetz auf den Testdatensatz"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d8d88143abaf36"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630/630 - 1s - loss: 2.6301e-04 - mae: 0.0023 - 1s/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.0002630056696943939, 0.0023403016384691]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test_scaled, y_test_scaled, verbose=2)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T14:05:13.168016800Z",
     "start_time": "2024-04-03T14:05:12.025771800Z"
    }
   },
   "id": "68d86893ad985b02"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f6f672a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T13:58:22.560352300Z",
     "start_time": "2024-03-19T13:58:20.999665200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bsp. Predicted: [1194.6018] Actual: [1195.9] \n",
      "Durchschnittliche Abweichung (MAE): [1.01075488]\n"
     ]
    }
   ],
   "source": [
    "#Rückrechnung des skalierten MAE zum unskalierten MAE für eines bessere Einschätzung des Ergebnisses\n",
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Rücktransformation der skalierten Werte\n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[1000]} Actual: {original_actual_values[1000]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "\n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "\n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: [0.99995427]\n"
     ]
    }
   ],
   "source": [
    "#Berechnung der Auswertungsgröße R^2\n",
    "def calculate_r_squared(predicted, actual):\n",
    "    # Berechnung des Mittelwerts der tatsächlichen Werte\n",
    "    mean_actual = sum(actual) / len(actual)\n",
    "    \n",
    "    # Berechnung der totalen Summe der Quadrate (SST)\n",
    "    sst = sum((x - mean_actual) ** 2 for x in actual)\n",
    "    \n",
    "    # Berechnung der Summe der Quadrate der Residuen (SSE)\n",
    "    sse = sum((actual[i] - predicted[i]) ** 2 for i in range(len(actual)))\n",
    "    \n",
    "    # Berechnung des R^2-Wertes\n",
    "    r_squared = 1 - (sse / sst)\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "# Berechnung von R^2 mit den bereitgestellten Listen\n",
    "r_squared = calculate_r_squared(list1, list2)\n",
    "\n",
    "print(f\"R^2: {r_squared}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:58:35.670855900Z",
     "start_time": "2024-03-19T13:58:35.516074700Z"
    }
   },
   "id": "48ac8cdcc05e55fd"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Werte die kleiner sind: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "             Echt  Vorhergesagt  X-Koordinate  Y-Koordinate  Differenz\n19281  791.695862        807.84         1.000        0.9125 -16.144138\n15566  755.544312        771.22         0.996        0.9250 -15.675688\n14143  817.508423        832.75         1.000        0.9025 -15.241577\n16514  823.677979        838.63         1.000        0.9000 -14.952021\n9896   776.956909        791.74         0.992        0.9175 -14.783091\n...           ...           ...           ...           ...        ...\n12315  592.730347        574.95         0.980        0.9975  17.780347\n6946   592.100037        573.83         0.976        1.0000  18.270037\n14915  592.840027        574.24         0.992        0.9975  18.600027\n16491  592.187500        573.06         0.988        1.0000  19.127500\n20012  610.867371        588.92         0.996        0.9750  21.947371\n\n[20131 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Echt</th>\n      <th>Vorhergesagt</th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Differenz</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19281</th>\n      <td>791.695862</td>\n      <td>807.84</td>\n      <td>1.000</td>\n      <td>0.9125</td>\n      <td>-16.144138</td>\n    </tr>\n    <tr>\n      <th>15566</th>\n      <td>755.544312</td>\n      <td>771.22</td>\n      <td>0.996</td>\n      <td>0.9250</td>\n      <td>-15.675688</td>\n    </tr>\n    <tr>\n      <th>14143</th>\n      <td>817.508423</td>\n      <td>832.75</td>\n      <td>1.000</td>\n      <td>0.9025</td>\n      <td>-15.241577</td>\n    </tr>\n    <tr>\n      <th>16514</th>\n      <td>823.677979</td>\n      <td>838.63</td>\n      <td>1.000</td>\n      <td>0.9000</td>\n      <td>-14.952021</td>\n    </tr>\n    <tr>\n      <th>9896</th>\n      <td>776.956909</td>\n      <td>791.74</td>\n      <td>0.992</td>\n      <td>0.9175</td>\n      <td>-14.783091</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12315</th>\n      <td>592.730347</td>\n      <td>574.95</td>\n      <td>0.980</td>\n      <td>0.9975</td>\n      <td>17.780347</td>\n    </tr>\n    <tr>\n      <th>6946</th>\n      <td>592.100037</td>\n      <td>573.83</td>\n      <td>0.976</td>\n      <td>1.0000</td>\n      <td>18.270037</td>\n    </tr>\n    <tr>\n      <th>14915</th>\n      <td>592.840027</td>\n      <td>574.24</td>\n      <td>0.992</td>\n      <td>0.9975</td>\n      <td>18.600027</td>\n    </tr>\n    <tr>\n      <th>16491</th>\n      <td>592.187500</td>\n      <td>573.06</td>\n      <td>0.988</td>\n      <td>1.0000</td>\n      <td>19.127500</td>\n    </tr>\n    <tr>\n      <th>20012</th>\n      <td>610.867371</td>\n      <td>588.92</td>\n      <td>0.996</td>\n      <td>0.9750</td>\n      <td>21.947371</td>\n    </tr>\n  </tbody>\n</table>\n<p>20131 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ausgabe der höchsten Abweichungen zwischen Echt und Vorhergesagt\n",
    "df_result = pd.DataFrame({'Echt': [val[0] for val in list2], 'Vorhergesagt': [val[0] for val in list1]})\n",
    "df_result['X-Koordinate'] = X_test_scaled[:, 0]\n",
    "df_result['Y-Koordinate'] = X_test_scaled[:, 1]\n",
    "\n",
    "df_result['Differenz'] = abs(df_result['Echt'] - df_result['Vorhergesagt'])\n",
    "df_result['Differenz'].sort_values()\n",
    "sorted_df = df_result.sort_values(by= 'Differenz')\n",
    "Anzahl_Punkte = (sorted_df['Differenz'] > 20).sum()\n",
    "print(\"Anzahl der Werte die kleiner sind:\", Anzahl_Punkte)\n",
    "\n",
    "sorted_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:58:41.208434700Z",
     "start_time": "2024-03-19T13:58:41.083538900Z"
    }
   },
   "id": "42bde60d3b4f2007"
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae = history.history['loss']\n",
    "val_mae = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(mae) + 1)\n",
    "# MAE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mae, 'r', label='Training MSE')\n",
    "plt.plot(epochs, val_mae, 'b', label='Validation MSE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Bilder Diplomarbeit/MSE_NeuroNetz/MSE_NeuroNetz_D1_2')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T13:58:51.930920Z",
     "start_time": "2024-03-19T13:58:51.477742600Z"
    }
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# GridSearch\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c177960cc729052"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f17e2cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-15T10:00:01.423818100Z",
     "start_time": "2024-03-15T10:00:01.416947800Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(learning_rate=0.001, activation='relu', regularization=0.0001, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(320, activation=activation, input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(176, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(288, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(192, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(208, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(224, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(80, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(304, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate)) \n",
    "\n",
    "    model.add(Dense(240, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))   \n",
    "\n",
    "    model.add(Dense(48, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = KerasRegressor(model=build_model, verbose=2)\n",
    "\n",
    "# Anpassung der Parameter im param_grid\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'model__regularization': [0.001, 0.0001, 0.00001],\n",
    "    'fit__batch_size': [100, 200, 400, 800],\n",
    "    'fit__epochs': [50],\n",
    "    'model__dropout_rate' : [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "# Beste Parameter und Score ausgeben\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "print(\"Beste Genauigkeit:\", grid_search.best_score_)\n",
    "\n",
    "with open(\"grid_search_D1_2.txt\", \"w\") as f:\n",
    "    f.write(f\"Beste Parameter: {grid_search.best_params_}\\n\")\n",
    "    f.write(f\"Beste Genauigkeit: {grid_search.best_score_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

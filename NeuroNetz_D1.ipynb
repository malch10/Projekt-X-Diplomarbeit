{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b0518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.615514300Z",
     "start_time": "2024-02-29T22:53:26.165838900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ff61b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.643668100Z",
     "start_time": "2024-02-29T22:53:38.615514300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n0             0.0000      -0.00200        500   7000   9000      669.05\n1             0.0000      -0.00199        500   7000   9000      675.83\n2             0.0000      -0.00198        500   7000   9000      682.81\n3             0.0000      -0.00197        500   7000   9000      689.82\n4             0.0000      -0.00196        500   7000   9000      696.80\n...              ...           ...        ...    ...    ...         ...\n100646        0.0025       0.00196        500   7000   9000      578.47\n100647        0.0025       0.00197        500   7000   9000      576.89\n100648        0.0025       0.00198        500   7000   9000      575.32\n100649        0.0025       0.00199        500   7000   9000      573.76\n100650        0.0025       0.00200        500   7000   9000      572.20\n\n[100651 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00199</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>675.83</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00198</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>682.81</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00197</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>689.82</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>696.80</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>100646</th>\n      <td>0.0025</td>\n      <td>0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>578.47</td>\n    </tr>\n    <tr>\n      <th>100647</th>\n      <td>0.0025</td>\n      <td>0.00197</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>576.89</td>\n    </tr>\n    <tr>\n      <th>100648</th>\n      <td>0.0025</td>\n      <td>0.00198</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>575.32</td>\n    </tr>\n    <tr>\n      <th>100649</th>\n      <td>0.0025</td>\n      <td>0.00199</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>573.76</td>\n    </tr>\n    <tr>\n      <th>100650</th>\n      <td>0.0025</td>\n      <td>0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>572.20</td>\n    </tr>\n  </tbody>\n</table>\n<p>100651 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/TPath_300_finish_data.pkl')\n",
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish_D1_I7000_F9000/TPath_500_finish_data_D1.pkl')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966e3c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.649312Z",
     "start_time": "2024-02-29T22:53:38.639668800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Temperatur\n0             0.0000      -0.00200      669.05\n1             0.0000      -0.00199      675.83\n2             0.0000      -0.00198      682.81\n3             0.0000      -0.00197      689.82\n4             0.0000      -0.00196      696.80\n...              ...           ...         ...\n100646        0.0025       0.00196      578.47\n100647        0.0025       0.00197      576.89\n100648        0.0025       0.00198      575.32\n100649        0.0025       0.00199      573.76\n100650        0.0025       0.00200      572.20\n\n[100651 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00199</td>\n      <td>675.83</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00198</td>\n      <td>682.81</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00197</td>\n      <td>689.82</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00196</td>\n      <td>696.80</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>100646</th>\n      <td>0.0025</td>\n      <td>0.00196</td>\n      <td>578.47</td>\n    </tr>\n    <tr>\n      <th>100647</th>\n      <td>0.0025</td>\n      <td>0.00197</td>\n      <td>576.89</td>\n    </tr>\n    <tr>\n      <th>100648</th>\n      <td>0.0025</td>\n      <td>0.00198</td>\n      <td>575.32</td>\n    </tr>\n    <tr>\n      <th>100649</th>\n      <td>0.0025</td>\n      <td>0.00199</td>\n      <td>573.76</td>\n    </tr>\n    <tr>\n      <th>100650</th>\n      <td>0.0025</td>\n      <td>0.00200</td>\n      <td>572.20</td>\n    </tr>\n  </tbody>\n</table>\n<p>100651 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop(data.columns[2:5], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8783d1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.727205500Z",
     "start_time": "2024-02-29T22:53:38.649312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       X-Koordinate  Y-Koordinate  Temperatur\n",
      "83145       0.00207      -0.00062      1282.2\n",
      "66701       0.00166      -0.00065      1347.6\n",
      "91325       0.00227       0.00098      1094.6\n",
      "46593       0.00116      -0.00123      1175.7\n",
      "49518       0.00123      -0.00005      1459.2\n",
      "...             ...           ...         ...\n",
      "6265        0.00015       0.00050      1439.1\n",
      "54886       0.00136       0.00150       876.7\n",
      "76820       0.00191       0.00029      1335.8\n",
      "860         0.00002      -0.00142      1071.2\n",
      "15795       0.00039      -0.00044      1488.1\n",
      "\n",
      "[100651 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Temperatur\n0            0.00207      -0.00062      1282.2\n1            0.00166      -0.00065      1347.6\n2            0.00227       0.00098      1094.6\n3            0.00116      -0.00123      1175.7\n4            0.00123      -0.00005      1459.2\n...              ...           ...         ...\n100646       0.00015       0.00050      1439.1\n100647       0.00136       0.00150       876.7\n100648       0.00191       0.00029      1335.8\n100649       0.00002      -0.00142      1071.2\n100650       0.00039      -0.00044      1488.1\n\n[100651 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00207</td>\n      <td>-0.00062</td>\n      <td>1282.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00166</td>\n      <td>-0.00065</td>\n      <td>1347.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00227</td>\n      <td>0.00098</td>\n      <td>1094.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00116</td>\n      <td>-0.00123</td>\n      <td>1175.7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00123</td>\n      <td>-0.00005</td>\n      <td>1459.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>100646</th>\n      <td>0.00015</td>\n      <td>0.00050</td>\n      <td>1439.1</td>\n    </tr>\n    <tr>\n      <th>100647</th>\n      <td>0.00136</td>\n      <td>0.00150</td>\n      <td>876.7</td>\n    </tr>\n    <tr>\n      <th>100648</th>\n      <td>0.00191</td>\n      <td>0.00029</td>\n      <td>1335.8</td>\n    </tr>\n    <tr>\n      <th>100649</th>\n      <td>0.00002</td>\n      <td>-0.00142</td>\n      <td>1071.2</td>\n    </tr>\n    <tr>\n      <th>100650</th>\n      <td>0.00039</td>\n      <td>-0.00044</td>\n      <td>1488.1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100651 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "\n",
    "print(df1)\n",
    "df_reset = df1.reset_index(drop=True)\n",
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e72a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.729205400Z",
     "start_time": "2024-02-29T22:53:38.665449400Z"
    }
   },
   "outputs": [],
   "source": [
    "label = df_reset[\"Temperatur\"]\n",
    "# Korrektur: Verwenden Sie den Spaltennamen direkt, ohne Indexierung der columns-Eigenschaft\n",
    "df1 = df_reset.drop(\"Temperatur\", axis=1)\n",
    "X = df1\n",
    "y = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7fa289a50d87423"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e694a236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.776205200Z",
     "start_time": "2024-02-29T22:53:38.671035900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate\ncount  100651.000000  1.006510e+05\nmean        0.001250  1.103042e-20\nstd         0.000725  1.157589e-03\nmin         0.000000 -2.000000e-03\n25%         0.000620 -1.000000e-03\n50%         0.001250  4.529900e-18\n75%         0.001880  1.000000e-03\nmax         0.002500  2.000000e-03",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100651.000000</td>\n      <td>1.006510e+05</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.001250</td>\n      <td>1.103042e-20</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.000725</td>\n      <td>1.157589e-03</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-2.000000e-03</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000620</td>\n      <td>-1.000000e-03</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.001250</td>\n      <td>4.529900e-18</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.001880</td>\n      <td>1.000000e-03</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.002500</td>\n      <td>2.000000e-03</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3303b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.776205200Z",
     "start_time": "2024-02-29T22:53:38.689270Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "count    100651.000000\nmean       1144.030064\nstd         264.135723\nmin         572.200000\n25%         937.330000\n50%        1201.100000\n75%        1368.700000\nmax        1520.000000\nName: Temperatur, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ad8da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.776205200Z",
     "start_time": "2024-02-29T22:53:38.700164300Z"
    }
   },
   "outputs": [],
   "source": [
    " # train_df enthält 80% der Daten, test_df enthält 20% der Daten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c705edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.793205500Z",
     "start_time": "2024-02-29T22:53:38.708121400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_features2 = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "X_test_scaled = scaler_features.transform(X_test)  # Nutze unterschiedliche Skalierungsparameter\n",
    "\n",
    "# Initialisiere einen SEPARATEN MinMaxScaler für das Ziel, wenn nötig\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Skaliere y_train und y_test. Beachte, dass y_train.reshape(-1, 1) verwendet wird, da MinMaxScaler \n",
    "# erwartet, dass die Eingaben als 2D-Arrays kommen, und Ziele normalerweise als 1D-Arrays vorliegen.\n",
    "y_train_scaled = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbefe631e495b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-29T22:53:38.794204700Z",
     "start_time": "2024-02-29T22:53:38.717743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.416 , 0.5725],\n       [0.812 , 0.7325],\n       [0.628 , 0.5075],\n       ...,\n       [0.604 , 0.3875],\n       [0.748 , 0.65  ],\n       [0.028 , 0.6225]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "81/81 [==============================] - 2s 13ms/step - loss: 0.5078 - mae: 0.2564 - val_loss: 0.2939 - val_mae: 0.0674\n",
      "Epoch 2/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.2691 - mae: 0.0405 - val_loss: 0.2499 - val_mae: 0.0128\n",
      "Epoch 3/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.2422 - mae: 0.0214 - val_loss: 0.2345 - val_mae: 0.0191\n",
      "Epoch 4/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.2290 - mae: 0.0147 - val_loss: 0.2237 - val_mae: 0.0078\n",
      "Epoch 5/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.2200 - mae: 0.0175 - val_loss: 0.2153 - val_mae: 0.0108\n",
      "Epoch 6/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.2119 - mae: 0.0144 - val_loss: 0.2080 - val_mae: 0.0147\n",
      "Epoch 7/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.2046 - mae: 0.0136 - val_loss: 0.2007 - val_mae: 0.0045\n",
      "Epoch 8/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1979 - mae: 0.0135 - val_loss: 0.1946 - val_mae: 0.0136\n",
      "Epoch 9/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1915 - mae: 0.0099 - val_loss: 0.1903 - val_mae: 0.0367\n",
      "Epoch 10/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1857 - mae: 0.0117 - val_loss: 0.1826 - val_mae: 0.0083\n",
      "Epoch 11/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1801 - mae: 0.0129 - val_loss: 0.1773 - val_mae: 0.0141\n",
      "Epoch 12/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1746 - mae: 0.0089 - val_loss: 0.1718 - val_mae: 0.0032\n",
      "Epoch 13/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1696 - mae: 0.0124 - val_loss: 0.1671 - val_mae: 0.0146\n",
      "Epoch 14/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1646 - mae: 0.0085 - val_loss: 0.1620 - val_mae: 0.0053\n",
      "Epoch 15/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1600 - mae: 0.0125 - val_loss: 0.1574 - val_mae: 0.0052\n",
      "Epoch 16/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1553 - mae: 0.0073 - val_loss: 0.1530 - val_mae: 0.0060\n",
      "Epoch 17/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1511 - mae: 0.0121 - val_loss: 0.1491 - val_mae: 0.0178\n",
      "Epoch 18/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1467 - mae: 0.0080 - val_loss: 0.1446 - val_mae: 0.0130\n",
      "Epoch 19/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1426 - mae: 0.0113 - val_loss: 0.1404 - val_mae: 0.0047\n",
      "Epoch 20/400\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.1385 - mae: 0.0075 - val_loss: 0.1365 - val_mae: 0.0102\n",
      "Epoch 21/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1347 - mae: 0.0100 - val_loss: 0.1331 - val_mae: 0.0207\n",
      "Epoch 22/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1308 - mae: 0.0078 - val_loss: 0.1289 - val_mae: 0.0100\n",
      "Epoch 23/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1272 - mae: 0.0114 - val_loss: 0.1251 - val_mae: 0.0045\n",
      "Epoch 24/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1235 - mae: 0.0089 - val_loss: 0.1221 - val_mae: 0.0203\n",
      "Epoch 25/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1200 - mae: 0.0094 - val_loss: 0.1189 - val_mae: 0.0224\n",
      "Epoch 26/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1166 - mae: 0.0097 - val_loss: 0.1147 - val_mae: 0.0057\n",
      "Epoch 27/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1131 - mae: 0.0085 - val_loss: 0.1113 - val_mae: 0.0042\n",
      "Epoch 28/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1098 - mae: 0.0075 - val_loss: 0.1080 - val_mae: 0.0047\n",
      "Epoch 29/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1066 - mae: 0.0086 - val_loss: 0.1048 - val_mae: 0.0033\n",
      "Epoch 30/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1034 - mae: 0.0087 - val_loss: 0.1021 - val_mae: 0.0160\n",
      "Epoch 31/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.1003 - mae: 0.0091 - val_loss: 0.0986 - val_mae: 0.0052\n",
      "Epoch 32/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0973 - mae: 0.0106 - val_loss: 0.0956 - val_mae: 0.0052\n",
      "Epoch 33/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0942 - mae: 0.0079 - val_loss: 0.0926 - val_mae: 0.0040\n",
      "Epoch 34/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0913 - mae: 0.0075 - val_loss: 0.0897 - val_mae: 0.0046\n",
      "Epoch 35/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0885 - mae: 0.0103 - val_loss: 0.0877 - val_mae: 0.0216\n",
      "Epoch 36/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0856 - mae: 0.0080 - val_loss: 0.0843 - val_mae: 0.0128\n",
      "Epoch 37/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0829 - mae: 0.0092 - val_loss: 0.0814 - val_mae: 0.0042\n",
      "Epoch 38/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0802 - mae: 0.0081 - val_loss: 0.0788 - val_mae: 0.0107\n",
      "Epoch 39/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0776 - mae: 0.0099 - val_loss: 0.0762 - val_mae: 0.0081\n",
      "Epoch 40/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0749 - mae: 0.0066 - val_loss: 0.0736 - val_mae: 0.0078\n",
      "Epoch 41/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0725 - mae: 0.0093 - val_loss: 0.0710 - val_mae: 0.0034\n",
      "Epoch 42/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0701 - mae: 0.0113 - val_loss: 0.0686 - val_mae: 0.0059\n",
      "Epoch 43/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0674 - mae: 0.0069 - val_loss: 0.0661 - val_mae: 0.0033\n",
      "Epoch 44/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0651 - mae: 0.0084 - val_loss: 0.0638 - val_mae: 0.0052\n",
      "Epoch 45/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0629 - mae: 0.0107 - val_loss: 0.0615 - val_mae: 0.0041\n",
      "Epoch 46/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0605 - mae: 0.0089 - val_loss: 0.0593 - val_mae: 0.0050\n",
      "Epoch 47/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0583 - mae: 0.0096 - val_loss: 0.0575 - val_mae: 0.0152\n",
      "Epoch 48/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0562 - mae: 0.0103 - val_loss: 0.0554 - val_mae: 0.0173\n",
      "Epoch 49/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0542 - mae: 0.0111 - val_loss: 0.0530 - val_mae: 0.0085\n",
      "Epoch 50/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0520 - mae: 0.0082 - val_loss: 0.0509 - val_mae: 0.0070\n",
      "Epoch 51/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0500 - mae: 0.0085 - val_loss: 0.0490 - val_mae: 0.0061\n",
      "Epoch 52/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0482 - mae: 0.0103 - val_loss: 0.0471 - val_mae: 0.0087\n",
      "Epoch 53/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0462 - mae: 0.0092 - val_loss: 0.0457 - val_mae: 0.0208\n",
      "Epoch 54/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0444 - mae: 0.0092 - val_loss: 0.0434 - val_mae: 0.0075\n",
      "Epoch 55/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0426 - mae: 0.0099 - val_loss: 0.0416 - val_mae: 0.0063\n",
      "Epoch 56/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0409 - mae: 0.0081 - val_loss: 0.0399 - val_mae: 0.0056\n",
      "Epoch 57/400\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0393 - mae: 0.0104 - val_loss: 0.0383 - val_mae: 0.0038\n",
      "Epoch 58/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0376 - mae: 0.0077 - val_loss: 0.0367 - val_mae: 0.0034\n",
      "Epoch 59/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0362 - mae: 0.0122 - val_loss: 0.0351 - val_mae: 0.0040\n",
      "Epoch 60/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0345 - mae: 0.0081 - val_loss: 0.0338 - val_mae: 0.0138\n",
      "Epoch 61/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0331 - mae: 0.0098 - val_loss: 0.0322 - val_mae: 0.0053\n",
      "Epoch 62/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0317 - mae: 0.0110 - val_loss: 0.0316 - val_mae: 0.0215\n",
      "Epoch 63/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0303 - mae: 0.0102 - val_loss: 0.0296 - val_mae: 0.0095\n",
      "Epoch 64/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0289 - mae: 0.0071 - val_loss: 0.0282 - val_mae: 0.0033\n",
      "Epoch 65/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0277 - mae: 0.0107 - val_loss: 0.0272 - val_mae: 0.0152\n",
      "Epoch 66/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0265 - mae: 0.0085 - val_loss: 0.0258 - val_mae: 0.0083\n",
      "Epoch 67/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0253 - mae: 0.0099 - val_loss: 0.0246 - val_mae: 0.0044\n",
      "Epoch 68/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0241 - mae: 0.0081 - val_loss: 0.0238 - val_mae: 0.0135\n",
      "Epoch 69/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0230 - mae: 0.0066 - val_loss: 0.0224 - val_mae: 0.0052\n",
      "Epoch 70/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0221 - mae: 0.0114 - val_loss: 0.0214 - val_mae: 0.0090\n",
      "Epoch 71/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0210 - mae: 0.0092 - val_loss: 0.0205 - val_mae: 0.0091\n",
      "Epoch 72/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0200 - mae: 0.0088 - val_loss: 0.0195 - val_mae: 0.0064\n",
      "Epoch 73/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0191 - mae: 0.0082 - val_loss: 0.0187 - val_mae: 0.0104\n",
      "Epoch 74/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0183 - mae: 0.0109 - val_loss: 0.0177 - val_mae: 0.0075\n",
      "Epoch 75/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0174 - mae: 0.0093 - val_loss: 0.0168 - val_mae: 0.0047\n",
      "Epoch 76/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0166 - mae: 0.0089 - val_loss: 0.0161 - val_mae: 0.0081\n",
      "Epoch 77/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0158 - mae: 0.0111 - val_loss: 0.0159 - val_mae: 0.0242\n",
      "Epoch 78/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0150 - mae: 0.0081 - val_loss: 0.0146 - val_mae: 0.0038\n",
      "Epoch 79/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0151 - mae: 0.0181 - val_loss: 0.0140 - val_mae: 0.0076\n",
      "Epoch 80/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0136 - mae: 0.0046 - val_loss: 0.0133 - val_mae: 0.0047\n",
      "Epoch 81/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0130 - mae: 0.0062 - val_loss: 0.0127 - val_mae: 0.0082\n",
      "Epoch 82/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0125 - mae: 0.0105 - val_loss: 0.0131 - val_mae: 0.0307\n",
      "Epoch 83/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0119 - mae: 0.0074 - val_loss: 0.0115 - val_mae: 0.0048\n",
      "Epoch 84/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0114 - mae: 0.0106 - val_loss: 0.0109 - val_mae: 0.0046\n",
      "Epoch 85/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0108 - mae: 0.0084 - val_loss: 0.0106 - val_mae: 0.0115\n",
      "Epoch 86/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0103 - mae: 0.0086 - val_loss: 0.0099 - val_mae: 0.0063\n",
      "Epoch 87/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0100 - mae: 0.0117 - val_loss: 0.0095 - val_mae: 0.0034\n",
      "Epoch 88/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0093 - mae: 0.0051 - val_loss: 0.0094 - val_mae: 0.0151\n",
      "Epoch 89/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0089 - mae: 0.0084 - val_loss: 0.0087 - val_mae: 0.0117\n",
      "Epoch 90/400\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0086 - mae: 0.0103 - val_loss: 0.0082 - val_mae: 0.0037\n",
      "Epoch 91/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0082 - mae: 0.0103 - val_loss: 0.0082 - val_mae: 0.0154\n",
      "Epoch 92/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0077 - mae: 0.0063 - val_loss: 0.0075 - val_mae: 0.0045\n",
      "Epoch 93/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0074 - mae: 0.0094 - val_loss: 0.0071 - val_mae: 0.0048\n",
      "Epoch 94/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0071 - mae: 0.0104 - val_loss: 0.0068 - val_mae: 0.0033\n",
      "Epoch 95/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0067 - mae: 0.0085 - val_loss: 0.0066 - val_mae: 0.0083\n",
      "Epoch 96/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0065 - mae: 0.0100 - val_loss: 0.0063 - val_mae: 0.0080\n",
      "Epoch 97/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0062 - mae: 0.0091 - val_loss: 0.0059 - val_mae: 0.0035\n",
      "Epoch 98/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0061 - mae: 0.0118 - val_loss: 0.0057 - val_mae: 0.0044\n",
      "Epoch 99/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0056 - mae: 0.0052 - val_loss: 0.0055 - val_mae: 0.0086\n",
      "Epoch 100/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0055 - mae: 0.0083 - val_loss: 0.0052 - val_mae: 0.0050\n",
      "Epoch 101/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0053 - mae: 0.0095 - val_loss: 0.0050 - val_mae: 0.0042\n",
      "Epoch 102/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0050 - mae: 0.0082 - val_loss: 0.0048 - val_mae: 0.0052\n",
      "Epoch 103/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0051 - mae: 0.0142 - val_loss: 0.0047 - val_mae: 0.0087\n",
      "Epoch 104/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0045 - mae: 0.0044 - val_loss: 0.0044 - val_mae: 0.0042\n",
      "Epoch 105/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0044 - mae: 0.0066 - val_loss: 0.0043 - val_mae: 0.0067\n",
      "Epoch 106/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0043 - mae: 0.0089 - val_loss: 0.0041 - val_mae: 0.0048\n",
      "Epoch 107/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0041 - mae: 0.0073 - val_loss: 0.0039 - val_mae: 0.0034\n",
      "Epoch 108/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0041 - mae: 0.0102 - val_loss: 0.0038 - val_mae: 0.0035\n",
      "Epoch 109/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0039 - mae: 0.0090 - val_loss: 0.0037 - val_mae: 0.0065\n",
      "Epoch 110/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0037 - mae: 0.0076 - val_loss: 0.0036 - val_mae: 0.0050\n",
      "Epoch 111/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0036 - mae: 0.0095 - val_loss: 0.0036 - val_mae: 0.0113\n",
      "Epoch 112/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0035 - mae: 0.0099 - val_loss: 0.0037 - val_mae: 0.0180\n",
      "Epoch 113/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0034 - mae: 0.0083 - val_loss: 0.0032 - val_mae: 0.0057\n",
      "Epoch 114/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0033 - mae: 0.0096 - val_loss: 0.0035 - val_mae: 0.0176\n",
      "Epoch 115/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0032 - mae: 0.0086 - val_loss: 0.0031 - val_mae: 0.0051\n",
      "Epoch 116/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0032 - mae: 0.0108 - val_loss: 0.0030 - val_mae: 0.0056\n",
      "Epoch 117/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0030 - mae: 0.0069 - val_loss: 0.0029 - val_mae: 0.0064\n",
      "Epoch 118/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0029 - mae: 0.0085 - val_loss: 0.0028 - val_mae: 0.0050\n",
      "Epoch 119/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0029 - mae: 0.0104 - val_loss: 0.0028 - val_mae: 0.0068\n",
      "Epoch 120/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0028 - mae: 0.0079 - val_loss: 0.0031 - val_mae: 0.0186\n",
      "Epoch 121/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0028 - mae: 0.0093 - val_loss: 0.0026 - val_mae: 0.0034\n",
      "Epoch 122/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0028 - mae: 0.0103 - val_loss: 0.0026 - val_mae: 0.0078\n",
      "Epoch 123/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0026 - mae: 0.0065 - val_loss: 0.0026 - val_mae: 0.0086\n",
      "Epoch 124/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0026 - mae: 0.0078 - val_loss: 0.0031 - val_mae: 0.0242\n",
      "Epoch 125/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0026 - mae: 0.0092 - val_loss: 0.0025 - val_mae: 0.0078\n",
      "Epoch 126/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0026 - mae: 0.0100 - val_loss: 0.0024 - val_mae: 0.0035\n",
      "Epoch 127/400\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0024 - mae: 0.0076 - val_loss: 0.0024 - val_mae: 0.0081\n",
      "Epoch 128/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0024 - mae: 0.0090 - val_loss: 0.0026 - val_mae: 0.0166\n",
      "Epoch 129/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0024 - mae: 0.0094 - val_loss: 0.0023 - val_mae: 0.0062\n",
      "Epoch 130/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0023 - mae: 0.0082 - val_loss: 0.0029 - val_mae: 0.0245\n",
      "Epoch 131/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0024 - mae: 0.0096 - val_loss: 0.0022 - val_mae: 0.0048\n",
      "Epoch 132/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0023 - mae: 0.0104 - val_loss: 0.0022 - val_mae: 0.0082\n",
      "Epoch 133/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0023 - mae: 0.0099 - val_loss: 0.0021 - val_mae: 0.0035\n",
      "Epoch 134/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0022 - mae: 0.0058 - val_loss: 0.0023 - val_mae: 0.0125\n",
      "Epoch 135/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0022 - mae: 0.0086 - val_loss: 0.0021 - val_mae: 0.0034\n",
      "Epoch 136/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0023 - mae: 0.0105 - val_loss: 0.0021 - val_mae: 0.0048\n",
      "Epoch 137/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0022 - mae: 0.0081 - val_loss: 0.0021 - val_mae: 0.0092\n",
      "Epoch 138/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0021 - mae: 0.0066 - val_loss: 0.0025 - val_mae: 0.0204\n",
      "Epoch 139/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0021 - mae: 0.0089 - val_loss: 0.0020 - val_mae: 0.0042\n",
      "Epoch 140/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0021 - mae: 0.0089 - val_loss: 0.0020 - val_mae: 0.0073\n",
      "Epoch 141/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0021 - mae: 0.0082 - val_loss: 0.0021 - val_mae: 0.0127\n",
      "Epoch 142/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0021 - mae: 0.0092 - val_loss: 0.0020 - val_mae: 0.0044\n",
      "Epoch 143/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0084 - val_loss: 0.0019 - val_mae: 0.0046\n",
      "Epoch 144/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0021 - mae: 0.0094 - val_loss: 0.0021 - val_mae: 0.0127\n",
      "Epoch 145/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0085 - val_loss: 0.0019 - val_mae: 0.0061\n",
      "Epoch 146/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0021 - mae: 0.0099 - val_loss: 0.0020 - val_mae: 0.0075\n",
      "Epoch 147/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0073 - val_loss: 0.0019 - val_mae: 0.0040\n",
      "Epoch 148/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0088 - val_loss: 0.0019 - val_mae: 0.0069\n",
      "Epoch 149/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0092 - val_loss: 0.0019 - val_mae: 0.0044\n",
      "Epoch 150/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0082 - val_loss: 0.0019 - val_mae: 0.0076\n",
      "Epoch 151/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0019 - mae: 0.0073 - val_loss: 0.0018 - val_mae: 0.0043\n",
      "Epoch 152/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0099 - val_loss: 0.0021 - val_mae: 0.0159\n",
      "Epoch 153/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0095 - val_loss: 0.0027 - val_mae: 0.0213\n",
      "Epoch 154/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0019 - mae: 0.0079 - val_loss: 0.0018 - val_mae: 0.0049\n",
      "Epoch 155/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0061 - val_loss: 0.0018 - val_mae: 0.0055\n",
      "Epoch 156/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0019 - mae: 0.0088 - val_loss: 0.0018 - val_mae: 0.0034\n",
      "Epoch 157/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0019 - mae: 0.0095 - val_loss: 0.0018 - val_mae: 0.0058\n",
      "Epoch 158/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0058 - val_loss: 0.0020 - val_mae: 0.0147\n",
      "Epoch 159/400\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0019 - mae: 0.0088 - val_loss: 0.0018 - val_mae: 0.0062\n",
      "Epoch 160/400\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0019 - mae: 0.0100 - val_loss: 0.0024 - val_mae: 0.0257\n",
      "Epoch 161/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0073 - val_loss: 0.0018 - val_mae: 0.0044\n",
      "Epoch 162/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0019 - mae: 0.0095 - val_loss: 0.0017 - val_mae: 0.0037\n",
      "Epoch 163/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0019 - mae: 0.0092 - val_loss: 0.0018 - val_mae: 0.0046\n",
      "Epoch 164/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0059 - val_loss: 0.0019 - val_mae: 0.0139\n",
      "Epoch 165/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0019 - mae: 0.0113 - val_loss: 0.0025 - val_mae: 0.0249\n",
      "Epoch 166/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0059 - val_loss: 0.0017 - val_mae: 0.0048\n",
      "Epoch 167/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0068 - val_loss: 0.0018 - val_mae: 0.0084\n",
      "Epoch 168/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0092 - val_loss: 0.0017 - val_mae: 0.0044\n",
      "Epoch 169/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0092 - val_loss: 0.0017 - val_mae: 0.0035\n",
      "Epoch 170/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0019 - mae: 0.0092 - val_loss: 0.0017 - val_mae: 0.0050\n",
      "Epoch 171/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0049\n",
      "Epoch 172/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0087 - val_loss: 0.0018 - val_mae: 0.0079\n",
      "Epoch 173/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0070 - val_loss: 0.0021 - val_mae: 0.0188\n",
      "Epoch 174/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0020 - mae: 0.0125 - val_loss: 0.0017 - val_mae: 0.0038\n",
      "Epoch 175/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0044 - val_loss: 0.0017 - val_mae: 0.0058\n",
      "Epoch 176/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0066 - val_loss: 0.0017 - val_mae: 0.0047\n",
      "Epoch 177/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0078 - val_loss: 0.0017 - val_mae: 0.0054\n",
      "Epoch 178/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0073 - val_loss: 0.0018 - val_mae: 0.0097\n",
      "Epoch 179/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0099 - val_loss: 0.0019 - val_mae: 0.0111\n",
      "Epoch 180/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0090 - val_loss: 0.0017 - val_mae: 0.0035\n",
      "Epoch 181/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0057 - val_loss: 0.0024 - val_mae: 0.0232\n",
      "Epoch 182/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0078 - val_loss: 0.0016 - val_mae: 0.0039\n",
      "Epoch 183/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0083 - val_loss: 0.0017 - val_mae: 0.0084\n",
      "Epoch 184/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0082 - val_loss: 0.0016 - val_mae: 0.0037\n",
      "Epoch 185/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0075 - val_loss: 0.0017 - val_mae: 0.0078\n",
      "Epoch 186/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0079 - val_loss: 0.0016 - val_mae: 0.0034\n",
      "Epoch 187/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0080 - val_loss: 0.0017 - val_mae: 0.0073\n",
      "Epoch 188/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0091 - val_loss: 0.0017 - val_mae: 0.0088\n",
      "Epoch 189/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0065 - val_loss: 0.0016 - val_mae: 0.0058\n",
      "Epoch 190/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0075 - val_loss: 0.0019 - val_mae: 0.0154\n",
      "Epoch 191/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0090 - val_loss: 0.0016 - val_mae: 0.0043\n",
      "Epoch 192/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0086 - val_loss: 0.0022 - val_mae: 0.0198\n",
      "Epoch 193/400\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0016 - mae: 0.0057 - val_loss: 0.0016 - val_mae: 0.0050\n",
      "Epoch 194/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0074 - val_loss: 0.0022 - val_mae: 0.0238\n",
      "Epoch 195/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0086 - val_loss: 0.0016 - val_mae: 0.0036\n",
      "Epoch 196/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0052 - val_loss: 0.0020 - val_mae: 0.0201\n",
      "Epoch 197/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0090 - val_loss: 0.0017 - val_mae: 0.0080\n",
      "Epoch 198/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0061 - val_loss: 0.0016 - val_mae: 0.0044\n",
      "Epoch 199/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0096 - val_loss: 0.0017 - val_mae: 0.0116\n",
      "Epoch 200/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0065 - val_loss: 0.0016 - val_mae: 0.0047\n",
      "Epoch 201/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0071 - val_loss: 0.0019 - val_mae: 0.0170\n",
      "Epoch 202/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0079 - val_loss: 0.0018 - val_mae: 0.0144\n",
      "Epoch 203/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0069 - val_loss: 0.0019 - val_mae: 0.0182\n",
      "Epoch 204/400\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0017 - mae: 0.0083 - val_loss: 0.0016 - val_mae: 0.0046\n",
      "Epoch 205/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0086 - val_loss: 0.0016 - val_mae: 0.0034\n",
      "Epoch 206/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0061 - val_loss: 0.0016 - val_mae: 0.0049\n",
      "Epoch 207/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0091 - val_loss: 0.0016 - val_mae: 0.0037\n",
      "Epoch 208/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0082 - val_loss: 0.0018 - val_mae: 0.0136\n",
      "Epoch 209/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0083 - val_loss: 0.0017 - val_mae: 0.0086\n",
      "Epoch 210/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0066 - val_loss: 0.0017 - val_mae: 0.0114\n",
      "Epoch 211/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0019 - mae: 0.0122 - val_loss: 0.0016 - val_mae: 0.0060\n",
      "Epoch 212/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0037\n",
      "Epoch 213/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0039\n",
      "Epoch 214/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0092 - val_loss: 0.0016 - val_mae: 0.0069\n",
      "Epoch 215/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0049 - val_loss: 0.0015 - val_mae: 0.0039\n",
      "Epoch 216/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0078 - val_loss: 0.0016 - val_mae: 0.0047\n",
      "Epoch 217/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0079 - val_loss: 0.0016 - val_mae: 0.0061\n",
      "Epoch 218/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0071 - val_loss: 0.0016 - val_mae: 0.0056\n",
      "Epoch 219/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0085 - val_loss: 0.0019 - val_mae: 0.0139\n",
      "Epoch 220/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0086 - val_loss: 0.0015 - val_mae: 0.0039\n",
      "Epoch 221/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0065 - val_loss: 0.0016 - val_mae: 0.0073\n",
      "Epoch 222/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0074 - val_loss: 0.0016 - val_mae: 0.0099\n",
      "Epoch 223/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0090 - val_loss: 0.0015 - val_mae: 0.0053\n",
      "Epoch 224/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0015 - mae: 0.0056 - val_loss: 0.0016 - val_mae: 0.0080\n",
      "Epoch 225/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0017 - mae: 0.0108 - val_loss: 0.0015 - val_mae: 0.0034\n",
      "Epoch 226/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0015 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0073\n",
      "Epoch 227/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0082 - val_loss: 0.0017 - val_mae: 0.0122\n",
      "Epoch 228/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0090 - val_loss: 0.0016 - val_mae: 0.0104\n",
      "Epoch 229/400\n",
      "81/81 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0079 - val_loss: 0.0015 - val_mae: 0.0050\n",
      "Epoch 230/400\n",
      "76/81 [===========================>..] - ETA: 0s - loss: 0.0016 - mae: 0.0098Restoring model weights from the end of the best epoch: 225.\n",
      "81/81 [==============================] - 1s 12ms/step - loss: 0.0016 - mae: 0.0099 - val_loss: 0.0020 - val_mae: 0.0222\n",
      "Epoch 230: early stopping\n",
      "Die Ausführungszeit betrug 203.96538853645325 Sekunden.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "    # Eingabeschicht\n",
    "\n",
    "    Dense(320, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(178, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(288, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(192, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(208, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(224, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(80, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(304, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(240, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(48, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=2, mode='min', restore_best_weights=True)#, min_delta = 0.00005)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 800\n",
    "epochs = 400\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose = 1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Berechne die Dauer\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"Die Ausführungszeit betrug {duration} Sekunden.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T11:57:25.623079400Z",
     "start_time": "2024-03-01T11:54:01.645707Z"
    }
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630/630 - 1s - loss: 0.0015 - mae: 0.0034 - 647ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.0015176483429968357, 0.0034180309157818556]"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test_scaled, y_test_scaled, verbose=2)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T11:58:22.127102900Z",
     "start_time": "2024-03-01T11:58:21.437418900Z"
    }
   },
   "id": "68d86893ad985b02"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f6f672a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T11:59:06.617458700Z",
     "start_time": "2024-03-01T11:59:05.699568300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bsp. Predicted: [1193.0062] Actual: [1195.9] \n",
      "Durchschnittliche Abweichung (MAE): [3.23960823]\n"
     ]
    }
   ],
   "source": [
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Führen Sie die Rücktransformation der skalierten Werte durch\n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[1000]} Actual: {original_actual_values[1000]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "\n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "\n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Werte die kleiner sind: 72\n"
     ]
    },
    {
     "data": {
      "text/plain": "             Echt  Vorhergesagt  X-Koordinate  Y-Koordinate  Differenz\n19281  782.048828        807.84         1.000        0.9125 -25.791172\n14143  808.242554        832.75         1.000        0.9025 -24.507446\n16514  814.791077        838.63         1.000        0.9000 -23.838923\n4160   781.866699        805.35         0.992        0.9125 -23.483301\n4177   821.317810        844.45         1.000        0.8975 -23.132190\n...           ...           ...           ...           ...        ...\n13809  618.043274        593.15         0.988        0.9750  24.893274\n11547  690.403198        665.34         0.916        0.0000  25.063198\n7694   693.200806        667.48         0.944        0.0000  25.720806\n1409   692.383972        666.57         0.936        0.0000  25.813972\n20012  618.225891        588.92         0.996        0.9750  29.305891\n\n[20131 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Echt</th>\n      <th>Vorhergesagt</th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Differenz</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19281</th>\n      <td>782.048828</td>\n      <td>807.84</td>\n      <td>1.000</td>\n      <td>0.9125</td>\n      <td>-25.791172</td>\n    </tr>\n    <tr>\n      <th>14143</th>\n      <td>808.242554</td>\n      <td>832.75</td>\n      <td>1.000</td>\n      <td>0.9025</td>\n      <td>-24.507446</td>\n    </tr>\n    <tr>\n      <th>16514</th>\n      <td>814.791077</td>\n      <td>838.63</td>\n      <td>1.000</td>\n      <td>0.9000</td>\n      <td>-23.838923</td>\n    </tr>\n    <tr>\n      <th>4160</th>\n      <td>781.866699</td>\n      <td>805.35</td>\n      <td>0.992</td>\n      <td>0.9125</td>\n      <td>-23.483301</td>\n    </tr>\n    <tr>\n      <th>4177</th>\n      <td>821.317810</td>\n      <td>844.45</td>\n      <td>1.000</td>\n      <td>0.8975</td>\n      <td>-23.132190</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13809</th>\n      <td>618.043274</td>\n      <td>593.15</td>\n      <td>0.988</td>\n      <td>0.9750</td>\n      <td>24.893274</td>\n    </tr>\n    <tr>\n      <th>11547</th>\n      <td>690.403198</td>\n      <td>665.34</td>\n      <td>0.916</td>\n      <td>0.0000</td>\n      <td>25.063198</td>\n    </tr>\n    <tr>\n      <th>7694</th>\n      <td>693.200806</td>\n      <td>667.48</td>\n      <td>0.944</td>\n      <td>0.0000</td>\n      <td>25.720806</td>\n    </tr>\n    <tr>\n      <th>1409</th>\n      <td>692.383972</td>\n      <td>666.57</td>\n      <td>0.936</td>\n      <td>0.0000</td>\n      <td>25.813972</td>\n    </tr>\n    <tr>\n      <th>20012</th>\n      <td>618.225891</td>\n      <td>588.92</td>\n      <td>0.996</td>\n      <td>0.9750</td>\n      <td>29.305891</td>\n    </tr>\n  </tbody>\n</table>\n<p>20131 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'Echt': [val[0] for val in list1], 'Vorhergesagt': [val[0] for val in list2]})\n",
    "df_result['X-Koordinate'] = X_test_scaled[:, 0]\n",
    "df_result['Y-Koordinate'] = X_test_scaled[:, 1]\n",
    "\n",
    "df_result['Differenz'] = df_result['Echt'] - df_result['Vorhergesagt']\n",
    "df_result['Differenz'].sort_values()\n",
    "sorted_df = df_result.sort_values(by= 'Differenz')\n",
    "Anzahl_Punkte = (sorted_df['Differenz'] > 20).sum()\n",
    "print(\"Anzahl der Werte die kleiner sind:\", Anzahl_Punkte)\n",
    "\n",
    "sorted_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T11:21:35.606813400Z",
     "start_time": "2024-03-01T11:21:35.567830200Z"
    }
   },
   "id": "42bde60d3b4f2007"
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae = history.history['loss']\n",
    "val_mae = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(mae) + 1)\n",
    "# MAE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mae, 'r', label='Training MSE')\n",
    "plt.plot(epochs, val_mae, 'b', label='Validation MSE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-01T11:21:40.888881300Z",
     "start_time": "2024-03-01T11:21:40.736569800Z"
    }
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# GridSearch\n",
    "Grid Search bietet die Vorteile der anpassbaren Rechenzeit sowie die Möglichkeit des EInsatzes von Verteilungen. So können theoretisch Hyperparamterkonfuigurationen gefunden wende, welche durch GridSearch nicht auffindbar wären. ZUdem ist das Ziel der Hyperparamteroptimierung eine Einstellung zu finden, welche auf Trainings und Testset gut angepasst ist. Die EInstellung muss nicht die bestmöglichste Einstellung sein, sondern eine Einstellung die das gewähltre Problem gut wiederspiegelt. \n",
    "Bayesian Optimierung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c177960cc729052"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f17e2cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-01T10:18:12.811682Z",
     "start_time": "2024-03-01T08:04:08.961509400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Epoch 1/50\n",
      "101/101 - 2s - loss: 0.4234 - mae: 0.2246 - 2s/epoch - 24ms/step\n",
      "Epoch 2/50\n",
      "101/101 - 1s - loss: 0.2489 - mae: 0.0298 - 898ms/epoch - 9ms/step\n",
      "Epoch 3/50\n",
      "101/101 - 1s - loss: 0.2246 - mae: 0.0213 - 913ms/epoch - 9ms/step\n",
      "Epoch 4/50\n",
      "101/101 - 1s - loss: 0.2096 - mae: 0.0177 - 901ms/epoch - 9ms/step\n",
      "Epoch 5/50\n",
      "101/101 - 1s - loss: 0.1979 - mae: 0.0148 - 917ms/epoch - 9ms/step\n",
      "Epoch 6/50\n",
      "101/101 - 1s - loss: 0.1875 - mae: 0.0108 - 914ms/epoch - 9ms/step\n",
      "Epoch 7/50\n",
      "101/101 - 1s - loss: 0.1785 - mae: 0.0125 - 905ms/epoch - 9ms/step\n",
      "Epoch 8/50\n",
      "101/101 - 1s - loss: 0.1700 - mae: 0.0139 - 900ms/epoch - 9ms/step\n",
      "Epoch 9/50\n",
      "101/101 - 1s - loss: 0.1620 - mae: 0.0095 - 902ms/epoch - 9ms/step\n",
      "Epoch 10/50\n",
      "101/101 - 1s - loss: 0.1548 - mae: 0.0120 - 893ms/epoch - 9ms/step\n",
      "Epoch 11/50\n",
      "101/101 - 1s - loss: 0.1480 - mae: 0.0124 - 892ms/epoch - 9ms/step\n",
      "Epoch 12/50\n",
      "101/101 - 1s - loss: 0.1414 - mae: 0.0097 - 886ms/epoch - 9ms/step\n",
      "Epoch 13/50\n",
      "101/101 - 1s - loss: 0.1353 - mae: 0.0095 - 891ms/epoch - 9ms/step\n",
      "Epoch 14/50\n",
      "101/101 - 1s - loss: 0.1295 - mae: 0.0100 - 889ms/epoch - 9ms/step\n",
      "Epoch 15/50\n",
      "101/101 - 1s - loss: 0.1240 - mae: 0.0092 - 906ms/epoch - 9ms/step\n",
      "Epoch 16/50\n",
      "101/101 - 1s - loss: 0.1187 - mae: 0.0092 - 893ms/epoch - 9ms/step\n",
      "Epoch 17/50\n",
      "101/101 - 1s - loss: 0.1139 - mae: 0.0120 - 894ms/epoch - 9ms/step\n",
      "Epoch 18/50\n",
      "101/101 - 1s - loss: 0.1089 - mae: 0.0076 - 899ms/epoch - 9ms/step\n",
      "Epoch 19/50\n",
      "101/101 - 1s - loss: 0.1042 - mae: 0.0066 - 893ms/epoch - 9ms/step\n",
      "Epoch 20/50\n",
      "101/101 - 1s - loss: 0.0999 - mae: 0.0096 - 885ms/epoch - 9ms/step\n",
      "Epoch 21/50\n",
      "101/101 - 1s - loss: 0.0955 - mae: 0.0069 - 882ms/epoch - 9ms/step\n",
      "Epoch 22/50\n",
      "101/101 - 1s - loss: 0.0915 - mae: 0.0095 - 887ms/epoch - 9ms/step\n",
      "Epoch 23/50\n",
      "101/101 - 1s - loss: 0.0875 - mae: 0.0087 - 894ms/epoch - 9ms/step\n",
      "Epoch 24/50\n",
      "101/101 - 1s - loss: 0.0836 - mae: 0.0059 - 894ms/epoch - 9ms/step\n",
      "Epoch 25/50\n",
      "101/101 - 1s - loss: 0.0800 - mae: 0.0101 - 942ms/epoch - 9ms/step\n",
      "Epoch 26/50\n",
      "101/101 - 1s - loss: 0.0763 - mae: 0.0072 - 912ms/epoch - 9ms/step\n",
      "Epoch 27/50\n",
      "101/101 - 1s - loss: 0.0730 - mae: 0.0114 - 912ms/epoch - 9ms/step\n",
      "Epoch 28/50\n",
      "101/101 - 1s - loss: 0.0695 - mae: 0.0057 - 912ms/epoch - 9ms/step\n",
      "Epoch 29/50\n",
      "101/101 - 1s - loss: 0.0664 - mae: 0.0103 - 913ms/epoch - 9ms/step\n",
      "Epoch 30/50\n",
      "101/101 - 1s - loss: 0.0631 - mae: 0.0070 - 914ms/epoch - 9ms/step\n",
      "Epoch 31/50\n",
      "101/101 - 1s - loss: 0.0601 - mae: 0.0072 - 914ms/epoch - 9ms/step\n",
      "Epoch 32/50\n",
      "101/101 - 1s - loss: 0.0574 - mae: 0.0092 - 893ms/epoch - 9ms/step\n",
      "Epoch 33/50\n",
      "101/101 - 1s - loss: 0.0545 - mae: 0.0083 - 889ms/epoch - 9ms/step\n",
      "Epoch 34/50\n",
      "101/101 - 1s - loss: 0.0516 - mae: 0.0058 - 936ms/epoch - 9ms/step\n",
      "Epoch 35/50\n",
      "101/101 - 1s - loss: 0.0491 - mae: 0.0089 - 890ms/epoch - 9ms/step\n",
      "Epoch 36/50\n",
      "101/101 - 1s - loss: 0.0467 - mae: 0.0107 - 895ms/epoch - 9ms/step\n",
      "Epoch 37/50\n",
      "101/101 - 1s - loss: 0.0441 - mae: 0.0076 - 891ms/epoch - 9ms/step\n",
      "Epoch 38/50\n",
      "101/101 - 1s - loss: 0.0419 - mae: 0.0095 - 904ms/epoch - 9ms/step\n",
      "Epoch 39/50\n",
      "101/101 - 1s - loss: 0.0396 - mae: 0.0074 - 894ms/epoch - 9ms/step\n",
      "Epoch 40/50\n",
      "101/101 - 1s - loss: 0.0375 - mae: 0.0094 - 886ms/epoch - 9ms/step\n",
      "Epoch 41/50\n",
      "101/101 - 1s - loss: 0.0354 - mae: 0.0108 - 881ms/epoch - 9ms/step\n",
      "Epoch 42/50\n",
      "101/101 - 1s - loss: 0.0334 - mae: 0.0084 - 890ms/epoch - 9ms/step\n",
      "Epoch 43/50\n",
      "101/101 - 1s - loss: 0.0315 - mae: 0.0081 - 887ms/epoch - 9ms/step\n",
      "Epoch 44/50\n",
      "101/101 - 1s - loss: 0.0298 - mae: 0.0096 - 880ms/epoch - 9ms/step\n",
      "Epoch 45/50\n",
      "101/101 - 1s - loss: 0.0281 - mae: 0.0096 - 892ms/epoch - 9ms/step\n",
      "Epoch 46/50\n",
      "101/101 - 1s - loss: 0.0266 - mae: 0.0117 - 890ms/epoch - 9ms/step\n",
      "Epoch 47/50\n",
      "101/101 - 1s - loss: 0.0248 - mae: 0.0046 - 890ms/epoch - 9ms/step\n",
      "Epoch 48/50\n",
      "101/101 - 1s - loss: 0.0234 - mae: 0.0077 - 890ms/epoch - 9ms/step\n",
      "Epoch 49/50\n",
      "101/101 - 1s - loss: 0.0220 - mae: 0.0090 - 891ms/epoch - 9ms/step\n",
      "Epoch 50/50\n",
      "101/101 - 1s - loss: 0.0207 - mae: 0.0092 - 892ms/epoch - 9ms/step\n",
      "Beste Parameter: {'fit__batch_size': 800, 'fit__epochs': 50, 'model__dropout_rate': 0.0, 'model__learning_rate': 0.001, 'model__regularization': 0.0001}\n",
      "Beste Genauigkeit: 0.9996594089931544\n"
     ]
    }
   ],
   "source": [
    "# def build_model(learning_rate=0.001, activation='relu', regularization=0.0001, dropout_rate=0.0):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(320, activation=activation, input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(176, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(288, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(192, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(208, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(224, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     \n",
    "#     model.add(Dense(80, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     \n",
    "#     model.add(Dense(304, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate)) \n",
    "#     \n",
    "#     model.add(Dense(240, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))   \n",
    "#     \n",
    "#     model.add(Dense(48, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "#     return model\n",
    "# \n",
    "# # Verwenden Sie eine Funktion, um das Modell zu instanziieren, für scikit-learn Wrapper\n",
    "# model = KerasRegressor(model=build_model, verbose=2)\n",
    "# \n",
    "# # Anpassung der Parameter im param_grid\n",
    "# param_grid = {\n",
    "#     'model__learning_rate': [0.01, 0.001, 0.0001],\n",
    "#     'model__regularization': [0.001, 0.0001],\n",
    "#     'fit__batch_size': [100, 200, 400, 800],\n",
    "#     'fit__epochs': [50],\n",
    "#     'model__dropout_rate' : [0.0, 0.1, 0.2]\n",
    "# }\n",
    "# \n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "# # Hinweis: Stellen Sie sicher, dass Ihre Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\n",
    "# grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "# # Beste Parameter und Score ausgeben\n",
    "# print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "# print(\"Beste Genauigkeit:\", grid_search.best_score_)\n",
    "# \n",
    "# with open(\"grid_search_D1_2.txt\", \"w\") as f:\n",
    "#     f.write(f\"Beste Parameter: {grid_search.best_params_}\\n\")\n",
    "#     f.write(f\"Beste Genauigkeit: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "  # Bayesian Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bb41910a42cbdee"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# from bayes_opt import BayesianOptimization\n",
    "# \n",
    "# \n",
    "# # Angenommene Daten\n",
    "# # X_train_scaled, y_train_scaled = # Deine skalierten Trainingsdaten\n",
    "# \n",
    "# def train_evaluate(neurons_layer_1, neurons_layer_2, neurons_layer_3, neurons_layer_4, neurons_layer_5, learning_rate):\n",
    "#     model = Sequential([\n",
    "#         Dense(int(neurons_layer_1), activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "#         Dense(int(neurons_layer_2), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "#         Dense(int(neurons_layer_3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "#         Dense(int(neurons_layer_4), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "#         Dense(int(neurons_layer_5), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "#         \n",
    "#         Dense(1, activation='linear')\n",
    "#     ])\n",
    "# \n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#     model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "# \n",
    "#     early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min', restore_best_weights=True, min_delta=0.0001)\n",
    "# \n",
    "#     history = model.fit(X_train_scaled, y_train_scaled, batch_size=32, epochs=100, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "# \n",
    "#     # Hier wählen wir den negativen Mean Squared Error, da Bayesian Optimization maximiert\n",
    "#     mse = np.min(history.history['val_loss'])\n",
    "#     return -mse\n",
    "# \n",
    "# # Definieren des Bereichs der Hyperparameter\n",
    "# pbounds = {\n",
    "#     'neurons_layer_1': (16, 200),\n",
    "#     'neurons_layer_2': (16, 200),\n",
    "#     'neurons_layer_3': (16, 200),\n",
    "#     'neurons_layer_4': (16, 200),\n",
    "#     'neurons_layer_5': (16, 200),\n",
    "#     'learning_rate': (0.0001, 0.01),\n",
    "# }\n",
    "# \n",
    "# # Initialisieren des BayesianOptimization-Objekts\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=train_evaluate,\n",
    "#     pbounds=pbounds,\n",
    "#     random_state=1,\n",
    "# )\n",
    "# \n",
    "# # Starten der Optimierung\n",
    "# optimizer.maximize(init_points=2, n_iter=20)\n",
    "# \n",
    "# print(optimizer.max)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T23:32:18.017548300Z",
     "start_time": "2024-02-28T23:32:18.013537200Z"
    }
   },
   "id": "a5b6232547cdae67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Search Architektur\n",
    "Tiefes Netz besser als breites Netz; Layer lernen auf unterschiedliche Weise"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75773dfef8260e5f"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from random_search\\random_search_D1_1\\tuner0.json\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.39\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.40\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.41\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.42\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.43\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'random_search_D1_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[53], line 41\u001B[0m\n\u001B[0;32m     39\u001B[0m df_hyperparameters \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame([best_hyperparameters\u001B[38;5;241m.\u001B[39mvalues])\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Speichern des DataFrame als CSV\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m \u001B[43mdf_hyperparameters\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrandom_search_D1_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mrun\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m best_model\u001B[38;5;241m.\u001B[39mdescribe()\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBeste Hyperparameter für Lauf \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrun\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_hyperparameters\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    328\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    329\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    330\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    331\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    332\u001B[0m     )\n\u001B[1;32m--> 333\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\pandas\\core\\generic.py:3964\u001B[0m, in \u001B[0;36mNDFrame.to_csv\u001B[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001B[0m\n\u001B[0;32m   3953\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, ABCDataFrame) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_frame()\n\u001B[0;32m   3955\u001B[0m formatter \u001B[38;5;241m=\u001B[39m DataFrameFormatter(\n\u001B[0;32m   3956\u001B[0m     frame\u001B[38;5;241m=\u001B[39mdf,\n\u001B[0;32m   3957\u001B[0m     header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3961\u001B[0m     decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   3962\u001B[0m )\n\u001B[1;32m-> 3964\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath_or_buf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3966\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlineterminator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlineterminator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3967\u001B[0m \u001B[43m    \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3968\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3969\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3970\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3971\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquoting\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquoting\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3972\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3973\u001B[0m \u001B[43m    \u001B[49m\u001B[43mindex_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3974\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3975\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3976\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquotechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquotechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3977\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdate_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdate_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3978\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdoublequote\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdoublequote\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3979\u001B[0m \u001B[43m    \u001B[49m\u001B[43mescapechar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mescapechar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3980\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3981\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_csv\u001B[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001B[0m\n\u001B[0;32m    993\u001B[0m     created_buffer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    995\u001B[0m csv_formatter \u001B[38;5;241m=\u001B[39m CSVFormatter(\n\u001B[0;32m    996\u001B[0m     path_or_buf\u001B[38;5;241m=\u001B[39mpath_or_buf,\n\u001B[0;32m    997\u001B[0m     lineterminator\u001B[38;5;241m=\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1012\u001B[0m     formatter\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1013\u001B[0m )\n\u001B[1;32m-> 1014\u001B[0m \u001B[43mcsv_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m created_buffer:\n\u001B[0;32m   1017\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(path_or_buf, StringIO)\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001B[0m, in \u001B[0;36mCSVFormatter.save\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    247\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;124;03mCreate the writer & save.\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# apply compression and byte/text conversion\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompression\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[0;32m    259\u001B[0m     \u001B[38;5;66;03m# Note: self.encoding is irrelevant here\u001B[39;00m\n\u001B[0;32m    260\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter \u001B[38;5;241m=\u001B[39m csvlib\u001B[38;5;241m.\u001B[39mwriter(\n\u001B[0;32m    261\u001B[0m         handles\u001B[38;5;241m.\u001B[39mhandle,\n\u001B[0;32m    262\u001B[0m         lineterminator\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineterminator,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    267\u001B[0m         quotechar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquotechar,\n\u001B[0;32m    268\u001B[0m     )\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save()\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mPermissionError\u001B[0m: [Errno 13] Permission denied: 'random_search_D1_1.csv'"
     ]
    }
   ],
   "source": [
    "# # Funktion zum Erstellen des Modells\n",
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(hp.Int('input_units', min_value=16, max_value=328, step=16), input_shape=(2,), activation='relu'))\n",
    "#     for i in range(hp.Int('n_layers', 1, 10)):\n",
    "#         model.add(Dense(hp.Int(f'units_{i}', min_value=16, max_value=328, step=16), activation='relu'))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#     return model\n",
    "# \n",
    "# # Durchführung der Random Search dreimal\n",
    "# for run in range(1, 4):\n",
    "#     # Anpassen des Verzeichnisses und des Projektnamens für jeden Durchlauf\n",
    "#     directory = 'random_search'\n",
    "#     project_name = f'random_search_D1_{run}'\n",
    "#     \n",
    "#     tuner = RandomSearch(\n",
    "#         build_model,\n",
    "#         objective='val_loss',\n",
    "#         max_trials=100,\n",
    "#         executions_per_trial=1,\n",
    "#         directory=directory,\n",
    "#         project_name=project_name\n",
    "#     )\n",
    "#     \n",
    "#     # Durchführung des Random Search\n",
    "#     tuner.search(X_train_scaled, y_train_scaled, epochs=50, verbose =0, batch_size=500, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "#     \n",
    "#     # Abrufen und Speichern des besten Modells\n",
    "#     best_model = tuner.get_best_models(num_models=1)[0]\n",
    "#     model_path = os.path.join(directory, project_name, 'best_model.h5') \n",
    "#     best_model.save(model_path)\n",
    "#     \n",
    "# \n",
    "#     # Optional: Abrufen und Ausgeben der besten Hyperparameter\n",
    "#     best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "#     \n",
    "#     # Konvertieren der Hyperparameter in ein DataFrame\n",
    "#     df_hyperparameters = pd.DataFrame([best_hyperparameters.values])\n",
    "#     # Speichern des DataFrame als CSV\n",
    "#     df_hyperparameters.to_csv(f'random_search_D1_{run}.csv', index=False)\n",
    "#     best_model.describe()\n",
    "#     print(f\"Beste Hyperparameter für Lauf {run}: {best_hyperparameters.values}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T09:20:52.809500800Z",
     "start_time": "2024-02-29T09:20:49.358443Z"
    }
   },
   "id": "158d81fabf560fc4"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               768       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 160)               20640     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 224)               36064     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 112)               25200     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 320)               36160     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                5136      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 112)               1904      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 113       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 158881 (620.63 KB)\n",
      "Trainable params: 158881 (620.63 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T09:23:22.039814800Z",
     "start_time": "2024-02-29T09:23:22.016141600Z"
    }
   },
   "id": "6f86db4f21a8c913"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2a8e01cea48e945f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

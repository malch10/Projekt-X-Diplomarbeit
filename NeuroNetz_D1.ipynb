{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b0518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:05.412821300Z",
     "start_time": "2024-02-28T10:46:01.134335600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "from keras_tuner import HyperModel\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ff61b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:08.596232700Z",
     "start_time": "2024-02-28T10:46:08.557219600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n0             0.0000      -0.00200        500   7000   9000      669.05\n1             0.0000      -0.00199        500   7000   9000      675.83\n2             0.0000      -0.00198        500   7000   9000      682.81\n3             0.0000      -0.00197        500   7000   9000      689.82\n4             0.0000      -0.00196        500   7000   9000      696.80\n...              ...           ...        ...    ...    ...         ...\n100646        0.0025       0.00196        500   7000   9000      578.47\n100647        0.0025       0.00197        500   7000   9000      576.89\n100648        0.0025       0.00198        500   7000   9000      575.32\n100649        0.0025       0.00199        500   7000   9000      573.76\n100650        0.0025       0.00200        500   7000   9000      572.20\n\n[100651 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00199</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>675.83</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00198</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>682.81</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00197</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>689.82</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>696.80</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>100646</th>\n      <td>0.0025</td>\n      <td>0.00196</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>578.47</td>\n    </tr>\n    <tr>\n      <th>100647</th>\n      <td>0.0025</td>\n      <td>0.00197</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>576.89</td>\n    </tr>\n    <tr>\n      <th>100648</th>\n      <td>0.0025</td>\n      <td>0.00198</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>575.32</td>\n    </tr>\n    <tr>\n      <th>100649</th>\n      <td>0.0025</td>\n      <td>0.00199</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>573.76</td>\n    </tr>\n    <tr>\n      <th>100650</th>\n      <td>0.0025</td>\n      <td>0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>572.20</td>\n    </tr>\n  </tbody>\n</table>\n<p>100651 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/TPath_300_finish_data.pkl')\n",
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish_D1_I7000_F9000/TPath_500_finish_data_D1.pkl')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966e3c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:09.393270100Z",
     "start_time": "2024-02-28T10:46:09.363010900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Temperatur\n0             0.0000      -0.00200      669.05\n1             0.0000      -0.00199      675.83\n2             0.0000      -0.00198      682.81\n3             0.0000      -0.00197      689.82\n4             0.0000      -0.00196      696.80\n...              ...           ...         ...\n100646        0.0025       0.00196      578.47\n100647        0.0025       0.00197      576.89\n100648        0.0025       0.00198      575.32\n100649        0.0025       0.00199      573.76\n100650        0.0025       0.00200      572.20\n\n[100651 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00199</td>\n      <td>675.83</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00198</td>\n      <td>682.81</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00197</td>\n      <td>689.82</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00196</td>\n      <td>696.80</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>100646</th>\n      <td>0.0025</td>\n      <td>0.00196</td>\n      <td>578.47</td>\n    </tr>\n    <tr>\n      <th>100647</th>\n      <td>0.0025</td>\n      <td>0.00197</td>\n      <td>576.89</td>\n    </tr>\n    <tr>\n      <th>100648</th>\n      <td>0.0025</td>\n      <td>0.00198</td>\n      <td>575.32</td>\n    </tr>\n    <tr>\n      <th>100649</th>\n      <td>0.0025</td>\n      <td>0.00199</td>\n      <td>573.76</td>\n    </tr>\n    <tr>\n      <th>100650</th>\n      <td>0.0025</td>\n      <td>0.00200</td>\n      <td>572.20</td>\n    </tr>\n  </tbody>\n</table>\n<p>100651 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop(data.columns[2:5], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8783d1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:09.733876500Z",
     "start_time": "2024-02-28T10:46:09.670879700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       X-Koordinate  Y-Koordinate  Temperatur\n",
      "83145       0.00207      -0.00062      1282.2\n",
      "66701       0.00166      -0.00065      1347.6\n",
      "91325       0.00227       0.00098      1094.6\n",
      "46593       0.00116      -0.00123      1175.7\n",
      "49518       0.00123      -0.00005      1459.2\n",
      "...             ...           ...         ...\n",
      "6265        0.00015       0.00050      1439.1\n",
      "54886       0.00136       0.00150       876.7\n",
      "76820       0.00191       0.00029      1335.8\n",
      "860         0.00002      -0.00142      1071.2\n",
      "15795       0.00039      -0.00044      1488.1\n",
      "\n",
      "[100651 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Temperatur\n0            0.00207      -0.00062      1282.2\n1            0.00166      -0.00065      1347.6\n2            0.00227       0.00098      1094.6\n3            0.00116      -0.00123      1175.7\n4            0.00123      -0.00005      1459.2\n...              ...           ...         ...\n100646       0.00015       0.00050      1439.1\n100647       0.00136       0.00150       876.7\n100648       0.00191       0.00029      1335.8\n100649       0.00002      -0.00142      1071.2\n100650       0.00039      -0.00044      1488.1\n\n[100651 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00207</td>\n      <td>-0.00062</td>\n      <td>1282.2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00166</td>\n      <td>-0.00065</td>\n      <td>1347.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00227</td>\n      <td>0.00098</td>\n      <td>1094.6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00116</td>\n      <td>-0.00123</td>\n      <td>1175.7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00123</td>\n      <td>-0.00005</td>\n      <td>1459.2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>100646</th>\n      <td>0.00015</td>\n      <td>0.00050</td>\n      <td>1439.1</td>\n    </tr>\n    <tr>\n      <th>100647</th>\n      <td>0.00136</td>\n      <td>0.00150</td>\n      <td>876.7</td>\n    </tr>\n    <tr>\n      <th>100648</th>\n      <td>0.00191</td>\n      <td>0.00029</td>\n      <td>1335.8</td>\n    </tr>\n    <tr>\n      <th>100649</th>\n      <td>0.00002</td>\n      <td>-0.00142</td>\n      <td>1071.2</td>\n    </tr>\n    <tr>\n      <th>100650</th>\n      <td>0.00039</td>\n      <td>-0.00044</td>\n      <td>1488.1</td>\n    </tr>\n  </tbody>\n</table>\n<p>100651 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "\n",
    "print(df1)\n",
    "df_reset = df1.reset_index(drop=True)\n",
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e72a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:09.883114700Z",
     "start_time": "2024-02-28T10:46:09.838711200Z"
    }
   },
   "outputs": [],
   "source": [
    "label = df_reset[\"Temperatur\"]\n",
    "# Korrektur: Verwenden Sie den Spaltennamen direkt, ohne Indexierung der columns-Eigenschaft\n",
    "df1 = df_reset.drop(\"Temperatur\", axis=1)\n",
    "X = df1\n",
    "y = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7fa289a50d87423"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e694a236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:10.219998900Z",
     "start_time": "2024-02-28T10:46:10.178152400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate\ncount  100651.000000  1.006510e+05\nmean        0.001250  1.103042e-20\nstd         0.000725  1.157589e-03\nmin         0.000000 -2.000000e-03\n25%         0.000620 -1.000000e-03\n50%         0.001250  4.529900e-18\n75%         0.001880  1.000000e-03\nmax         0.002500  2.000000e-03",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100651.000000</td>\n      <td>1.006510e+05</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.001250</td>\n      <td>1.103042e-20</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.000725</td>\n      <td>1.157589e-03</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-2.000000e-03</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000620</td>\n      <td>-1.000000e-03</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.001250</td>\n      <td>4.529900e-18</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.001880</td>\n      <td>1.000000e-03</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.002500</td>\n      <td>2.000000e-03</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3303b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:10.404794Z",
     "start_time": "2024-02-28T10:46:10.389794500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "count    100651.000000\nmean       1144.030064\nstd         264.135723\nmin         572.200000\n25%         937.330000\n50%        1201.100000\n75%        1368.700000\nmax        1520.000000\nName: Temperatur, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ad8da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:10.619230700Z",
     "start_time": "2024-02-28T10:46:10.573586Z"
    }
   },
   "outputs": [],
   "source": [
    " # train_df enthält 80% der Daten, test_df enthält 20% der Daten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c705edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:10.701034600Z",
     "start_time": "2024-02-28T10:46:10.692230Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_features2 = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "X_test_scaled = scaler_features.transform(X_test)  # Nutze unterschiedliche Skalierungsparameter\n",
    "\n",
    "# Initialisiere einen SEPARATEN MinMaxScaler für das Ziel, wenn nötig\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Skaliere y_train und y_test. Beachte, dass y_train.reshape(-1, 1) verwendet wird, da MinMaxScaler \n",
    "# erwartet, dass die Eingaben als 2D-Arrays kommen, und Ziele normalerweise als 1D-Arrays vorliegen.\n",
    "y_train_scaled = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbefe631e495b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T10:46:12.739249300Z",
     "start_time": "2024-02-28T10:46:12.730885900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.416 , 0.5725],\n       [0.812 , 0.7325],\n       [0.628 , 0.5075],\n       ...,\n       [0.604 , 0.3875],\n       [0.748 , 0.65  ],\n       [0.028 , 0.6225]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "129/129 [==============================] - 2s 6ms/step - loss: 0.2231 - mae: 0.1356 - val_loss: 0.1428 - val_mae: 0.0094\n",
      "Epoch 2/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.1312 - mae: 0.0177 - val_loss: 0.1213 - val_mae: 0.0110\n",
      "Epoch 3/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.1151 - mae: 0.0093 - val_loss: 0.1093 - val_mae: 0.0048\n",
      "Epoch 4/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.1048 - mae: 0.0113 - val_loss: 0.1002 - val_mae: 0.0074\n",
      "Epoch 5/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0965 - mae: 0.0107 - val_loss: 0.0927 - val_mae: 0.0086\n",
      "Epoch 6/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0894 - mae: 0.0089 - val_loss: 0.0860 - val_mae: 0.0049\n",
      "Epoch 7/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0832 - mae: 0.0096 - val_loss: 0.0803 - val_mae: 0.0096\n",
      "Epoch 8/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0777 - mae: 0.0108 - val_loss: 0.0749 - val_mae: 0.0037\n",
      "Epoch 9/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0726 - mae: 0.0065 - val_loss: 0.0702 - val_mae: 0.0060\n",
      "Epoch 10/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0681 - mae: 0.0077 - val_loss: 0.0664 - val_mae: 0.0209\n",
      "Epoch 11/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0640 - mae: 0.0094 - val_loss: 0.0619 - val_mae: 0.0040\n",
      "Epoch 12/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0601 - mae: 0.0061 - val_loss: 0.0583 - val_mae: 0.0081\n",
      "Epoch 13/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0566 - mae: 0.0075 - val_loss: 0.0549 - val_mae: 0.0066\n",
      "Epoch 14/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0534 - mae: 0.0087 - val_loss: 0.0518 - val_mae: 0.0075\n",
      "Epoch 15/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0503 - mae: 0.0051 - val_loss: 0.0488 - val_mae: 0.0052\n",
      "Epoch 16/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0475 - mae: 0.0079 - val_loss: 0.0460 - val_mae: 0.0036\n",
      "Epoch 17/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0447 - mae: 0.0055 - val_loss: 0.0434 - val_mae: 0.0035\n",
      "Epoch 18/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0423 - mae: 0.0073 - val_loss: 0.0410 - val_mae: 0.0064\n",
      "Epoch 19/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0399 - mae: 0.0064 - val_loss: 0.0387 - val_mae: 0.0035\n",
      "Epoch 20/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0377 - mae: 0.0067 - val_loss: 0.0366 - val_mae: 0.0083\n",
      "Epoch 21/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0355 - mae: 0.0051 - val_loss: 0.0345 - val_mae: 0.0076\n",
      "Epoch 22/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0335 - mae: 0.0069 - val_loss: 0.0325 - val_mae: 0.0035\n",
      "Epoch 23/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0316 - mae: 0.0060 - val_loss: 0.0307 - val_mae: 0.0060\n",
      "Epoch 24/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0298 - mae: 0.0073 - val_loss: 0.0289 - val_mae: 0.0032\n",
      "Epoch 25/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0281 - mae: 0.0059 - val_loss: 0.0274 - val_mae: 0.0111\n",
      "Epoch 26/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0265 - mae: 0.0069 - val_loss: 0.0257 - val_mae: 0.0068\n",
      "Epoch 27/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0250 - mae: 0.0075 - val_loss: 0.0243 - val_mae: 0.0105\n",
      "Epoch 28/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0236 - mae: 0.0093 - val_loss: 0.0228 - val_mae: 0.0054\n",
      "Epoch 29/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0221 - mae: 0.0037 - val_loss: 0.0214 - val_mae: 0.0032\n",
      "Epoch 30/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0208 - mae: 0.0040 - val_loss: 0.0203 - val_mae: 0.0108\n",
      "Epoch 31/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0196 - mae: 0.0081 - val_loss: 0.0190 - val_mae: 0.0080\n",
      "Epoch 32/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0184 - mae: 0.0065 - val_loss: 0.0179 - val_mae: 0.0072\n",
      "Epoch 33/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0173 - mae: 0.0063 - val_loss: 0.0171 - val_mae: 0.0185\n",
      "Epoch 34/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0164 - mae: 0.0086 - val_loss: 0.0158 - val_mae: 0.0077\n",
      "Epoch 35/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0154 - mae: 0.0074 - val_loss: 0.0149 - val_mae: 0.0095\n",
      "Epoch 36/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0145 - mae: 0.0074 - val_loss: 0.0139 - val_mae: 0.0035\n",
      "Epoch 37/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0136 - mae: 0.0077 - val_loss: 0.0132 - val_mae: 0.0083\n",
      "Epoch 38/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0128 - mae: 0.0066 - val_loss: 0.0123 - val_mae: 0.0037\n",
      "Epoch 39/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0121 - mae: 0.0074 - val_loss: 0.0116 - val_mae: 0.0048\n",
      "Epoch 40/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0113 - mae: 0.0063 - val_loss: 0.0112 - val_mae: 0.0155\n",
      "Epoch 41/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0107 - mae: 0.0080 - val_loss: 0.0103 - val_mae: 0.0033\n",
      "Epoch 42/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0100 - mae: 0.0056 - val_loss: 0.0097 - val_mae: 0.0067\n",
      "Epoch 43/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0095 - mae: 0.0086 - val_loss: 0.0095 - val_mae: 0.0167\n",
      "Epoch 44/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0090 - mae: 0.0065 - val_loss: 0.0089 - val_mae: 0.0137\n",
      "Epoch 45/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0085 - mae: 0.0077 - val_loss: 0.0082 - val_mae: 0.0032\n",
      "Epoch 46/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0079 - mae: 0.0050 - val_loss: 0.0077 - val_mae: 0.0058\n",
      "Epoch 47/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0075 - mae: 0.0068 - val_loss: 0.0073 - val_mae: 0.0032\n",
      "Epoch 48/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0071 - mae: 0.0070 - val_loss: 0.0069 - val_mae: 0.0039\n",
      "Epoch 49/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0068 - mae: 0.0064 - val_loss: 0.0065 - val_mae: 0.0042\n",
      "Epoch 50/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0064 - mae: 0.0072 - val_loss: 0.0062 - val_mae: 0.0035\n",
      "Epoch 51/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0061 - mae: 0.0067 - val_loss: 0.0058 - val_mae: 0.0033\n",
      "Epoch 52/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0057 - mae: 0.0065 - val_loss: 0.0055 - val_mae: 0.0036\n",
      "Epoch 53/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0054 - mae: 0.0066 - val_loss: 0.0052 - val_mae: 0.0030\n",
      "Epoch 54/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0052 - mae: 0.0061 - val_loss: 0.0051 - val_mae: 0.0106\n",
      "Epoch 55/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0049 - mae: 0.0078 - val_loss: 0.0047 - val_mae: 0.0033\n",
      "Epoch 56/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0046 - mae: 0.0049 - val_loss: 0.0045 - val_mae: 0.0070\n",
      "Epoch 57/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0044 - mae: 0.0060 - val_loss: 0.0043 - val_mae: 0.0029\n",
      "Epoch 58/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0042 - mae: 0.0066 - val_loss: 0.0042 - val_mae: 0.0112\n",
      "Epoch 59/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0040 - mae: 0.0060 - val_loss: 0.0039 - val_mae: 0.0037\n",
      "Epoch 60/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0038 - mae: 0.0059 - val_loss: 0.0038 - val_mae: 0.0121\n",
      "Epoch 61/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0036 - mae: 0.0059 - val_loss: 0.0035 - val_mae: 0.0057\n",
      "Epoch 62/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0035 - mae: 0.0068 - val_loss: 0.0034 - val_mae: 0.0074\n",
      "Epoch 63/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0033 - mae: 0.0054 - val_loss: 0.0032 - val_mae: 0.0034\n",
      "Epoch 64/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0032 - mae: 0.0060 - val_loss: 0.0036 - val_mae: 0.0217\n",
      "Epoch 65/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0030 - mae: 0.0055 - val_loss: 0.0029 - val_mae: 0.0063\n",
      "Epoch 66/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0029 - mae: 0.0058 - val_loss: 0.0029 - val_mae: 0.0082\n",
      "Epoch 67/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0028 - mae: 0.0053 - val_loss: 0.0027 - val_mae: 0.0043\n",
      "Epoch 68/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0027 - mae: 0.0067 - val_loss: 0.0027 - val_mae: 0.0104\n",
      "Epoch 69/200\n",
      "129/129 [==============================] - 1s 9ms/step - loss: 0.0025 - mae: 0.0060 - val_loss: 0.0025 - val_mae: 0.0077\n",
      "Epoch 70/200\n",
      "129/129 [==============================] - 1s 7ms/step - loss: 0.0024 - mae: 0.0045 - val_loss: 0.0024 - val_mae: 0.0073\n",
      "Epoch 71/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0024 - mae: 0.0073 - val_loss: 0.0023 - val_mae: 0.0069\n",
      "Epoch 72/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0023 - mae: 0.0055 - val_loss: 0.0022 - val_mae: 0.0051\n",
      "Epoch 73/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0022 - mae: 0.0057 - val_loss: 0.0021 - val_mae: 0.0028\n",
      "Epoch 74/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0021 - mae: 0.0059 - val_loss: 0.0021 - val_mae: 0.0065\n",
      "Epoch 75/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0020 - mae: 0.0063 - val_loss: 0.0022 - val_mae: 0.0129\n",
      "Epoch 76/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0020 - mae: 0.0058 - val_loss: 0.0020 - val_mae: 0.0088\n",
      "Epoch 77/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0019 - mae: 0.0062 - val_loss: 0.0018 - val_mae: 0.0035\n",
      "Epoch 78/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0040 - val_loss: 0.0018 - val_mae: 0.0040\n",
      "Epoch 79/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0018 - mae: 0.0063 - val_loss: 0.0017 - val_mae: 0.0060\n",
      "Epoch 80/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0017 - mae: 0.0056 - val_loss: 0.0017 - val_mae: 0.0045\n",
      "Epoch 81/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0054 - val_loss: 0.0016 - val_mae: 0.0032\n",
      "Epoch 82/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0017 - mae: 0.0070 - val_loss: 0.0016 - val_mae: 0.0028\n",
      "Epoch 83/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0016 - mae: 0.0054 - val_loss: 0.0018 - val_mae: 0.0164\n",
      "Epoch 84/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0015 - mae: 0.0055 - val_loss: 0.0015 - val_mae: 0.0070\n",
      "Epoch 85/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0062 - val_loss: 0.0014 - val_mae: 0.0030\n",
      "Epoch 86/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0062 - val_loss: 0.0014 - val_mae: 0.0066\n",
      "Epoch 87/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0052 - val_loss: 0.0015 - val_mae: 0.0114\n",
      "Epoch 88/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0014 - mae: 0.0069 - val_loss: 0.0013 - val_mae: 0.0042\n",
      "Epoch 89/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0014 - mae: 0.0054 - val_loss: 0.0013 - val_mae: 0.0035\n",
      "Epoch 90/200\n",
      "129/129 [==============================] - 1s 6ms/step - loss: 0.0013 - mae: 0.0061 - val_loss: 0.0013 - val_mae: 0.0029\n",
      "Epoch 91/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0052 - val_loss: 0.0013 - val_mae: 0.0037\n",
      "Epoch 92/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0066 - val_loss: 0.0012 - val_mae: 0.0034\n",
      "Epoch 93/200\n",
      "129/129 [==============================] - 1s 5ms/step - loss: 0.0013 - mae: 0.0064 - val_loss: 0.0012 - val_mae: 0.0043\n",
      "Epoch 94/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0012 - mae: 0.0054 - val_loss: 0.0012 - val_mae: 0.0028\n",
      "Epoch 95/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0012 - mae: 0.0057 - val_loss: 0.0012 - val_mae: 0.0050\n",
      "Epoch 96/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0012 - mae: 0.0063 - val_loss: 0.0016 - val_mae: 0.0209\n",
      "Epoch 97/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0012 - mae: 0.0064 - val_loss: 0.0011 - val_mae: 0.0031\n",
      "Epoch 98/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0011 - mae: 0.0049 - val_loss: 0.0011 - val_mae: 0.0042\n",
      "Epoch 99/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0056 - val_loss: 0.0011 - val_mae: 0.0053\n",
      "Epoch 100/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.0011 - mae: 0.0066 - val_loss: 0.0011 - val_mae: 0.0033\n",
      "Epoch 101/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0011 - mae: 0.0055 - val_loss: 0.0013 - val_mae: 0.0163\n",
      "Epoch 102/200\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 0.0011 - mae: 0.0067 - val_loss: 0.0010 - val_mae: 0.0037\n",
      "Epoch 103/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0010 - mae: 0.0041 - val_loss: 0.0011 - val_mae: 0.0093\n",
      "Epoch 104/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0011 - mae: 0.0059 - val_loss: 0.0010 - val_mae: 0.0055\n",
      "Epoch 105/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0065 - val_loss: 9.9315e-04 - val_mae: 0.0030\n",
      "Epoch 106/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 0.0010 - mae: 0.0062 - val_loss: 0.0011 - val_mae: 0.0117\n",
      "Epoch 107/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0010 - mae: 0.0062 - val_loss: 0.0010 - val_mae: 0.0082\n",
      "Epoch 108/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 0.0010 - mae: 0.0066 - val_loss: 0.0010 - val_mae: 0.0069\n",
      "Epoch 109/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 9.8268e-04 - mae: 0.0050 - val_loss: 9.9169e-04 - val_mae: 0.0070\n",
      "Epoch 110/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.9876e-04 - mae: 0.0065 - val_loss: 0.0011 - val_mae: 0.0110\n",
      "Epoch 111/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 9.7331e-04 - mae: 0.0058 - val_loss: 9.2895e-04 - val_mae: 0.0031\n",
      "Epoch 112/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.6879e-04 - mae: 0.0061 - val_loss: 9.1840e-04 - val_mae: 0.0029\n",
      "Epoch 113/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.6448e-04 - mae: 0.0062 - val_loss: 9.2294e-04 - val_mae: 0.0043\n",
      "Epoch 114/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.5360e-04 - mae: 0.0064 - val_loss: 8.9951e-04 - val_mae: 0.0027\n",
      "Epoch 115/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.3919e-04 - mae: 0.0058 - val_loss: 9.2206e-04 - val_mae: 0.0057\n",
      "Epoch 116/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.4572e-04 - mae: 0.0064 - val_loss: 8.8725e-04 - val_mae: 0.0030\n",
      "Epoch 117/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.3118e-04 - mae: 0.0064 - val_loss: 0.0010 - val_mae: 0.0109\n",
      "Epoch 118/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.2014e-04 - mae: 0.0059 - val_loss: 8.8339e-04 - val_mae: 0.0042\n",
      "Epoch 119/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.2666e-04 - mae: 0.0067 - val_loss: 9.1531e-04 - val_mae: 0.0070\n",
      "Epoch 120/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 9.0061e-04 - mae: 0.0055 - val_loss: 8.6079e-04 - val_mae: 0.0032\n",
      "Epoch 121/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 9.0939e-04 - mae: 0.0066 - val_loss: 8.5371e-04 - val_mae: 0.0029\n",
      "Epoch 122/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.9193e-04 - mae: 0.0059 - val_loss: 8.5031e-04 - val_mae: 0.0033\n",
      "Epoch 123/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.9339e-04 - mae: 0.0062 - val_loss: 8.7670e-04 - val_mae: 0.0062\n",
      "Epoch 124/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.7782e-04 - mae: 0.0059 - val_loss: 8.7109e-04 - val_mae: 0.0062\n",
      "Epoch 125/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.8868e-04 - mae: 0.0066 - val_loss: 8.8259e-04 - val_mae: 0.0072\n",
      "Epoch 126/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.8729e-04 - mae: 0.0067 - val_loss: 9.2404e-04 - val_mae: 0.0093\n",
      "Epoch 127/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 8.5892e-04 - mae: 0.0055 - val_loss: 8.2165e-04 - val_mae: 0.0030\n",
      "Epoch 128/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 8.7516e-04 - mae: 0.0063 - val_loss: 8.5217e-04 - val_mae: 0.0062\n",
      "Epoch 129/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 8.6176e-04 - mae: 0.0060 - val_loss: 8.2555e-04 - val_mae: 0.0045\n",
      "Epoch 130/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 8.6949e-04 - mae: 0.0067 - val_loss: 8.0850e-04 - val_mae: 0.0029\n",
      "Epoch 131/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 8.3583e-04 - mae: 0.0051 - val_loss: 8.0454e-04 - val_mae: 0.0032\n",
      "Epoch 132/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.4006e-04 - mae: 0.0056 - val_loss: 7.9962e-04 - val_mae: 0.0032\n",
      "Epoch 133/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.6760e-04 - mae: 0.0069 - val_loss: 8.6088e-04 - val_mae: 0.0082\n",
      "Epoch 134/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.3362e-04 - mae: 0.0059 - val_loss: 8.0881e-04 - val_mae: 0.0046\n",
      "Epoch 135/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.4830e-04 - mae: 0.0068 - val_loss: 7.9893e-04 - val_mae: 0.0043\n",
      "Epoch 136/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 8.3192e-04 - mae: 0.0063 - val_loss: 7.8184e-04 - val_mae: 0.0028\n",
      "Epoch 137/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 8.3039e-04 - mae: 0.0061 - val_loss: 8.3021e-04 - val_mae: 0.0070\n",
      "Epoch 138/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 8.3015e-04 - mae: 0.0063 - val_loss: 7.9927e-04 - val_mae: 0.0049\n",
      "Epoch 139/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.0978e-04 - mae: 0.0055 - val_loss: 7.7910e-04 - val_mae: 0.0038\n",
      "Epoch 140/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.2072e-04 - mae: 0.0062 - val_loss: 8.8354e-04 - val_mae: 0.0105\n",
      "Epoch 141/200\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 8.0535e-04 - mae: 0.0056 - val_loss: 9.5290e-04 - val_mae: 0.0133\n",
      "Epoch 142/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.2207e-04 - mae: 0.0069 - val_loss: 7.6221e-04 - val_mae: 0.0029\n",
      "Epoch 143/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.1841e-04 - mae: 0.0067 - val_loss: 7.6933e-04 - val_mae: 0.0041\n",
      "Epoch 144/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 8.0034e-04 - mae: 0.0059 - val_loss: 7.5913e-04 - val_mae: 0.0032\n",
      "Epoch 145/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.9009e-04 - mae: 0.0055 - val_loss: 7.5132e-04 - val_mae: 0.0028\n",
      "Epoch 146/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.0065e-04 - mae: 0.0063 - val_loss: 9.4961e-04 - val_mae: 0.0139\n",
      "Epoch 147/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.9149e-04 - mae: 0.0058 - val_loss: 7.4572e-04 - val_mae: 0.0030\n",
      "Epoch 148/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.9782e-04 - mae: 0.0060 - val_loss: 8.2467e-04 - val_mae: 0.0090\n",
      "Epoch 149/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 8.1460e-04 - mae: 0.0070 - val_loss: 7.4381e-04 - val_mae: 0.0033\n",
      "Epoch 150/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 7.9062e-04 - mae: 0.0062 - val_loss: 7.7086e-04 - val_mae: 0.0055\n",
      "Epoch 151/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.6338e-04 - mae: 0.0051 - val_loss: 7.6023e-04 - val_mae: 0.0054\n",
      "Epoch 152/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.9039e-04 - mae: 0.0068 - val_loss: 8.3097e-04 - val_mae: 0.0097\n",
      "Epoch 153/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.8163e-04 - mae: 0.0061 - val_loss: 7.3206e-04 - val_mae: 0.0030\n",
      "Epoch 154/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.6868e-04 - mae: 0.0060 - val_loss: 7.3082e-04 - val_mae: 0.0032\n",
      "Epoch 155/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.7227e-04 - mae: 0.0063 - val_loss: 7.3443e-04 - val_mae: 0.0036\n",
      "Epoch 156/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.8741e-04 - mae: 0.0066 - val_loss: 7.3251e-04 - val_mae: 0.0040\n",
      "Epoch 157/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.5964e-04 - mae: 0.0056 - val_loss: 9.0052e-04 - val_mae: 0.0128\n",
      "Epoch 158/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.7802e-04 - mae: 0.0067 - val_loss: 7.4250e-04 - val_mae: 0.0055\n",
      "Epoch 159/200\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 7.5610e-04 - mae: 0.0057 - val_loss: 7.1644e-04 - val_mae: 0.0030\n",
      "Epoch 160/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.6618e-04 - mae: 0.0064 - val_loss: 7.8105e-04 - val_mae: 0.0081\n",
      "Epoch 161/200\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 7.6813e-04 - mae: 0.0065 - val_loss: 7.1092e-04 - val_mae: 0.0029\n",
      "Epoch 162/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 7.6095e-04 - mae: 0.0063 - val_loss: 7.6819e-04 - val_mae: 0.0077\n",
      "Epoch 163/200\n",
      "129/129 [==============================] - 0s 3ms/step - loss: 7.5385e-04 - mae: 0.0062 - val_loss: 7.0813e-04 - val_mae: 0.0030\n",
      "Epoch 164/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.6151e-04 - mae: 0.0065 - val_loss: 7.5682e-04 - val_mae: 0.0072\n",
      "Epoch 165/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.4170e-04 - mae: 0.0056 - val_loss: 7.3644e-04 - val_mae: 0.0060\n",
      "Epoch 166/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.6093e-04 - mae: 0.0070 - val_loss: 7.0430e-04 - val_mae: 0.0038\n",
      "Epoch 167/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 7.8441e-04 - mae: 0.0072 - val_loss: 7.0015e-04 - val_mae: 0.0034\n",
      "Epoch 168/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 7.0828e-04 - mae: 0.0041 - val_loss: 7.0195e-04 - val_mae: 0.0040\n",
      "Epoch 169/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.4695e-04 - mae: 0.0059 - val_loss: 7.1621e-04 - val_mae: 0.0053\n",
      "Epoch 170/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.3431e-04 - mae: 0.0060 - val_loss: 6.9483e-04 - val_mae: 0.0037\n",
      "Epoch 171/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.4256e-04 - mae: 0.0066 - val_loss: 7.0475e-04 - val_mae: 0.0044\n",
      "Epoch 172/200\n",
      "129/129 [==============================] - 0s 4ms/step - loss: 7.2914e-04 - mae: 0.0060 - val_loss: 7.7426e-04 - val_mae: 0.0095\n",
      "Epoch 173/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 7.4912e-04 - mae: 0.0069 - val_loss: 8.5459e-04 - val_mae: 0.0127\n",
      "Epoch 174/200\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 7.3382e-04 - mae: 0.0060 - val_loss: 7.0444e-04 - val_mae: 0.0052\n",
      "Epoch 175/200\n",
      "118/129 [==========================>...] - ETA: 0s - loss: 7.2614e-04 - mae: 0.0060Restoring model weights from the end of the best epoch: 170.\n",
      "129/129 [==============================] - 1s 4ms/step - loss: 7.2763e-04 - mae: 0.0061 - val_loss: 7.7484e-04 - val_mae: 0.0097\n",
      "Epoch 175: early stopping\n",
      "Die Ausführungszeit betrug 103.6082022190094 Sekunden.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "    # Eingabeschicht\n",
    "    Dense(224, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(32, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(96, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    " \n",
    "    Dense(224, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(352, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=2, mode='min', restore_best_weights=True)#, min_delta = 0.00005)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 500\n",
    "epochs = 200\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose = 1)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Berechne die Dauer\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(f\"Die Ausführungszeit betrug {duration} Sekunden.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T12:02:19.100603800Z",
     "start_time": "2024-02-28T12:00:35.475593100Z"
    }
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f6f672a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T12:02:41.951650400Z",
     "start_time": "2024-02-28T12:02:41.033520700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bsp. Predicted: [1197.9163] Actual: [1195.9] \n",
      "Durchschnittliche Abweichung (MAE): [3.52265818]\n"
     ]
    }
   ],
   "source": [
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Führen Sie die Rücktransformation der skalierten Werte durch\n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[1000]} Actual: {original_actual_values[1000]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "    \n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "    \n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Werte die kleiner sind: 370\n"
     ]
    },
    {
     "data": {
      "text/plain": "             Echt  Vorhergesagt  X-Koordinate  Y-Koordinate  Differenz\n19281  773.464722        807.84         1.000        0.9125 -34.375278\n14143  799.875488        832.75         1.000        0.9025 -32.874512\n16514  806.433167        838.63         1.000        0.9000 -32.196833\n4160   773.369629        805.35         0.992        0.9125 -31.980371\n9896   760.146057        791.74         0.992        0.9175 -31.593943\n...           ...           ...           ...           ...        ...\n9782   686.061401        665.99         0.920        0.0000  20.071401\n7694   687.607117        667.48         0.944        0.0000  20.127117\n11547  685.803589        665.34         0.916        0.0000  20.463589\n1409   687.091919        666.57         0.936        0.0000  20.521919\n18290  682.196655        661.65         0.860        0.0000  20.546655\n\n[20131 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Echt</th>\n      <th>Vorhergesagt</th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Differenz</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>19281</th>\n      <td>773.464722</td>\n      <td>807.84</td>\n      <td>1.000</td>\n      <td>0.9125</td>\n      <td>-34.375278</td>\n    </tr>\n    <tr>\n      <th>14143</th>\n      <td>799.875488</td>\n      <td>832.75</td>\n      <td>1.000</td>\n      <td>0.9025</td>\n      <td>-32.874512</td>\n    </tr>\n    <tr>\n      <th>16514</th>\n      <td>806.433167</td>\n      <td>838.63</td>\n      <td>1.000</td>\n      <td>0.9000</td>\n      <td>-32.196833</td>\n    </tr>\n    <tr>\n      <th>4160</th>\n      <td>773.369629</td>\n      <td>805.35</td>\n      <td>0.992</td>\n      <td>0.9125</td>\n      <td>-31.980371</td>\n    </tr>\n    <tr>\n      <th>9896</th>\n      <td>760.146057</td>\n      <td>791.74</td>\n      <td>0.992</td>\n      <td>0.9175</td>\n      <td>-31.593943</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9782</th>\n      <td>686.061401</td>\n      <td>665.99</td>\n      <td>0.920</td>\n      <td>0.0000</td>\n      <td>20.071401</td>\n    </tr>\n    <tr>\n      <th>7694</th>\n      <td>687.607117</td>\n      <td>667.48</td>\n      <td>0.944</td>\n      <td>0.0000</td>\n      <td>20.127117</td>\n    </tr>\n    <tr>\n      <th>11547</th>\n      <td>685.803589</td>\n      <td>665.34</td>\n      <td>0.916</td>\n      <td>0.0000</td>\n      <td>20.463589</td>\n    </tr>\n    <tr>\n      <th>1409</th>\n      <td>687.091919</td>\n      <td>666.57</td>\n      <td>0.936</td>\n      <td>0.0000</td>\n      <td>20.521919</td>\n    </tr>\n    <tr>\n      <th>18290</th>\n      <td>682.196655</td>\n      <td>661.65</td>\n      <td>0.860</td>\n      <td>0.0000</td>\n      <td>20.546655</td>\n    </tr>\n  </tbody>\n</table>\n<p>20131 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'Echt': [val[0] for val in list1], 'Vorhergesagt': [val[0] for val in list2]})\n",
    "df_result['X-Koordinate'] = X_test_scaled[:, 0]\n",
    "df_result['Y-Koordinate'] = X_test_scaled[:, 1]\n",
    "\n",
    "df_result['Differenz'] = df_result['Echt'] - df_result['Vorhergesagt']\n",
    "df_result['Differenz'].sort_values()\n",
    "sorted_df = df_result.sort_values(by= 'Differenz')\n",
    "Anzahl_Punkte = (sorted_df['Differenz'] < -10).sum()\n",
    "print(\"Anzahl der Werte die kleiner sind:\", Anzahl_Punkte)\n",
    " \n",
    "sorted_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T12:02:57.475590300Z",
     "start_time": "2024-02-28T12:02:57.375128200Z"
    }
   },
   "id": "42bde60d3b4f2007"
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae = history.history['loss']\n",
    "val_mae = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(mae) + 1)\n",
    "# MAE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mae, 'r', label='Training MSE')\n",
    "plt.plot(epochs, val_mae, 'b', label='Validation MSE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T12:03:02.628843800Z",
     "start_time": "2024-02-28T12:03:02.076886700Z"
    }
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# GridSearch\n",
    "Grid Search bietet die Vorteile der anpassbaren Rechenzeit sowie die Möglichkeit des EInsatzes von Verteilungen. So können theoretisch Hyperparamterkonfuigurationen gefunden wende, welche durch GridSearch nicht auffindbar wären. ZUdem ist das Ziel der Hyperparamteroptimierung eine Einstellung zu finden, welche auf Trainings und Testset gut angepasst ist. Die EInstellung muss nicht die bestmöglichste Einstellung sein, sondern eine Einstellung die das gewähltre Problem gut wiederspiegelt. \n",
    "Bayesian Optimierung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c177960cc729052"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f17e2cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-28T11:43:40.551323300Z",
     "start_time": "2024-02-28T10:46:31.827883700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "162/162 - 12s - loss: 0.1968 - mae: 0.0992 - 12s/epoch - 76ms/step\n",
      "Epoch 2/30\n",
      "162/162 - 1s - loss: 0.1165 - mae: 0.0120 - 1s/epoch - 7ms/step\n",
      "Epoch 3/30\n",
      "162/162 - 1s - loss: 0.1008 - mae: 0.0113 - 1s/epoch - 8ms/step\n",
      "Epoch 4/30\n",
      "162/162 - 2s - loss: 0.0900 - mae: 0.0098 - 2s/epoch - 10ms/step\n",
      "Epoch 5/30\n",
      "162/162 - 2s - loss: 0.0813 - mae: 0.0086 - 2s/epoch - 14ms/step\n",
      "Epoch 6/30\n",
      "162/162 - 3s - loss: 0.0740 - mae: 0.0100 - 3s/epoch - 17ms/step\n",
      "Epoch 7/30\n",
      "162/162 - 3s - loss: 0.0674 - mae: 0.0073 - 3s/epoch - 17ms/step\n",
      "Epoch 8/30\n",
      "162/162 - 3s - loss: 0.0617 - mae: 0.0081 - 3s/epoch - 19ms/step\n",
      "Epoch 9/30\n",
      "162/162 - 3s - loss: 0.0566 - mae: 0.0080 - 3s/epoch - 17ms/step\n",
      "Epoch 10/30\n",
      "162/162 - 3s - loss: 0.0520 - mae: 0.0070 - 3s/epoch - 17ms/step\n",
      "Epoch 11/30\n",
      "162/162 - 3s - loss: 0.0479 - mae: 0.0068 - 3s/epoch - 18ms/step\n",
      "Epoch 12/30\n",
      "162/162 - 3s - loss: 0.0442 - mae: 0.0072 - 3s/epoch - 18ms/step\n",
      "Epoch 13/30\n",
      "162/162 - 3s - loss: 0.0408 - mae: 0.0075 - 3s/epoch - 19ms/step\n",
      "Epoch 14/30\n",
      "162/162 - 3s - loss: 0.0377 - mae: 0.0066 - 3s/epoch - 18ms/step\n",
      "Epoch 15/30\n",
      "162/162 - 3s - loss: 0.0349 - mae: 0.0082 - 3s/epoch - 17ms/step\n",
      "Epoch 16/30\n",
      "162/162 - 3s - loss: 0.0322 - mae: 0.0059 - 3s/epoch - 16ms/step\n",
      "Epoch 17/30\n",
      "162/162 - 3s - loss: 0.0298 - mae: 0.0054 - 3s/epoch - 16ms/step\n",
      "Epoch 18/30\n",
      "162/162 - 2s - loss: 0.0276 - mae: 0.0066 - 2s/epoch - 15ms/step\n",
      "Epoch 19/30\n",
      "162/162 - 2s - loss: 0.0255 - mae: 0.0069 - 2s/epoch - 15ms/step\n",
      "Epoch 20/30\n",
      "162/162 - 2s - loss: 0.0236 - mae: 0.0078 - 2s/epoch - 14ms/step\n",
      "Epoch 21/30\n",
      "162/162 - 2s - loss: 0.0218 - mae: 0.0064 - 2s/epoch - 15ms/step\n",
      "Epoch 22/30\n",
      "162/162 - 2s - loss: 0.0202 - mae: 0.0068 - 2s/epoch - 15ms/step\n",
      "Epoch 23/30\n",
      "162/162 - 2s - loss: 0.0186 - mae: 0.0063 - 2s/epoch - 14ms/step\n",
      "Epoch 24/30\n",
      "162/162 - 2s - loss: 0.0172 - mae: 0.0076 - 2s/epoch - 15ms/step\n",
      "Epoch 25/30\n",
      "162/162 - 3s - loss: 0.0159 - mae: 0.0061 - 3s/epoch - 17ms/step\n",
      "Epoch 26/30\n",
      "162/162 - 3s - loss: 0.0147 - mae: 0.0063 - 3s/epoch - 18ms/step\n",
      "Epoch 27/30\n",
      "162/162 - 3s - loss: 0.0136 - mae: 0.0071 - 3s/epoch - 19ms/step\n",
      "Epoch 28/30\n",
      "162/162 - 3s - loss: 0.0126 - mae: 0.0072 - 3s/epoch - 19ms/step\n",
      "Epoch 29/30\n",
      "162/162 - 3s - loss: 0.0116 - mae: 0.0064 - 3s/epoch - 17ms/step\n",
      "Epoch 30/30\n",
      "162/162 - 3s - loss: 0.0107 - mae: 0.0071 - 3s/epoch - 19ms/step\n",
      "Beste Parameter: {'fit__batch_size': 500, 'fit__epochs': 30, 'model__dropout_rate': 0.0, 'model__learning_rate': 0.001, 'model__regularization': 0.0001}\n",
      "Beste Genauigkeit: 0.9995720602540953\n"
     ]
    }
   ],
   "source": [
    "def build_model(learning_rate=0.001, activation='relu', regularization=0.0001, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(224, activation=activation, input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(32, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(96, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(224, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(128, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(352, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Verwenden Sie eine Funktion, um das Modell zu instanziieren, für scikit-learn Wrapper\n",
    "model = KerasRegressor(model=build_model, verbose=2)\n",
    "\n",
    "# Anpassung der Parameter im param_grid\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'model__regularization': [0.001, 0.0001],\n",
    "    'fit__batch_size': [50, 100, 200, 500],\n",
    "    'fit__epochs': [30],\n",
    "    'model__dropout_rate' : [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "# Hinweis: Stellen Sie sicher, dass Ihre Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "# Beste Parameter und Score ausgeben\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "print(\"Beste Genauigkeit:\", grid_search.best_score_)\n",
    "\n",
    "with open(\"Gridsearch_D1.txt\", \"w\") as f:\n",
    "    f.write(f\"Beste Parameter: {grid_search.best_params_}\\n\")\n",
    "    f.write(f\"Beste Genauigkeit: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bayesian Optimization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bb41910a42cbdee"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | learni... | neuron... | neuron... | neuron... | neuron... | neuron... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 17: early stopping\n",
      "| \u001B[0m1        \u001B[0m | \u001B[0m-0.000638\u001B[0m | \u001B[0m0.004229 \u001B[0m | \u001B[0m148.5    \u001B[0m | \u001B[0m16.02    \u001B[0m | \u001B[0m71.63    \u001B[0m | \u001B[0m43.0     \u001B[0m | \u001B[0m32.99    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 19: early stopping\n",
      "| \u001B[95m2        \u001B[0m | \u001B[95m-0.000562\u001B[0m | \u001B[95m0.001944 \u001B[0m | \u001B[95m79.58    \u001B[0m | \u001B[95m89.01    \u001B[0m | \u001B[95m115.1    \u001B[0m | \u001B[95m93.13    \u001B[0m | \u001B[95m142.1    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 18: early stopping\n",
      "| \u001B[0m3        \u001B[0m | \u001B[0m-0.000766\u001B[0m | \u001B[0m0.005658 \u001B[0m | \u001B[0m85.07    \u001B[0m | \u001B[0m104.1    \u001B[0m | \u001B[0m119.3    \u001B[0m | \u001B[0m41.35    \u001B[0m | \u001B[0m134.6    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 16: early stopping\n",
      "| \u001B[0m4        \u001B[0m | \u001B[0m-0.000834\u001B[0m | \u001B[0m0.008647 \u001B[0m | \u001B[0m85.11    \u001B[0m | \u001B[0m86.15    \u001B[0m | \u001B[0m110.5    \u001B[0m | \u001B[0m103.4    \u001B[0m | \u001B[0m144.2    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 20: early stopping\n",
      "| \u001B[0m5        \u001B[0m | \u001B[0m-0.000733\u001B[0m | \u001B[0m0.008982 \u001B[0m | \u001B[0m113.5    \u001B[0m | \u001B[0m26.86    \u001B[0m | \u001B[0m170.1    \u001B[0m | \u001B[0m57.75    \u001B[0m | \u001B[0m50.06    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 12: early stopping\n",
      "| \u001B[0m6        \u001B[0m | \u001B[0m-0.000743\u001B[0m | \u001B[0m0.004154 \u001B[0m | \u001B[0m38.5     \u001B[0m | \u001B[0m150.3    \u001B[0m | \u001B[0m175.7    \u001B[0m | \u001B[0m138.0    \u001B[0m | \u001B[0m88.68    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 16.\n",
      "Epoch 21: early stopping\n",
      "| \u001B[0m7        \u001B[0m | \u001B[0m-0.000796\u001B[0m | \u001B[0m0.009176 \u001B[0m | \u001B[0m33.41    \u001B[0m | \u001B[0m197.5    \u001B[0m | \u001B[0m190.2    \u001B[0m | \u001B[0m18.45    \u001B[0m | \u001B[0m99.55    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 17: early stopping\n",
      "| \u001B[0m8        \u001B[0m | \u001B[0m-0.000782\u001B[0m | \u001B[0m0.006971 \u001B[0m | \u001B[0m121.5    \u001B[0m | \u001B[0m76.55    \u001B[0m | \u001B[0m146.6    \u001B[0m | \u001B[0m162.7    \u001B[0m | \u001B[0m21.59    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Epoch 22: early stopping\n",
      "| \u001B[95m9        \u001B[0m | \u001B[95m-0.000546\u001B[0m | \u001B[95m0.002098 \u001B[0m | \u001B[95m70.04    \u001B[0m | \u001B[95m45.3     \u001B[0m | \u001B[95m67.57    \u001B[0m | \u001B[95m165.3    \u001B[0m | \u001B[95m146.0    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 34.\n",
      "Epoch 39: early stopping\n",
      "| \u001B[0m10       \u001B[0m | \u001B[0m-0.000793\u001B[0m | \u001B[0m0.0001457\u001B[0m | \u001B[0m167.1    \u001B[0m | \u001B[0m110.3    \u001B[0m | \u001B[0m114.7    \u001B[0m | \u001B[0m151.9    \u001B[0m | \u001B[0m189.9    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "Epoch 25: early stopping\n",
      "| \u001B[0m11       \u001B[0m | \u001B[0m-0.000610\u001B[0m | \u001B[0m0.0004375\u001B[0m | \u001B[0m148.2    \u001B[0m | \u001B[0m174.9    \u001B[0m | \u001B[0m71.28    \u001B[0m | \u001B[0m171.8    \u001B[0m | \u001B[0m93.63    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 17.\n",
      "Epoch 22: early stopping\n",
      "| \u001B[0m12       \u001B[0m | \u001B[0m-0.000757\u001B[0m | \u001B[0m0.008991 \u001B[0m | \u001B[0m31.9     \u001B[0m | \u001B[0m119.0    \u001B[0m | \u001B[0m62.88    \u001B[0m | \u001B[0m155.0    \u001B[0m | \u001B[0m59.75    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 15: early stopping\n",
      "| \u001B[0m13       \u001B[0m | \u001B[0m-0.000891\u001B[0m | \u001B[0m0.00989  \u001B[0m | \u001B[0m153.3    \u001B[0m | \u001B[0m75.35    \u001B[0m | \u001B[0m84.86    \u001B[0m | \u001B[0m76.01    \u001B[0m | \u001B[0m122.9    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 17: early stopping\n",
      "| \u001B[0m14       \u001B[0m | \u001B[0m-0.000719\u001B[0m | \u001B[0m0.007607 \u001B[0m | \u001B[0m53.49    \u001B[0m | \u001B[0m58.95    \u001B[0m | \u001B[0m123.7    \u001B[0m | \u001B[0m157.1    \u001B[0m | \u001B[0m193.7    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 19: early stopping\n",
      "| \u001B[0m15       \u001B[0m | \u001B[0m-0.000742\u001B[0m | \u001B[0m0.008752 \u001B[0m | \u001B[0m198.6    \u001B[0m | \u001B[0m87.49    \u001B[0m | \u001B[0m174.8    \u001B[0m | \u001B[0m161.6    \u001B[0m | \u001B[0m149.4    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "Epoch 30: early stopping\n",
      "| \u001B[0m16       \u001B[0m | \u001B[0m-0.000624\u001B[0m | \u001B[0m0.0002723\u001B[0m | \u001B[0m76.96    \u001B[0m | \u001B[0m101.7    \u001B[0m | \u001B[0m68.82    \u001B[0m | \u001B[0m133.2    \u001B[0m | \u001B[0m89.11    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "Epoch 24: early stopping\n",
      "| \u001B[0m17       \u001B[0m | \u001B[0m-0.000692\u001B[0m | \u001B[0m0.005645 \u001B[0m | \u001B[0m124.5    \u001B[0m | \u001B[0m149.4    \u001B[0m | \u001B[0m155.6    \u001B[0m | \u001B[0m96.35    \u001B[0m | \u001B[0m137.5    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 26.\n",
      "Epoch 31: early stopping\n",
      "| \u001B[95m18       \u001B[0m | \u001B[95m-0.000504\u001B[0m | \u001B[95m0.0008563\u001B[0m | \u001B[95m139.4    \u001B[0m | \u001B[95m143.1    \u001B[0m | \u001B[95m142.2    \u001B[0m | \u001B[95m67.5     \u001B[0m | \u001B[95m129.6    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 12: early stopping\n",
      "| \u001B[0m19       \u001B[0m | \u001B[0m-0.000737\u001B[0m | \u001B[0m0.008334 \u001B[0m | \u001B[0m17.35    \u001B[0m | \u001B[0m23.21    \u001B[0m | \u001B[0m164.6    \u001B[0m | \u001B[0m32.63    \u001B[0m | \u001B[0m155.2    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 19: early stopping\n",
      "| \u001B[0m20       \u001B[0m | \u001B[0m-0.000575\u001B[0m | \u001B[0m0.001016 \u001B[0m | \u001B[0m198.8    \u001B[0m | \u001B[0m101.1    \u001B[0m | \u001B[0m41.64    \u001B[0m | \u001B[0m70.41    \u001B[0m | \u001B[0m37.44    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 20: early stopping\n",
      "| \u001B[0m21       \u001B[0m | \u001B[0m-0.000635\u001B[0m | \u001B[0m0.001644 \u001B[0m | \u001B[0m28.21    \u001B[0m | \u001B[0m184.1    \u001B[0m | \u001B[0m34.07    \u001B[0m | \u001B[0m73.35    \u001B[0m | \u001B[0m133.9    \u001B[0m |\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "Epoch 19: early stopping\n",
      "| \u001B[0m22       \u001B[0m | \u001B[0m-0.000690\u001B[0m | \u001B[0m0.004116 \u001B[0m | \u001B[0m150.9    \u001B[0m | \u001B[0m173.6    \u001B[0m | \u001B[0m198.3    \u001B[0m | \u001B[0m173.2    \u001B[0m | \u001B[0m61.95    \u001B[0m |\n",
      "=================================================================================================\n",
      "{'target': -0.0005042816046625376, 'params': {'learning_rate': 0.0008562808887904138, 'neurons_layer_1': 139.37382512499065, 'neurons_layer_2': 143.13142125355472, 'neurons_layer_3': 142.17467953414652, 'neurons_layer_4': 67.50109625880788, 'neurons_layer_5': 129.5541186323257}}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "\n",
    "# Angenommene Daten\n",
    "# X_train_scaled, y_train_scaled = # Deine skalierten Trainingsdaten\n",
    "\n",
    "def train_evaluate(neurons_layer_1, neurons_layer_2, neurons_layer_3, neurons_layer_4, neurons_layer_5, learning_rate):\n",
    "    model = Sequential([\n",
    "        Dense(int(neurons_layer_1), activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "        Dense(int(neurons_layer_2), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "        Dense(int(neurons_layer_3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "        Dense(int(neurons_layer_4), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "        Dense(int(neurons_layer_5), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "        \n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min', restore_best_weights=True, min_delta=0.0001)\n",
    "\n",
    "    history = model.fit(X_train_scaled, y_train_scaled, batch_size=32, epochs=100, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Hier wählen wir den negativen Mean Squared Error, da Bayesian Optimization maximiert\n",
    "    mse = np.min(history.history['val_loss'])\n",
    "    return -mse\n",
    "\n",
    "# Definieren des Bereichs der Hyperparameter\n",
    "pbounds = {\n",
    "    'neurons_layer_1': (16, 200),\n",
    "    'neurons_layer_2': (16, 200),\n",
    "    'neurons_layer_3': (16, 200),\n",
    "    'neurons_layer_4': (16, 200),\n",
    "    'neurons_layer_5': (16, 200),\n",
    "    'learning_rate': (0.0001, 0.01),\n",
    "}\n",
    "\n",
    "# Initialisieren des BayesianOptimization-Objekts\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_evaluate,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "# Starten der Optimierung\n",
    "optimizer.maximize(init_points=2, n_iter=20)\n",
    "\n",
    "print(optimizer.max)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T15:22:26.910485500Z",
     "start_time": "2024-02-21T15:05:40.545629700Z"
    }
   },
   "id": "a5b6232547cdae67"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Search Architektur"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75773dfef8260e5f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 48s]\n",
      "val_loss: 1.355679523840081e-05\n",
      "\n",
      "Best val_loss So Far: 1.3108049188303994e-06\n",
      "Total elapsed time: 01h 41m 02s\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 224)               672       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                7200      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 96)                3168      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 224)               21728     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 128)               28800     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 352)               45408     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 353       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 107329 (419.25 KB)\n",
      "Trainable params: 107329 (419.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "{'input_units': 224, 'n_layers': 5, 'units_0': 32, 'units_1': 96, 'units_2': 224, 'units_3': 128, 'units_4': 352, 'units_5': 448, 'units_6': 64, 'units_7': 448, 'units_8': 480, 'units_9': 480}\n"
     ]
    }
   ],
   "source": [
    "# Definieren der Funktion, die das Modell erstellt\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp.Int('input_units', min_value=32, max_value=512, step=32), input_shape=(2,), activation='relu'))\n",
    "    # Hinzufügen von Schichten basierend auf dem Suchraum\n",
    "    for i in range(hp.Int('n_layers', 1, 10)):\n",
    "        model.add(Dense(hp.Int(f'units_{i}', min_value=32, max_value=512, step=32), activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Erstellen des RandomSearch Objekts\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=100,  # Anzahl der zu testenden Modellkonfigurationen\n",
    "    executions_per_trial=1,  # Anzahl der Male, die jede Modellkonfiguration trainiert wird\n",
    "    directory='random_search',  # Verzeichnis zur Speicherung der Suchlogs\n",
    "    project_name='neural_network_optimization'\n",
    ")\n",
    "\n",
    "# Durchführung des Random Search\n",
    "tuner.search(X_train_scaled, y_train_scaled, epochs=20, batch_size=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "# Abrufen des besten Modells\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Zusammenfassung des besten Modells\n",
    "best_model.summary()\n",
    "\n",
    "# Sie können auch die besten Hyperparameter direkt abrufen\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "print(best_hyperparameters.values) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bcbb2bbe83bfb6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a6fa208c5d8a01"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

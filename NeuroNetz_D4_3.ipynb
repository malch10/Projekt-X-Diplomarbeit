{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b0518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.582301400Z",
     "start_time": "2024-03-18T10:22:47.506988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ff61b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.600519600Z",
     "start_time": "2024-03-18T10:22:55.571301800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n0           0.0000      -0.00200        500   7000   9000      669.05\n1           0.0000      -0.00192        500   7000   9000      724.42\n2           0.0000      -0.00184        500   7000   9000      779.83\n3           0.0000      -0.00176        500   7000   9000      835.21\n4           0.0000      -0.00168        500   7000   9000      890.44\n...            ...           ...        ...    ...    ...         ...\n1066        0.0024       0.00168        500   7000   9000      775.40\n1067        0.0024       0.00176        500   7000   9000      715.43\n1068        0.0024       0.00184        500   7000   9000      645.85\n1069        0.0024       0.00192        500   7000   9000      585.87\n1070        0.0024       0.00200        500   7000   9000      574.64\n\n[1071 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00192</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>724.42</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00184</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>779.83</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00176</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>835.21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00168</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>890.44</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1066</th>\n      <td>0.0024</td>\n      <td>0.00168</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>775.40</td>\n    </tr>\n    <tr>\n      <th>1067</th>\n      <td>0.0024</td>\n      <td>0.00176</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>715.43</td>\n    </tr>\n    <tr>\n      <th>1068</th>\n      <td>0.0024</td>\n      <td>0.00184</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>645.85</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.0024</td>\n      <td>0.00192</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>585.87</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.0024</td>\n      <td>0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>574.64</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/Finish_D4_I7000_F9000/TPath_500_finish_data_D4.pkl')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966e3c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.601517800Z",
     "start_time": "2024-03-18T10:22:55.588516900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Temperatur\n0           0.0000      -0.00200      669.05\n1           0.0000      -0.00192      724.42\n2           0.0000      -0.00184      779.83\n3           0.0000      -0.00176      835.21\n4           0.0000      -0.00168      890.44\n...            ...           ...         ...\n1066        0.0024       0.00168      775.40\n1067        0.0024       0.00176      715.43\n1068        0.0024       0.00184      645.85\n1069        0.0024       0.00192      585.87\n1070        0.0024       0.00200      574.64\n\n[1071 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00192</td>\n      <td>724.42</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00184</td>\n      <td>779.83</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00176</td>\n      <td>835.21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00168</td>\n      <td>890.44</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1066</th>\n      <td>0.0024</td>\n      <td>0.00168</td>\n      <td>775.40</td>\n    </tr>\n    <tr>\n      <th>1067</th>\n      <td>0.0024</td>\n      <td>0.00176</td>\n      <td>715.43</td>\n    </tr>\n    <tr>\n      <th>1068</th>\n      <td>0.0024</td>\n      <td>0.00184</td>\n      <td>645.85</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.0024</td>\n      <td>0.00192</td>\n      <td>585.87</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.0024</td>\n      <td>0.00200</td>\n      <td>574.64</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop(data.columns[2:5], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8783d1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.647766400Z",
     "start_time": "2024-03-18T10:22:55.599517100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X-Koordinate  Y-Koordinate  Temperatur\n",
      "184        0.00036       0.00048     1441.10\n",
      "572        0.00132      -0.00112     1225.70\n",
      "309        0.00072      -0.00176      832.88\n",
      "930        0.00216      -0.00104     1158.00\n",
      "711        0.00156       0.00184      642.96\n",
      "...            ...           ...         ...\n",
      "330        0.00072      -0.00008     1500.20\n",
      "466        0.00108      -0.00144     1045.00\n",
      "121        0.00024      -0.00048     1484.90\n",
      "1044       0.00240      -0.00008     1259.10\n",
      "860        0.00192       0.00152      861.75\n",
      "\n",
      "[1071 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Temperatur\n0          0.00036       0.00048     1441.10\n1          0.00132      -0.00112     1225.70\n2          0.00072      -0.00176      832.88\n3          0.00216      -0.00104     1158.00\n4          0.00156       0.00184      642.96\n...            ...           ...         ...\n1066       0.00072      -0.00008     1500.20\n1067       0.00108      -0.00144     1045.00\n1068       0.00024      -0.00048     1484.90\n1069       0.00240      -0.00008     1259.10\n1070       0.00192       0.00152      861.75\n\n[1071 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00036</td>\n      <td>0.00048</td>\n      <td>1441.10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00132</td>\n      <td>-0.00112</td>\n      <td>1225.70</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00072</td>\n      <td>-0.00176</td>\n      <td>832.88</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00216</td>\n      <td>-0.00104</td>\n      <td>1158.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00156</td>\n      <td>0.00184</td>\n      <td>642.96</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1066</th>\n      <td>0.00072</td>\n      <td>-0.00008</td>\n      <td>1500.20</td>\n    </tr>\n    <tr>\n      <th>1067</th>\n      <td>0.00108</td>\n      <td>-0.00144</td>\n      <td>1045.00</td>\n    </tr>\n    <tr>\n      <th>1068</th>\n      <td>0.00024</td>\n      <td>-0.00048</td>\n      <td>1484.90</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.00240</td>\n      <td>-0.00008</td>\n      <td>1259.10</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.00192</td>\n      <td>0.00152</td>\n      <td>861.75</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "\n",
    "print(df1)\n",
    "df_reset = df1.reset_index(drop=True)\n",
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e72a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.674003500Z",
     "start_time": "2024-03-18T10:22:55.612736500Z"
    }
   },
   "outputs": [],
   "source": [
    "label = df_reset[\"Temperatur\"]\n",
    "df1 = df_reset.drop(\"Temperatur\", axis=1)\n",
    "X = df1\n",
    "y = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7fa289a50d87423"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e694a236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.675004800Z",
     "start_time": "2024-03-18T10:22:55.617737800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate\n0          0.00036       0.00048\n1          0.00132      -0.00112\n2          0.00072      -0.00176\n3          0.00216      -0.00104\n4          0.00156       0.00184\n...            ...           ...\n1066       0.00072      -0.00008\n1067       0.00108      -0.00144\n1068       0.00024      -0.00048\n1069       0.00240      -0.00008\n1070       0.00192       0.00152\n\n[1071 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00036</td>\n      <td>0.00048</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00132</td>\n      <td>-0.00112</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00072</td>\n      <td>-0.00176</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00216</td>\n      <td>-0.00104</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00156</td>\n      <td>0.00184</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1066</th>\n      <td>0.00072</td>\n      <td>-0.00008</td>\n    </tr>\n    <tr>\n      <th>1067</th>\n      <td>0.00108</td>\n      <td>-0.00144</td>\n    </tr>\n    <tr>\n      <th>1068</th>\n      <td>0.00024</td>\n      <td>-0.00048</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.00240</td>\n      <td>-0.00008</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.00192</td>\n      <td>0.00152</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3303b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.678004800Z",
     "start_time": "2024-03-18T10:22:55.627863600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0       1441.10\n1       1225.70\n2        832.88\n3       1158.00\n4        642.96\n         ...   \n1066    1500.20\n1067    1045.00\n1068    1484.90\n1069    1259.10\n1070     861.75\nName: Temperatur, Length: 1071, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ad8da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.678004800Z",
     "start_time": "2024-03-18T10:22:55.635889700Z"
    }
   },
   "outputs": [],
   "source": [
    " # train_df enthält 80% der Daten, test_df enthält 20% der Daten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c705edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.703005500Z",
     "start_time": "2024-03-18T10:22:55.642766100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "X_test_scaled = scaler_features.transform(X_test)  # Nutze unterschiedliche Skalierungsparameter\n",
    "\n",
    "# Initialisiere einen SEPARATEN MinMaxScaler für das Ziel, wenn nötig\n",
    "scaler_target = MinMaxScaler()\n",
    "# Skaliere y_train und y_test. Beachte, dass y_train.reshape(-1, 1) verwendet wird, da MinMaxScaler \n",
    "# erwartet, dass die Eingaben als 2D-Arrays kommen, und Ziele normalerweise als 1D-Arrays vorliegen.\n",
    "y_train_scaled = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbefe631e495b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.757004300Z",
     "start_time": "2024-03-18T10:22:55.651189800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.35, 0.88],\n       [0.75, 0.88],\n       [0.85, 0.02],\n       ...,\n       [0.2 , 0.86],\n       [1.  , 0.52],\n       [0.8 , 0.04]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_scaled.max()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T10:22:55.819003800Z",
     "start_time": "2024-03-18T10:22:55.656806300Z"
    }
   },
   "id": "ce04ce43aac2242f"
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "10/10 [==============================] - 2s 24ms/step - loss: 0.8117 - mae: 0.4965 - val_loss: 0.4782 - val_mae: 0.2814\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.4822 - mae: 0.2991 - val_loss: 0.4497 - val_mae: 0.2630\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.4387 - mae: 0.2755 - val_loss: 0.3860 - val_mae: 0.2096\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3988 - mae: 0.2419 - val_loss: 0.3673 - val_mae: 0.1922\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3595 - mae: 0.2174 - val_loss: 0.3484 - val_mae: 0.1797\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3371 - mae: 0.1910 - val_loss: 0.2940 - val_mae: 0.1245\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2925 - mae: 0.1217 - val_loss: 0.3115 - val_mae: 0.1742\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2840 - mae: 0.1170 - val_loss: 0.2778 - val_mae: 0.1356\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2607 - mae: 0.0814 - val_loss: 0.2518 - val_mae: 0.0663\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2455 - mae: 0.0444 - val_loss: 0.2446 - val_mae: 0.0617\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2389 - mae: 0.0415 - val_loss: 0.2345 - val_mae: 0.0331\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2330 - mae: 0.0384 - val_loss: 0.2332 - val_mae: 0.0581\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2279 - mae: 0.0363 - val_loss: 0.2260 - val_mae: 0.0423\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2227 - mae: 0.0293 - val_loss: 0.2212 - val_mae: 0.0354\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2185 - mae: 0.0270 - val_loss: 0.2212 - val_mae: 0.0635\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2163 - mae: 0.0383 - val_loss: 0.2128 - val_mae: 0.0255\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2115 - mae: 0.0241 - val_loss: 0.2096 - val_mae: 0.0237\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2082 - mae: 0.0198 - val_loss: 0.2116 - val_mae: 0.0616\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2072 - mae: 0.0343 - val_loss: 0.2045 - val_mae: 0.0262\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2036 - mae: 0.0255 - val_loss: 0.2020 - val_mae: 0.0228\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2006 - mae: 0.0159 - val_loss: 0.2010 - val_mae: 0.0346\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2005 - mae: 0.0334 - val_loss: 0.2154 - val_mae: 0.0976\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2071 - mae: 0.0700 - val_loss: 0.2054 - val_mae: 0.0787\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2025 - mae: 0.0651 - val_loss: 0.1981 - val_mae: 0.0494\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1966 - mae: 0.0464 - val_loss: 0.1960 - val_mae: 0.0537\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1932 - mae: 0.0360 - val_loss: 0.1912 - val_mae: 0.0287\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1907 - mae: 0.0289 - val_loss: 0.1974 - val_mae: 0.0773\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1920 - mae: 0.0466 - val_loss: 0.1878 - val_mae: 0.0240\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1890 - mae: 0.0382 - val_loss: 0.1868 - val_mae: 0.0292\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1858 - mae: 0.0224 - val_loss: 0.1845 - val_mae: 0.0169\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1840 - mae: 0.0164 - val_loss: 0.1834 - val_mae: 0.0208\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1825 - mae: 0.0133 - val_loss: 0.1822 - val_mae: 0.0225\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1813 - mae: 0.0136 - val_loss: 0.1805 - val_mae: 0.0144\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1799 - mae: 0.0108 - val_loss: 0.1795 - val_mae: 0.0179\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1787 - mae: 0.0120 - val_loss: 0.1781 - val_mae: 0.0152\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1775 - mae: 0.0121 - val_loss: 0.1768 - val_mae: 0.0124\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1763 - mae: 0.0107 - val_loss: 0.1757 - val_mae: 0.0107\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1751 - mae: 0.0088 - val_loss: 0.1745 - val_mae: 0.0095\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1740 - mae: 0.0085 - val_loss: 0.1741 - val_mae: 0.0218\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1733 - mae: 0.0153 - val_loss: 0.1723 - val_mae: 0.0080\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1718 - mae: 0.0076 - val_loss: 0.1717 - val_mae: 0.0173\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1709 - mae: 0.0101 - val_loss: 0.1702 - val_mae: 0.0077\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1697 - mae: 0.0057 - val_loss: 0.1692 - val_mae: 0.0085\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1687 - mae: 0.0067 - val_loss: 0.1681 - val_mae: 0.0063\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1677 - mae: 0.0059 - val_loss: 0.1671 - val_mae: 0.0082\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1667 - mae: 0.0065 - val_loss: 0.1662 - val_mae: 0.0076\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1658 - mae: 0.0070 - val_loss: 0.1654 - val_mae: 0.0151\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1648 - mae: 0.0086 - val_loss: 0.1645 - val_mae: 0.0150\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1641 - mae: 0.0126 - val_loss: 0.1641 - val_mae: 0.0188\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1645 - mae: 0.0274 - val_loss: 0.1779 - val_mae: 0.0945\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1721 - mae: 0.0669 - val_loss: 0.1685 - val_mae: 0.0658\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1657 - mae: 0.0510 - val_loss: 0.1623 - val_mae: 0.0349\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1620 - mae: 0.0330 - val_loss: 0.1652 - val_mae: 0.0582\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1619 - mae: 0.0366 - val_loss: 0.1605 - val_mae: 0.0357\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1595 - mae: 0.0260 - val_loss: 0.1584 - val_mae: 0.0174\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1579 - mae: 0.0147 - val_loss: 0.1572 - val_mae: 0.0104\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1569 - mae: 0.0111 - val_loss: 0.1565 - val_mae: 0.0144\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1561 - mae: 0.0128 - val_loss: 0.1556 - val_mae: 0.0120\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1551 - mae: 0.0083 - val_loss: 0.1545 - val_mae: 0.0059\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1542 - mae: 0.0061 - val_loss: 0.1537 - val_mae: 0.0050\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1534 - mae: 0.0056 - val_loss: 0.1529 - val_mae: 0.0059\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1526 - mae: 0.0059 - val_loss: 0.1521 - val_mae: 0.0070\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1517 - mae: 0.0052 - val_loss: 0.1513 - val_mae: 0.0058\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1509 - mae: 0.0058 - val_loss: 0.1505 - val_mae: 0.0093\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1502 - mae: 0.0075 - val_loss: 0.1498 - val_mae: 0.0106\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1494 - mae: 0.0090 - val_loss: 0.1492 - val_mae: 0.0135\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1487 - mae: 0.0092 - val_loss: 0.1482 - val_mae: 0.0104\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1479 - mae: 0.0103 - val_loss: 0.1474 - val_mae: 0.0083\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1471 - mae: 0.0071 - val_loss: 0.1468 - val_mae: 0.0127\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1464 - mae: 0.0104 - val_loss: 0.1459 - val_mae: 0.0099\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1457 - mae: 0.0119 - val_loss: 0.1457 - val_mae: 0.0192\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1452 - mae: 0.0153 - val_loss: 0.1444 - val_mae: 0.0101\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1442 - mae: 0.0119 - val_loss: 0.1437 - val_mae: 0.0093\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1434 - mae: 0.0086 - val_loss: 0.1431 - val_mae: 0.0126\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1426 - mae: 0.0092 - val_loss: 0.1423 - val_mae: 0.0103\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1421 - mae: 0.0141 - val_loss: 0.1427 - val_mae: 0.0326\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1419 - mae: 0.0235 - val_loss: 0.1415 - val_mae: 0.0235\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1408 - mae: 0.0176 - val_loss: 0.1404 - val_mae: 0.0169\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1400 - mae: 0.0143 - val_loss: 0.1406 - val_mae: 0.0304\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1395 - mae: 0.0188 - val_loss: 0.1395 - val_mae: 0.0259\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1389 - mae: 0.0201 - val_loss: 0.1409 - val_mae: 0.0466\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1392 - mae: 0.0323 - val_loss: 0.1452 - val_mae: 0.0822\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1398 - mae: 0.0414 - val_loss: 0.1374 - val_mae: 0.0252\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1376 - mae: 0.0287 - val_loss: 0.1392 - val_mae: 0.0456\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1374 - mae: 0.0333 - val_loss: 0.1355 - val_mae: 0.0122\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1353 - mae: 0.0170 - val_loss: 0.1358 - val_mae: 0.0307\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1348 - mae: 0.0197 - val_loss: 0.1363 - val_mae: 0.0335\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1356 - mae: 0.0313 - val_loss: 0.1356 - val_mae: 0.0364\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1342 - mae: 0.0280 - val_loss: 0.1355 - val_mae: 0.0431\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1351 - mae: 0.0403 - val_loss: 0.1328 - val_mae: 0.0255\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1343 - mae: 0.0361 - val_loss: 0.1391 - val_mae: 0.0716\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1340 - mae: 0.0400 - val_loss: 0.1324 - val_mae: 0.0286\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1320 - mae: 0.0281 - val_loss: 0.1304 - val_mae: 0.0155\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1304 - mae: 0.0187 - val_loss: 0.1295 - val_mae: 0.0115\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1294 - mae: 0.0145 - val_loss: 0.1288 - val_mae: 0.0099\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1287 - mae: 0.0136 - val_loss: 0.1282 - val_mae: 0.0079\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1280 - mae: 0.0105 - val_loss: 0.1281 - val_mae: 0.0186\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1275 - mae: 0.0124 - val_loss: 0.1273 - val_mae: 0.0173\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1268 - mae: 0.0101 - val_loss: 0.1263 - val_mae: 0.0074\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1261 - mae: 0.0067 - val_loss: 0.1258 - val_mae: 0.0093\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1255 - mae: 0.0082 - val_loss: 0.1251 - val_mae: 0.0073\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1248 - mae: 0.0063 - val_loss: 0.1245 - val_mae: 0.0055\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1243 - mae: 0.0065 - val_loss: 0.1240 - val_mae: 0.0091\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1237 - mae: 0.0062 - val_loss: 0.1233 - val_mae: 0.0071\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1231 - mae: 0.0057 - val_loss: 0.1227 - val_mae: 0.0071\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1225 - mae: 0.0053 - val_loss: 0.1221 - val_mae: 0.0066\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1219 - mae: 0.0066 - val_loss: 0.1217 - val_mae: 0.0121\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1213 - mae: 0.0066 - val_loss: 0.1210 - val_mae: 0.0062\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1207 - mae: 0.0051 - val_loss: 0.1204 - val_mae: 0.0067\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1201 - mae: 0.0046 - val_loss: 0.1199 - val_mae: 0.0086\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1196 - mae: 0.0058 - val_loss: 0.1192 - val_mae: 0.0051\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1190 - mae: 0.0050 - val_loss: 0.1187 - val_mae: 0.0061\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1184 - mae: 0.0053 - val_loss: 0.1181 - val_mae: 0.0055\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1179 - mae: 0.0058 - val_loss: 0.1176 - val_mae: 0.0057\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1173 - mae: 0.0046 - val_loss: 0.1170 - val_mae: 0.0063\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1168 - mae: 0.0052 - val_loss: 0.1165 - val_mae: 0.0071\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1163 - mae: 0.0072 - val_loss: 0.1159 - val_mae: 0.0075\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1157 - mae: 0.0070 - val_loss: 0.1154 - val_mae: 0.0087\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1152 - mae: 0.0070 - val_loss: 0.1152 - val_mae: 0.0161\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1148 - mae: 0.0116 - val_loss: 0.1144 - val_mae: 0.0111\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1143 - mae: 0.0121 - val_loss: 0.1144 - val_mae: 0.0211\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1137 - mae: 0.0094 - val_loss: 0.1133 - val_mae: 0.0092\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1131 - mae: 0.0083 - val_loss: 0.1128 - val_mae: 0.0095\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1125 - mae: 0.0082 - val_loss: 0.1124 - val_mae: 0.0110\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1122 - mae: 0.0122 - val_loss: 0.1124 - val_mae: 0.0227\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1118 - mae: 0.0152 - val_loss: 0.1114 - val_mae: 0.0158\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1110 - mae: 0.0084 - val_loss: 0.1107 - val_mae: 0.0079\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1104 - mae: 0.0071 - val_loss: 0.1101 - val_mae: 0.0084\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1099 - mae: 0.0068 - val_loss: 0.1097 - val_mae: 0.0131\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1094 - mae: 0.0070 - val_loss: 0.1091 - val_mae: 0.0078\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1089 - mae: 0.0066 - val_loss: 0.1085 - val_mae: 0.0052\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1084 - mae: 0.0065 - val_loss: 0.1082 - val_mae: 0.0115\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1079 - mae: 0.0097 - val_loss: 0.1083 - val_mae: 0.0210\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1079 - mae: 0.0203 - val_loss: 0.1090 - val_mae: 0.0339\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1082 - mae: 0.0275 - val_loss: 0.1126 - val_mae: 0.0618\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1091 - mae: 0.0379 - val_loss: 0.1090 - val_mae: 0.0469\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1070 - mae: 0.0251 - val_loss: 0.1063 - val_mae: 0.0215\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1058 - mae: 0.0173 - val_loss: 0.1054 - val_mae: 0.0161\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1053 - mae: 0.0154 - val_loss: 0.1051 - val_mae: 0.0172\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1048 - mae: 0.0153 - val_loss: 0.1046 - val_mae: 0.0169\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1046 - mae: 0.0193 - val_loss: 0.1039 - val_mae: 0.0123\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1039 - mae: 0.0153 - val_loss: 0.1034 - val_mae: 0.0133\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1032 - mae: 0.0133 - val_loss: 0.1028 - val_mae: 0.0083\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1028 - mae: 0.0138 - val_loss: 0.1022 - val_mae: 0.0059\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1021 - mae: 0.0089 - val_loss: 0.1019 - val_mae: 0.0108\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1016 - mae: 0.0078 - val_loss: 0.1013 - val_mae: 0.0074\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1011 - mae: 0.0068 - val_loss: 0.1010 - val_mae: 0.0127\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1007 - mae: 0.0084 - val_loss: 0.1005 - val_mae: 0.0103\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1004 - mae: 0.0118 - val_loss: 0.1000 - val_mae: 0.0112\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0998 - mae: 0.0076 - val_loss: 0.0995 - val_mae: 0.0091\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0994 - mae: 0.0088 - val_loss: 0.0992 - val_mae: 0.0138\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0990 - mae: 0.0104 - val_loss: 0.0989 - val_mae: 0.0145\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0985 - mae: 0.0097 - val_loss: 0.0982 - val_mae: 0.0118\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0980 - mae: 0.0094 - val_loss: 0.0978 - val_mae: 0.0101\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0976 - mae: 0.0110 - val_loss: 0.0976 - val_mae: 0.0153\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0972 - mae: 0.0123 - val_loss: 0.0969 - val_mae: 0.0101\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0967 - mae: 0.0106 - val_loss: 0.0967 - val_mae: 0.0177\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0963 - mae: 0.0101 - val_loss: 0.0964 - val_mae: 0.0173\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0962 - mae: 0.0169 - val_loss: 0.0976 - val_mae: 0.0327\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0970 - mae: 0.0319 - val_loss: 0.0952 - val_mae: 0.0119\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0954 - mae: 0.0175 - val_loss: 0.0953 - val_mae: 0.0210\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0947 - mae: 0.0139 - val_loss: 0.0952 - val_mae: 0.0236\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0942 - mae: 0.0135 - val_loss: 0.0943 - val_mae: 0.0206\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0937 - mae: 0.0119 - val_loss: 0.0934 - val_mae: 0.0117\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0932 - mae: 0.0094 - val_loss: 0.0934 - val_mae: 0.0185\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0929 - mae: 0.0126 - val_loss: 0.0926 - val_mae: 0.0103\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0924 - mae: 0.0108 - val_loss: 0.0923 - val_mae: 0.0153\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0920 - mae: 0.0097 - val_loss: 0.0919 - val_mae: 0.0149\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0917 - mae: 0.0128 - val_loss: 0.0929 - val_mae: 0.0367\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0917 - mae: 0.0204 - val_loss: 0.0914 - val_mae: 0.0183\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0943 - mae: 0.0431 - val_loss: 0.1162 - val_mae: 0.1222\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1020 - mae: 0.0767 - val_loss: 0.0971 - val_mae: 0.0651\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0956 - mae: 0.0542 - val_loss: 0.0951 - val_mae: 0.0584\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0931 - mae: 0.0496 - val_loss: 0.0921 - val_mae: 0.0493\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0905 - mae: 0.0306 - val_loss: 0.0894 - val_mae: 0.0170\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0892 - mae: 0.0154 - val_loss: 0.0887 - val_mae: 0.0128\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0885 - mae: 0.0125 - val_loss: 0.0882 - val_mae: 0.0107\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0880 - mae: 0.0092 - val_loss: 0.0877 - val_mae: 0.0069\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0876 - mae: 0.0069 - val_loss: 0.0874 - val_mae: 0.0090\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0872 - mae: 0.0079 - val_loss: 0.0870 - val_mae: 0.0083\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0868 - mae: 0.0083 - val_loss: 0.0866 - val_mae: 0.0106\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0864 - mae: 0.0086 - val_loss: 0.0862 - val_mae: 0.0097\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0860 - mae: 0.0094 - val_loss: 0.0859 - val_mae: 0.0103\n",
      "Epoch 184/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0857 - mae: 0.0101 - val_loss: 0.0856 - val_mae: 0.0148\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0853 - mae: 0.0095 - val_loss: 0.0852 - val_mae: 0.0135\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0850 - mae: 0.0118 - val_loss: 0.0846 - val_mae: 0.0086\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0845 - mae: 0.0084 - val_loss: 0.0843 - val_mae: 0.0102\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0841 - mae: 0.0090 - val_loss: 0.0838 - val_mae: 0.0056\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0837 - mae: 0.0088 - val_loss: 0.0834 - val_mae: 0.0061\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0833 - mae: 0.0055 - val_loss: 0.0830 - val_mae: 0.0060\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0829 - mae: 0.0059 - val_loss: 0.0827 - val_mae: 0.0050\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0825 - mae: 0.0050 - val_loss: 0.0823 - val_mae: 0.0054\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0821 - mae: 0.0059 - val_loss: 0.0819 - val_mae: 0.0076\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0818 - mae: 0.0066 - val_loss: 0.0817 - val_mae: 0.0096\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0815 - mae: 0.0090 - val_loss: 0.0814 - val_mae: 0.0104\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0811 - mae: 0.0085 - val_loss: 0.0808 - val_mae: 0.0057\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0807 - mae: 0.0078 - val_loss: 0.0806 - val_mae: 0.0103\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0803 - mae: 0.0057 - val_loss: 0.0801 - val_mae: 0.0039\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0799 - mae: 0.0041 - val_loss: 0.0797 - val_mae: 0.0036\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0796 - mae: 0.0047 - val_loss: 0.0793 - val_mae: 0.0039\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0792 - mae: 0.0046 - val_loss: 0.0790 - val_mae: 0.0059\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0789 - mae: 0.0051 - val_loss: 0.0788 - val_mae: 0.0097\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0786 - mae: 0.0076 - val_loss: 0.0784 - val_mae: 0.0100\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0782 - mae: 0.0072 - val_loss: 0.0780 - val_mae: 0.0094\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0778 - mae: 0.0078 - val_loss: 0.0777 - val_mae: 0.0120\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0776 - mae: 0.0109 - val_loss: 0.0775 - val_mae: 0.0166\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0773 - mae: 0.0124 - val_loss: 0.0773 - val_mae: 0.0177\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0769 - mae: 0.0112 - val_loss: 0.0766 - val_mae: 0.0075\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0765 - mae: 0.0076 - val_loss: 0.0762 - val_mae: 0.0072\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0761 - mae: 0.0063 - val_loss: 0.0761 - val_mae: 0.0127\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0758 - mae: 0.0100 - val_loss: 0.0756 - val_mae: 0.0102\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0754 - mae: 0.0073 - val_loss: 0.0752 - val_mae: 0.0073\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0750 - mae: 0.0052 - val_loss: 0.0749 - val_mae: 0.0083\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0747 - mae: 0.0066 - val_loss: 0.0745 - val_mae: 0.0042\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0743 - mae: 0.0051 - val_loss: 0.0741 - val_mae: 0.0050\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0740 - mae: 0.0057 - val_loss: 0.0740 - val_mae: 0.0129\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0737 - mae: 0.0083 - val_loss: 0.0736 - val_mae: 0.0106\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0735 - mae: 0.0113 - val_loss: 0.0732 - val_mae: 0.0070\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0731 - mae: 0.0079 - val_loss: 0.0732 - val_mae: 0.0167\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0728 - mae: 0.0086 - val_loss: 0.0726 - val_mae: 0.0081\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0724 - mae: 0.0062 - val_loss: 0.0722 - val_mae: 0.0068\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0720 - mae: 0.0056 - val_loss: 0.0719 - val_mae: 0.0077\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0717 - mae: 0.0065 - val_loss: 0.0715 - val_mae: 0.0057\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0714 - mae: 0.0047 - val_loss: 0.0712 - val_mae: 0.0037\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0710 - mae: 0.0042 - val_loss: 0.0708 - val_mae: 0.0036\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0707 - mae: 0.0042 - val_loss: 0.0706 - val_mae: 0.0078\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0704 - mae: 0.0064 - val_loss: 0.0702 - val_mae: 0.0063\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0701 - mae: 0.0048 - val_loss: 0.0699 - val_mae: 0.0080\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0698 - mae: 0.0065 - val_loss: 0.0697 - val_mae: 0.0106\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0695 - mae: 0.0070 - val_loss: 0.0692 - val_mae: 0.0039\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0691 - mae: 0.0051 - val_loss: 0.0690 - val_mae: 0.0088\n",
      "Epoch 232/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0690 - mae: 0.0108 - val_loss: 0.0692 - val_mae: 0.0209\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0689 - mae: 0.0186 - val_loss: 0.0694 - val_mae: 0.0252\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0685 - mae: 0.0151 - val_loss: 0.0684 - val_mae: 0.0170\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0682 - mae: 0.0135 - val_loss: 0.0683 - val_mae: 0.0209\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0678 - mae: 0.0144 - val_loss: 0.0676 - val_mae: 0.0125\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0673 - mae: 0.0090 - val_loss: 0.0671 - val_mae: 0.0071\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0671 - mae: 0.0098 - val_loss: 0.0671 - val_mae: 0.0179\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0670 - mae: 0.0158 - val_loss: 0.0678 - val_mae: 0.0293\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0671 - mae: 0.0213 - val_loss: 0.0665 - val_mae: 0.0161\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0668 - mae: 0.0199 - val_loss: 0.0665 - val_mae: 0.0206\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0664 - mae: 0.0211 - val_loss: 0.0664 - val_mae: 0.0271\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0659 - mae: 0.0170 - val_loss: 0.0657 - val_mae: 0.0160\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0654 - mae: 0.0134 - val_loss: 0.0651 - val_mae: 0.0107\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0650 - mae: 0.0101 - val_loss: 0.0647 - val_mae: 0.0105\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0646 - mae: 0.0082 - val_loss: 0.0644 - val_mae: 0.0063\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0643 - mae: 0.0059 - val_loss: 0.0641 - val_mae: 0.0061\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0639 - mae: 0.0053 - val_loss: 0.0639 - val_mae: 0.0100\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0637 - mae: 0.0059 - val_loss: 0.0635 - val_mae: 0.0053\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0633 - mae: 0.0044 - val_loss: 0.0632 - val_mae: 0.0050\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0630 - mae: 0.0038 - val_loss: 0.0629 - val_mae: 0.0040\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0627 - mae: 0.0037 - val_loss: 0.0626 - val_mae: 0.0031\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0625 - mae: 0.0048 - val_loss: 0.0623 - val_mae: 0.0081\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0622 - mae: 0.0060 - val_loss: 0.0620 - val_mae: 0.0045\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0619 - mae: 0.0044 - val_loss: 0.0617 - val_mae: 0.0053\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0616 - mae: 0.0053 - val_loss: 0.0614 - val_mae: 0.0048\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0613 - mae: 0.0045 - val_loss: 0.0612 - val_mae: 0.0056\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0611 - mae: 0.0073 - val_loss: 0.0610 - val_mae: 0.0106\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0608 - mae: 0.0097 - val_loss: 0.0610 - val_mae: 0.0189\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0606 - mae: 0.0108 - val_loss: 0.0608 - val_mae: 0.0187\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0605 - mae: 0.0159 - val_loss: 0.0601 - val_mae: 0.0073\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0601 - mae: 0.0107 - val_loss: 0.0616 - val_mae: 0.0324\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0604 - mae: 0.0220 - val_loss: 0.0610 - val_mae: 0.0314\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0605 - mae: 0.0259 - val_loss: 0.0638 - val_mae: 0.0603\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0611 - mae: 0.0354 - val_loss: 0.0599 - val_mae: 0.0250\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0593 - mae: 0.0177 - val_loss: 0.0596 - val_mae: 0.0233\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0594 - mae: 0.0223 - val_loss: 0.0601 - val_mae: 0.0331\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0591 - mae: 0.0227 - val_loss: 0.0590 - val_mae: 0.0232\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0585 - mae: 0.0174 - val_loss: 0.0586 - val_mae: 0.0221\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0581 - mae: 0.0135 - val_loss: 0.0585 - val_mae: 0.0281\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0581 - mae: 0.0192 - val_loss: 0.0592 - val_mae: 0.0367\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0588 - mae: 0.0304 - val_loss: 0.0593 - val_mae: 0.0420\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0577 - mae: 0.0210 - val_loss: 0.0570 - val_mae: 0.0124\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0569 - mae: 0.0121 - val_loss: 0.0573 - val_mae: 0.0224\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0567 - mae: 0.0133 - val_loss: 0.0565 - val_mae: 0.0135\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0563 - mae: 0.0106 - val_loss: 0.0562 - val_mae: 0.0106\n",
      "Epoch 277/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0560 - mae: 0.0099 - val_loss: 0.0559 - val_mae: 0.0117\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0560 - mae: 0.0155 - val_loss: 0.0558 - val_mae: 0.0146\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0555 - mae: 0.0102 - val_loss: 0.0553 - val_mae: 0.0067\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0552 - mae: 0.0065 - val_loss: 0.0551 - val_mae: 0.0085\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0549 - mae: 0.0062 - val_loss: 0.0547 - val_mae: 0.0057\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0546 - mae: 0.0061 - val_loss: 0.0545 - val_mae: 0.0091\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0544 - mae: 0.0078 - val_loss: 0.0542 - val_mae: 0.0053\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0541 - mae: 0.0054 - val_loss: 0.0539 - val_mae: 0.0035\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0538 - mae: 0.0042 - val_loss: 0.0537 - val_mae: 0.0064\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0536 - mae: 0.0052 - val_loss: 0.0535 - val_mae: 0.0075\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0533 - mae: 0.0049 - val_loss: 0.0532 - val_mae: 0.0043\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0531 - mae: 0.0036 - val_loss: 0.0530 - val_mae: 0.0062\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0529 - mae: 0.0061 - val_loss: 0.0528 - val_mae: 0.0100\n",
      "Epoch 290/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0526 - mae: 0.0060 - val_loss: 0.0524 - val_mae: 0.0047\n",
      "Epoch 291/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0523 - mae: 0.0038 - val_loss: 0.0524 - val_mae: 0.0118\n",
      "Epoch 292/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0521 - mae: 0.0073 - val_loss: 0.0520 - val_mae: 0.0048\n",
      "Epoch 293/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0519 - mae: 0.0061 - val_loss: 0.0517 - val_mae: 0.0057\n",
      "Epoch 294/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0516 - mae: 0.0063 - val_loss: 0.0515 - val_mae: 0.0075\n",
      "Epoch 295/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0514 - mae: 0.0068 - val_loss: 0.0516 - val_mae: 0.0161\n",
      "Epoch 296/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0516 - mae: 0.0190 - val_loss: 0.0521 - val_mae: 0.0271\n",
      "Epoch 297/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0515 - mae: 0.0187 - val_loss: 0.0510 - val_mae: 0.0139\n",
      "Epoch 298/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0508 - mae: 0.0106 - val_loss: 0.0510 - val_mae: 0.0199\n",
      "Epoch 299/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0506 - mae: 0.0126 - val_loss: 0.0505 - val_mae: 0.0149\n",
      "Epoch 300/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0503 - mae: 0.0108 - val_loss: 0.0505 - val_mae: 0.0165\n",
      "Epoch 301/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0508 - mae: 0.0230 - val_loss: 0.0517 - val_mae: 0.0379\n",
      "Epoch 302/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0507 - mae: 0.0238 - val_loss: 0.0499 - val_mae: 0.0144\n",
      "Epoch 303/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0500 - mae: 0.0201 - val_loss: 0.0496 - val_mae: 0.0157\n",
      "Epoch 304/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0504 - mae: 0.0256 - val_loss: 0.0525 - val_mae: 0.0530\n",
      "Epoch 305/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0516 - mae: 0.0407 - val_loss: 0.0522 - val_mae: 0.0528\n",
      "Epoch 306/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0508 - mae: 0.0364 - val_loss: 0.0496 - val_mae: 0.0245\n",
      "Epoch 307/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0493 - mae: 0.0224 - val_loss: 0.0490 - val_mae: 0.0184\n",
      "Epoch 308/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0488 - mae: 0.0162 - val_loss: 0.0483 - val_mae: 0.0093\n",
      "Epoch 309/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0482 - mae: 0.0093 - val_loss: 0.0481 - val_mae: 0.0107\n",
      "Epoch 310/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0480 - mae: 0.0092 - val_loss: 0.0488 - val_mae: 0.0246\n",
      "Epoch 311/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0483 - mae: 0.0200 - val_loss: 0.0481 - val_mae: 0.0201\n",
      "Epoch 312/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0478 - mae: 0.0150 - val_loss: 0.0475 - val_mae: 0.0134\n",
      "Epoch 313/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0473 - mae: 0.0094 - val_loss: 0.0471 - val_mae: 0.0079\n",
      "Epoch 314/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0470 - mae: 0.0074 - val_loss: 0.0469 - val_mae: 0.0059\n",
      "Epoch 315/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0468 - mae: 0.0056 - val_loss: 0.0466 - val_mae: 0.0045\n",
      "Epoch 316/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0465 - mae: 0.0041 - val_loss: 0.0465 - val_mae: 0.0100\n",
      "Epoch 317/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0464 - mae: 0.0085 - val_loss: 0.0462 - val_mae: 0.0056\n",
      "Epoch 318/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0461 - mae: 0.0071 - val_loss: 0.0459 - val_mae: 0.0039\n",
      "Epoch 319/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0459 - mae: 0.0054 - val_loss: 0.0458 - val_mae: 0.0090\n",
      "Epoch 320/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0457 - mae: 0.0067 - val_loss: 0.0455 - val_mae: 0.0058\n",
      "Epoch 321/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0454 - mae: 0.0050 - val_loss: 0.0453 - val_mae: 0.0048\n",
      "Epoch 322/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0452 - mae: 0.0047 - val_loss: 0.0451 - val_mae: 0.0041\n",
      "Epoch 323/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0450 - mae: 0.0048 - val_loss: 0.0448 - val_mae: 0.0032\n",
      "Epoch 324/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0448 - mae: 0.0035 - val_loss: 0.0446 - val_mae: 0.0058\n",
      "Epoch 325/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0445 - mae: 0.0047 - val_loss: 0.0444 - val_mae: 0.0050\n",
      "Epoch 326/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0443 - mae: 0.0048 - val_loss: 0.0442 - val_mae: 0.0035\n",
      "Epoch 327/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0441 - mae: 0.0044 - val_loss: 0.0440 - val_mae: 0.0053\n",
      "Epoch 328/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0439 - mae: 0.0048 - val_loss: 0.0438 - val_mae: 0.0071\n",
      "Epoch 329/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0437 - mae: 0.0050 - val_loss: 0.0436 - val_mae: 0.0065\n",
      "Epoch 330/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0435 - mae: 0.0048 - val_loss: 0.0434 - val_mae: 0.0057\n",
      "Epoch 331/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0433 - mae: 0.0074 - val_loss: 0.0432 - val_mae: 0.0073\n",
      "Epoch 332/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0432 - mae: 0.0094 - val_loss: 0.0433 - val_mae: 0.0154\n",
      "Epoch 333/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0430 - mae: 0.0118 - val_loss: 0.0428 - val_mae: 0.0083\n",
      "Epoch 334/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0428 - mae: 0.0117 - val_loss: 0.0426 - val_mae: 0.0079\n",
      "Epoch 335/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0425 - mae: 0.0079 - val_loss: 0.0426 - val_mae: 0.0156\n",
      "Epoch 336/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0424 - mae: 0.0116 - val_loss: 0.0422 - val_mae: 0.0081\n",
      "Epoch 337/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0421 - mae: 0.0097 - val_loss: 0.0420 - val_mae: 0.0095\n",
      "Epoch 338/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0419 - mae: 0.0093 - val_loss: 0.0419 - val_mae: 0.0118\n",
      "Epoch 339/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0417 - mae: 0.0070 - val_loss: 0.0415 - val_mae: 0.0049\n",
      "Epoch 340/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0415 - mae: 0.0063 - val_loss: 0.0414 - val_mae: 0.0104\n",
      "Epoch 341/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0413 - mae: 0.0098 - val_loss: 0.0426 - val_mae: 0.0346\n",
      "Epoch 342/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0417 - mae: 0.0218 - val_loss: 0.0419 - val_mae: 0.0293\n",
      "Epoch 343/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0414 - mae: 0.0185 - val_loss: 0.0421 - val_mae: 0.0331\n",
      "Epoch 344/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0410 - mae: 0.0150 - val_loss: 0.0406 - val_mae: 0.0085\n",
      "Epoch 345/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0405 - mae: 0.0077 - val_loss: 0.0406 - val_mae: 0.0151\n",
      "Epoch 346/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0403 - mae: 0.0093 - val_loss: 0.0405 - val_mae: 0.0161\n",
      "Epoch 347/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0401 - mae: 0.0093 - val_loss: 0.0402 - val_mae: 0.0133\n",
      "Epoch 348/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0401 - mae: 0.0142 - val_loss: 0.0401 - val_mae: 0.0162\n",
      "Epoch 349/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0402 - mae: 0.0196 - val_loss: 0.0408 - val_mae: 0.0270\n",
      "Epoch 350/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0400 - mae: 0.0190 - val_loss: 0.0395 - val_mae: 0.0124\n",
      "Epoch 351/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0395 - mae: 0.0128 - val_loss: 0.0398 - val_mae: 0.0203\n",
      "Epoch 352/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0396 - mae: 0.0177 - val_loss: 0.0395 - val_mae: 0.0210\n",
      "Epoch 353/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0391 - mae: 0.0124 - val_loss: 0.0389 - val_mae: 0.0104\n",
      "Epoch 354/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0388 - mae: 0.0101 - val_loss: 0.0386 - val_mae: 0.0074\n",
      "Epoch 355/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0386 - mae: 0.0099 - val_loss: 0.0384 - val_mae: 0.0071\n",
      "Epoch 356/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0384 - mae: 0.0092 - val_loss: 0.0382 - val_mae: 0.0064\n",
      "Epoch 357/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0382 - mae: 0.0077 - val_loss: 0.0381 - val_mae: 0.0081\n",
      "Epoch 358/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0380 - mae: 0.0073 - val_loss: 0.0378 - val_mae: 0.0055\n",
      "Epoch 359/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0378 - mae: 0.0056 - val_loss: 0.0378 - val_mae: 0.0102\n",
      "Epoch 360/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0376 - mae: 0.0069 - val_loss: 0.0375 - val_mae: 0.0070\n",
      "Epoch 361/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0374 - mae: 0.0074 - val_loss: 0.0373 - val_mae: 0.0067\n",
      "Epoch 362/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0372 - mae: 0.0081 - val_loss: 0.0372 - val_mae: 0.0082\n",
      "Epoch 363/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0371 - mae: 0.0083 - val_loss: 0.0372 - val_mae: 0.0175\n",
      "Epoch 364/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0369 - mae: 0.0091 - val_loss: 0.0371 - val_mae: 0.0180\n",
      "Epoch 365/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0367 - mae: 0.0092 - val_loss: 0.0367 - val_mae: 0.0153\n",
      "Epoch 366/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0365 - mae: 0.0086 - val_loss: 0.0364 - val_mae: 0.0067\n",
      "Epoch 367/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0363 - mae: 0.0065 - val_loss: 0.0361 - val_mae: 0.0037\n",
      "Epoch 368/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0361 - mae: 0.0058 - val_loss: 0.0362 - val_mae: 0.0120\n",
      "Epoch 369/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0360 - mae: 0.0082 - val_loss: 0.0358 - val_mae: 0.0074\n",
      "Epoch 370/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0357 - mae: 0.0050 - val_loss: 0.0356 - val_mae: 0.0042\n",
      "Epoch 371/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0355 - mae: 0.0042 - val_loss: 0.0354 - val_mae: 0.0050\n",
      "Epoch 372/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0354 - mae: 0.0050 - val_loss: 0.0353 - val_mae: 0.0080\n",
      "Epoch 373/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0353 - mae: 0.0093 - val_loss: 0.0353 - val_mae: 0.0127\n",
      "Epoch 374/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0351 - mae: 0.0091 - val_loss: 0.0350 - val_mae: 0.0112\n",
      "Epoch 375/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0349 - mae: 0.0079 - val_loss: 0.0348 - val_mae: 0.0073\n",
      "Epoch 376/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0347 - mae: 0.0066 - val_loss: 0.0346 - val_mae: 0.0081\n",
      "Epoch 377/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0345 - mae: 0.0054 - val_loss: 0.0344 - val_mae: 0.0056\n",
      "Epoch 378/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0344 - mae: 0.0067 - val_loss: 0.0342 - val_mae: 0.0068\n",
      "Epoch 379/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0342 - mae: 0.0080 - val_loss: 0.0340 - val_mae: 0.0053\n",
      "Epoch 380/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0340 - mae: 0.0067 - val_loss: 0.0340 - val_mae: 0.0099\n",
      "Epoch 381/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0339 - mae: 0.0089 - val_loss: 0.0339 - val_mae: 0.0127\n",
      "Epoch 382/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0337 - mae: 0.0088 - val_loss: 0.0337 - val_mae: 0.0124\n",
      "Epoch 383/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0337 - mae: 0.0125 - val_loss: 0.0335 - val_mae: 0.0115\n",
      "Epoch 384/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0334 - mae: 0.0084 - val_loss: 0.0332 - val_mae: 0.0059\n",
      "Epoch 385/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0331 - mae: 0.0061 - val_loss: 0.0332 - val_mae: 0.0112\n",
      "Epoch 386/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0331 - mae: 0.0096 - val_loss: 0.0330 - val_mae: 0.0107\n",
      "Epoch 387/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0328 - mae: 0.0086 - val_loss: 0.0332 - val_mae: 0.0174\n",
      "Epoch 388/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0329 - mae: 0.0146 - val_loss: 0.0325 - val_mae: 0.0056\n",
      "Epoch 389/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0327 - mae: 0.0125 - val_loss: 0.0325 - val_mae: 0.0137\n",
      "Epoch 390/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0329 - mae: 0.0206 - val_loss: 0.0398 - val_mae: 0.0649\n",
      "Epoch 391/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0345 - mae: 0.0353 - val_loss: 0.0331 - val_mae: 0.0244\n",
      "Epoch 392/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0328 - mae: 0.0234 - val_loss: 0.0362 - val_mae: 0.0466\n",
      "Epoch 393/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0338 - mae: 0.0333 - val_loss: 0.0323 - val_mae: 0.0225\n",
      "Epoch 394/1000\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.0324 - mae: 0.0236Restoring model weights from the end of the best epoch: 389.\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0330 - mae: 0.0271 - val_loss: 0.0329 - val_mae: 0.0332\n",
      "Epoch 394: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "\n",
    "    Dense(136, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(168, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(24, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 75\n",
    "epochs = 1000\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T11:10:50.857659200Z",
     "start_time": "2024-03-15T11:10:25.186779200Z"
    }
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Training für Fold 1...\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "10/10 [==============================] - 1s 26ms/step - loss: 0.5147 - mae: 0.5375 - val_loss: 0.2040 - val_mae: 0.3003\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2674 - mae: 0.3787 - val_loss: 0.2109 - val_mae: 0.3127\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2154 - mae: 0.3267 - val_loss: 0.1736 - val_mae: 0.2857\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1902 - mae: 0.3019 - val_loss: 0.1559 - val_mae: 0.2394\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1744 - mae: 0.2789 - val_loss: 0.1443 - val_mae: 0.2249\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1642 - mae: 0.2657 - val_loss: 0.1402 - val_mae: 0.2262\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1621 - mae: 0.2670 - val_loss: 0.1334 - val_mae: 0.2177\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1501 - mae: 0.2528 - val_loss: 0.1257 - val_mae: 0.2107\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1400 - mae: 0.2386 - val_loss: 0.1256 - val_mae: 0.2184\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1411 - mae: 0.2487 - val_loss: 0.1107 - val_mae: 0.1932\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1284 - mae: 0.2281 - val_loss: 0.1147 - val_mae: 0.2084\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1153 - mae: 0.2104 - val_loss: 0.0995 - val_mae: 0.1796\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1076 - mae: 0.1998 - val_loss: 0.0956 - val_mae: 0.1784\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0997 - mae: 0.1894 - val_loss: 0.0894 - val_mae: 0.1655\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0922 - mae: 0.1759 - val_loss: 0.0829 - val_mae: 0.1564\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0863 - mae: 0.1694 - val_loss: 0.0778 - val_mae: 0.1457\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0800 - mae: 0.1553 - val_loss: 0.0747 - val_mae: 0.1425\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0750 - mae: 0.1432 - val_loss: 0.0699 - val_mae: 0.1301\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0727 - mae: 0.1384 - val_loss: 0.0691 - val_mae: 0.1337\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0669 - mae: 0.1251 - val_loss: 0.0643 - val_mae: 0.1171\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0622 - mae: 0.1111 - val_loss: 0.0595 - val_mae: 0.1059\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0585 - mae: 0.1009 - val_loss: 0.0569 - val_mae: 0.0943\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0569 - mae: 0.0940 - val_loss: 0.0538 - val_mae: 0.0808\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0529 - mae: 0.0789 - val_loss: 0.0516 - val_mae: 0.0780\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0512 - mae: 0.0707 - val_loss: 0.0510 - val_mae: 0.0717\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0503 - mae: 0.0714 - val_loss: 0.0488 - val_mae: 0.0555\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0492 - mae: 0.0643 - val_loss: 0.0475 - val_mae: 0.0517\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0470 - mae: 0.0598 - val_loss: 0.0458 - val_mae: 0.0524\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0453 - mae: 0.0503 - val_loss: 0.0447 - val_mae: 0.0447\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0449 - mae: 0.0467 - val_loss: 0.0437 - val_mae: 0.0377\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0436 - mae: 0.0389 - val_loss: 0.0437 - val_mae: 0.0408\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0432 - mae: 0.0370 - val_loss: 0.0435 - val_mae: 0.0413\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0430 - mae: 0.0379 - val_loss: 0.0440 - val_mae: 0.0428\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0440 - mae: 0.0468 - val_loss: 0.0463 - val_mae: 0.0588\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0436 - mae: 0.0432 - val_loss: 0.0427 - val_mae: 0.0396\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0414 - mae: 0.0286 - val_loss: 0.0414 - val_mae: 0.0275\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0412 - mae: 0.0285 - val_loss: 0.0416 - val_mae: 0.0308\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0413 - mae: 0.0297 - val_loss: 0.0425 - val_mae: 0.0382\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0411 - mae: 0.0291 - val_loss: 0.0411 - val_mae: 0.0322\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0404 - mae: 0.0242 - val_loss: 0.0408 - val_mae: 0.0289\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0403 - mae: 0.0241 - val_loss: 0.0403 - val_mae: 0.0236\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0401 - mae: 0.0229 - val_loss: 0.0404 - val_mae: 0.0278\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0402 - mae: 0.0261 - val_loss: 0.0397 - val_mae: 0.0229\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0395 - mae: 0.0199 - val_loss: 0.0396 - val_mae: 0.0228\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0393 - mae: 0.0198 - val_loss: 0.0394 - val_mae: 0.0222\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0391 - mae: 0.0184 - val_loss: 0.0391 - val_mae: 0.0192\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0389 - mae: 0.0171 - val_loss: 0.0391 - val_mae: 0.0213\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0387 - mae: 0.0168 - val_loss: 0.0387 - val_mae: 0.0181\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0385 - mae: 0.0153 - val_loss: 0.0385 - val_mae: 0.0161\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0384 - mae: 0.0155 - val_loss: 0.0392 - val_mae: 0.0278\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0388 - mae: 0.0229 - val_loss: 0.0383 - val_mae: 0.0176\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0384 - mae: 0.0202 - val_loss: 0.0384 - val_mae: 0.0207\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0381 - mae: 0.0172 - val_loss: 0.0379 - val_mae: 0.0140\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0379 - mae: 0.0160 - val_loss: 0.0383 - val_mae: 0.0248\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0379 - mae: 0.0194 - val_loss: 0.0376 - val_mae: 0.0145\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0376 - mae: 0.0150 - val_loss: 0.0374 - val_mae: 0.0142\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0373 - mae: 0.0128 - val_loss: 0.0373 - val_mae: 0.0139\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0373 - mae: 0.0145 - val_loss: 0.0371 - val_mae: 0.0127\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0371 - mae: 0.0126 - val_loss: 0.0370 - val_mae: 0.0123\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0370 - mae: 0.0133 - val_loss: 0.0368 - val_mae: 0.0117\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0368 - mae: 0.0116 - val_loss: 0.0367 - val_mae: 0.0120\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0367 - mae: 0.0125 - val_loss: 0.0366 - val_mae: 0.0121\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0365 - mae: 0.0118 - val_loss: 0.0364 - val_mae: 0.0113\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0364 - mae: 0.0112 - val_loss: 0.0363 - val_mae: 0.0111\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0362 - mae: 0.0107 - val_loss: 0.0362 - val_mae: 0.0110\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0362 - mae: 0.0118 - val_loss: 0.0361 - val_mae: 0.0112\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0360 - mae: 0.0114 - val_loss: 0.0361 - val_mae: 0.0163\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0360 - mae: 0.0140 - val_loss: 0.0359 - val_mae: 0.0142\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0359 - mae: 0.0132 - val_loss: 0.0357 - val_mae: 0.0114\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0356 - mae: 0.0111 - val_loss: 0.0358 - val_mae: 0.0168\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0356 - mae: 0.0136 - val_loss: 0.0355 - val_mae: 0.0126\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0355 - mae: 0.0133 - val_loss: 0.0355 - val_mae: 0.0161\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0354 - mae: 0.0123 - val_loss: 0.0352 - val_mae: 0.0103\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0351 - mae: 0.0095 - val_loss: 0.0351 - val_mae: 0.0097\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0351 - mae: 0.0103 - val_loss: 0.0350 - val_mae: 0.0099\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0350 - mae: 0.0116 - val_loss: 0.0349 - val_mae: 0.0102\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0349 - mae: 0.0130 - val_loss: 0.0347 - val_mae: 0.0100\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0348 - mae: 0.0127 - val_loss: 0.0347 - val_mae: 0.0125\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0347 - mae: 0.0135 - val_loss: 0.0348 - val_mae: 0.0166\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0347 - mae: 0.0152 - val_loss: 0.0346 - val_mae: 0.0147\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0344 - mae: 0.0113 - val_loss: 0.0344 - val_mae: 0.0124\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0343 - mae: 0.0117 - val_loss: 0.0343 - val_mae: 0.0124\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0342 - mae: 0.0114 - val_loss: 0.0341 - val_mae: 0.0115\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0341 - mae: 0.0107 - val_loss: 0.0340 - val_mae: 0.0106\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0339 - mae: 0.0096 - val_loss: 0.0339 - val_mae: 0.0112\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0339 - mae: 0.0103 - val_loss: 0.0341 - val_mae: 0.0184\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0339 - mae: 0.0131 - val_loss: 0.0337 - val_mae: 0.0103\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0337 - mae: 0.0113 - val_loss: 0.0336 - val_mae: 0.0093\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0335 - mae: 0.0099 - val_loss: 0.0335 - val_mae: 0.0102\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0334 - mae: 0.0096 - val_loss: 0.0335 - val_mae: 0.0135\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0335 - mae: 0.0133 - val_loss: 0.0335 - val_mae: 0.0163\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0333 - mae: 0.0123 - val_loss: 0.0333 - val_mae: 0.0146\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0332 - mae: 0.0118 - val_loss: 0.0331 - val_mae: 0.0118\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0331 - mae: 0.0115 - val_loss: 0.0330 - val_mae: 0.0117\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0330 - mae: 0.0105 - val_loss: 0.0329 - val_mae: 0.0118\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0328 - mae: 0.0099 - val_loss: 0.0328 - val_mae: 0.0113\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0327 - mae: 0.0095 - val_loss: 0.0326 - val_mae: 0.0096\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0326 - mae: 0.0092 - val_loss: 0.0325 - val_mae: 0.0091\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0325 - mae: 0.0090 - val_loss: 0.0328 - val_mae: 0.0171\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0326 - mae: 0.0130 - val_loss: 0.0329 - val_mae: 0.0203\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0325 - mae: 0.0143 - val_loss: 0.0328 - val_mae: 0.0214\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0325 - mae: 0.0155 - val_loss: 0.0323 - val_mae: 0.0147\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0322 - mae: 0.0113 - val_loss: 0.0321 - val_mae: 0.0098\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0320 - mae: 0.0093 - val_loss: 0.0320 - val_mae: 0.0093\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0319 - mae: 0.0091 - val_loss: 0.0319 - val_mae: 0.0094\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0319 - mae: 0.0101 - val_loss: 0.0318 - val_mae: 0.0101\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0318 - mae: 0.0088 - val_loss: 0.0318 - val_mae: 0.0111\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0317 - mae: 0.0088 - val_loss: 0.0316 - val_mae: 0.0083\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0316 - mae: 0.0089 - val_loss: 0.0315 - val_mae: 0.0084\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0315 - mae: 0.0089 - val_loss: 0.0314 - val_mae: 0.0089\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0314 - mae: 0.0092 - val_loss: 0.0313 - val_mae: 0.0088\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0313 - mae: 0.0099 - val_loss: 0.0314 - val_mae: 0.0131\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0313 - mae: 0.0114 - val_loss: 0.0312 - val_mae: 0.0096\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0311 - mae: 0.0092 - val_loss: 0.0311 - val_mae: 0.0113\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0310 - mae: 0.0088 - val_loss: 0.0310 - val_mae: 0.0086\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0310 - mae: 0.0088 - val_loss: 0.0309 - val_mae: 0.0093\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0309 - mae: 0.0093 - val_loss: 0.0310 - val_mae: 0.0140\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0308 - mae: 0.0103 - val_loss: 0.0308 - val_mae: 0.0102\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0307 - mae: 0.0094 - val_loss: 0.0306 - val_mae: 0.0091\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0306 - mae: 0.0093 - val_loss: 0.0305 - val_mae: 0.0087\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0305 - mae: 0.0083 - val_loss: 0.0305 - val_mae: 0.0103\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0304 - mae: 0.0091 - val_loss: 0.0306 - val_mae: 0.0144\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0304 - mae: 0.0117 - val_loss: 0.0303 - val_mae: 0.0086\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0303 - mae: 0.0096 - val_loss: 0.0302 - val_mae: 0.0098\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0302 - mae: 0.0086 - val_loss: 0.0302 - val_mae: 0.0100\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0301 - mae: 0.0094 - val_loss: 0.0301 - val_mae: 0.0122\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0301 - mae: 0.0104 - val_loss: 0.0301 - val_mae: 0.0127\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0300 - mae: 0.0107 - val_loss: 0.0299 - val_mae: 0.0085\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0315 - mae: 0.0289 - val_loss: 0.0303 - val_mae: 0.0206\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0312 - mae: 0.0314 - val_loss: 0.0327 - val_mae: 0.0461\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0308 - mae: 0.0273 - val_loss: 0.0300 - val_mae: 0.0153\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0302 - mae: 0.0203 - val_loss: 0.0303 - val_mae: 0.0230\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0298 - mae: 0.0158 - val_loss: 0.0296 - val_mae: 0.0112\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0295 - mae: 0.0110 - val_loss: 0.0295 - val_mae: 0.0109\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0294 - mae: 0.0104 - val_loss: 0.0298 - val_mae: 0.0196\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0295 - mae: 0.0135 - val_loss: 0.0293 - val_mae: 0.0108\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0293 - mae: 0.0107 - val_loss: 0.0297 - val_mae: 0.0188\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0295 - mae: 0.0175 - val_loss: 0.0307 - val_mae: 0.0341\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0301 - mae: 0.0262 - val_loss: 0.0305 - val_mae: 0.0354\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0297 - mae: 0.0233 - val_loss: 0.0290 - val_mae: 0.0105\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0293 - mae: 0.0184 - val_loss: 0.0296 - val_mae: 0.0226\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0291 - mae: 0.0143 - val_loss: 0.0288 - val_mae: 0.0104\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0288 - mae: 0.0093 - val_loss: 0.0288 - val_mae: 0.0125\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0288 - mae: 0.0113 - val_loss: 0.0287 - val_mae: 0.0088\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0287 - mae: 0.0111 - val_loss: 0.0288 - val_mae: 0.0157\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0287 - mae: 0.0128 - val_loss: 0.0285 - val_mae: 0.0089\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0285 - mae: 0.0088 - val_loss: 0.0285 - val_mae: 0.0120\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0284 - mae: 0.0103 - val_loss: 0.0287 - val_mae: 0.0164\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0285 - mae: 0.0126 - val_loss: 0.0284 - val_mae: 0.0126\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0283 - mae: 0.0117 - val_loss: 0.0282 - val_mae: 0.0088\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0282 - mae: 0.0084 - val_loss: 0.0281 - val_mae: 0.0077\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0281 - mae: 0.0079 - val_loss: 0.0280 - val_mae: 0.0086\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0280 - mae: 0.0087 - val_loss: 0.0281 - val_mae: 0.0099\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0280 - mae: 0.0109 - val_loss: 0.0279 - val_mae: 0.0078\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0279 - mae: 0.0091 - val_loss: 0.0280 - val_mae: 0.0120\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0278 - mae: 0.0084 - val_loss: 0.0278 - val_mae: 0.0100\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0277 - mae: 0.0089 - val_loss: 0.0277 - val_mae: 0.0108\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0277 - mae: 0.0088 - val_loss: 0.0278 - val_mae: 0.0135\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0277 - mae: 0.0105 - val_loss: 0.0277 - val_mae: 0.0141\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0276 - mae: 0.0110 - val_loss: 0.0275 - val_mae: 0.0075\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0275 - mae: 0.0091 - val_loss: 0.0274 - val_mae: 0.0085\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0274 - mae: 0.0099 - val_loss: 0.0274 - val_mae: 0.0102\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0273 - mae: 0.0087 - val_loss: 0.0273 - val_mae: 0.0078\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0272 - mae: 0.0079 - val_loss: 0.0273 - val_mae: 0.0102\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0272 - mae: 0.0090 - val_loss: 0.0271 - val_mae: 0.0075\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0271 - mae: 0.0076 - val_loss: 0.0271 - val_mae: 0.0101\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0270 - mae: 0.0082 - val_loss: 0.0273 - val_mae: 0.0176\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0271 - mae: 0.0127 - val_loss: 0.0269 - val_mae: 0.0078\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0269 - mae: 0.0075 - val_loss: 0.0270 - val_mae: 0.0133\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0269 - mae: 0.0102 - val_loss: 0.0268 - val_mae: 0.0078\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0267 - mae: 0.0076 - val_loss: 0.0271 - val_mae: 0.0180\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0268 - mae: 0.0112 - val_loss: 0.0266 - val_mae: 0.0079\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0266 - mae: 0.0090 - val_loss: 0.0268 - val_mae: 0.0155\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0268 - mae: 0.0150 - val_loss: 0.0268 - val_mae: 0.0178\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0266 - mae: 0.0128 - val_loss: 0.0268 - val_mae: 0.0174\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0265 - mae: 0.0119 - val_loss: 0.0265 - val_mae: 0.0132\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0264 - mae: 0.0102 - val_loss: 0.0263 - val_mae: 0.0091\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0265 - mae: 0.0137 - val_loss: 0.0266 - val_mae: 0.0163\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0264 - mae: 0.0134 - val_loss: 0.0262 - val_mae: 0.0081\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0262 - mae: 0.0097 - val_loss: 0.0263 - val_mae: 0.0141\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0262 - mae: 0.0123 - val_loss: 0.0263 - val_mae: 0.0157\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0266 - mae: 0.0207 - val_loss: 0.0275 - val_mae: 0.0341\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0271 - mae: 0.0287 - val_loss: 0.0266 - val_mae: 0.0230\n",
      "Epoch 184/1000\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.0265 - mae: 0.0209Restoring model weights from the end of the best epoch: 179.\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0265 - mae: 0.0208 - val_loss: 0.0265 - val_mae: 0.0223\n",
      "Epoch 184: early stopping\n",
      "Training für Fold 2...\n",
      "Epoch 1/1000\n",
      "10/10 [==============================] - 1s 25ms/step - loss: 0.2979 - mae: 0.3984 - val_loss: 0.2981 - val_mae: 0.3883\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2151 - mae: 0.3104 - val_loss: 0.2274 - val_mae: 0.3336\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2015 - mae: 0.3004 - val_loss: 0.2103 - val_mae: 0.3209\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1847 - mae: 0.2861 - val_loss: 0.1965 - val_mae: 0.3005\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1734 - mae: 0.2690 - val_loss: 0.1798 - val_mae: 0.2842\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1603 - mae: 0.2523 - val_loss: 0.1652 - val_mae: 0.2691\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1475 - mae: 0.2368 - val_loss: 0.1560 - val_mae: 0.2573\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1342 - mae: 0.2238 - val_loss: 0.1789 - val_mae: 0.2774\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1357 - mae: 0.2322 - val_loss: 0.1297 - val_mae: 0.2308\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1152 - mae: 0.2002 - val_loss: 0.1167 - val_mae: 0.2085\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1064 - mae: 0.1841 - val_loss: 0.1087 - val_mae: 0.1980\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0996 - mae: 0.1759 - val_loss: 0.1028 - val_mae: 0.1843\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0944 - mae: 0.1671 - val_loss: 0.1016 - val_mae: 0.1843\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0911 - mae: 0.1597 - val_loss: 0.0880 - val_mae: 0.1619\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0820 - mae: 0.1437 - val_loss: 0.0829 - val_mae: 0.1449\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0787 - mae: 0.1344 - val_loss: 0.0875 - val_mae: 0.1618\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0757 - mae: 0.1319 - val_loss: 0.0707 - val_mae: 0.1198\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0693 - mae: 0.1111 - val_loss: 0.0666 - val_mae: 0.1087\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0643 - mae: 0.0971 - val_loss: 0.0674 - val_mae: 0.1023\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0619 - mae: 0.0918 - val_loss: 0.0601 - val_mae: 0.0881\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0580 - mae: 0.0758 - val_loss: 0.0578 - val_mae: 0.0789\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0558 - mae: 0.0678 - val_loss: 0.0567 - val_mae: 0.0749\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0543 - mae: 0.0640 - val_loss: 0.0561 - val_mae: 0.0649\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0534 - mae: 0.0593 - val_loss: 0.0527 - val_mae: 0.0553\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0530 - mae: 0.0593 - val_loss: 0.0522 - val_mae: 0.0581\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0510 - mae: 0.0513 - val_loss: 0.0516 - val_mae: 0.0606\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0512 - mae: 0.0569 - val_loss: 0.0497 - val_mae: 0.0428\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0493 - mae: 0.0449 - val_loss: 0.0504 - val_mae: 0.0503\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0488 - mae: 0.0419 - val_loss: 0.0512 - val_mae: 0.0573\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0485 - mae: 0.0412 - val_loss: 0.0485 - val_mae: 0.0382\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0479 - mae: 0.0376 - val_loss: 0.0488 - val_mae: 0.0435\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0480 - mae: 0.0408 - val_loss: 0.0476 - val_mae: 0.0385\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0469 - mae: 0.0336 - val_loss: 0.0472 - val_mae: 0.0327\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0466 - mae: 0.0329 - val_loss: 0.0488 - val_mae: 0.0442\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0465 - mae: 0.0328 - val_loss: 0.0462 - val_mae: 0.0287\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0454 - mae: 0.0234 - val_loss: 0.0458 - val_mae: 0.0288\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0453 - mae: 0.0256 - val_loss: 0.0452 - val_mae: 0.0253\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0451 - mae: 0.0256 - val_loss: 0.0452 - val_mae: 0.0258\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0448 - mae: 0.0243 - val_loss: 0.0453 - val_mae: 0.0305\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0446 - mae: 0.0238 - val_loss: 0.0456 - val_mae: 0.0381\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0446 - mae: 0.0273 - val_loss: 0.0447 - val_mae: 0.0283\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0443 - mae: 0.0249 - val_loss: 0.0441 - val_mae: 0.0215\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0440 - mae: 0.0235 - val_loss: 0.0438 - val_mae: 0.0221\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0437 - mae: 0.0220 - val_loss: 0.0444 - val_mae: 0.0320\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0437 - mae: 0.0249 - val_loss: 0.0439 - val_mae: 0.0275\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0435 - mae: 0.0247 - val_loss: 0.0432 - val_mae: 0.0208\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0429 - mae: 0.0173 - val_loss: 0.0428 - val_mae: 0.0164\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0427 - mae: 0.0156 - val_loss: 0.0433 - val_mae: 0.0309\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0427 - mae: 0.0202 - val_loss: 0.0424 - val_mae: 0.0159\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0424 - mae: 0.0172 - val_loss: 0.0422 - val_mae: 0.0156\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0420 - mae: 0.0127 - val_loss: 0.0420 - val_mae: 0.0152\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0418 - mae: 0.0126 - val_loss: 0.0419 - val_mae: 0.0160\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0417 - mae: 0.0136 - val_loss: 0.0416 - val_mae: 0.0140\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0415 - mae: 0.0124 - val_loss: 0.0415 - val_mae: 0.0158\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0413 - mae: 0.0117 - val_loss: 0.0414 - val_mae: 0.0160\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0411 - mae: 0.0117 - val_loss: 0.0415 - val_mae: 0.0220\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0411 - mae: 0.0155 - val_loss: 0.0415 - val_mae: 0.0263\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0411 - mae: 0.0189 - val_loss: 0.0414 - val_mae: 0.0258\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0409 - mae: 0.0169 - val_loss: 0.0408 - val_mae: 0.0180\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0405 - mae: 0.0143 - val_loss: 0.0405 - val_mae: 0.0152\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0403 - mae: 0.0122 - val_loss: 0.0404 - val_mae: 0.0166\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0401 - mae: 0.0126 - val_loss: 0.0401 - val_mae: 0.0133\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0400 - mae: 0.0136 - val_loss: 0.0401 - val_mae: 0.0177\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0398 - mae: 0.0126 - val_loss: 0.0397 - val_mae: 0.0116\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0396 - mae: 0.0096 - val_loss: 0.0398 - val_mae: 0.0186\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0395 - mae: 0.0132 - val_loss: 0.0394 - val_mae: 0.0141\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0395 - mae: 0.0156 - val_loss: 0.0395 - val_mae: 0.0195\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0392 - mae: 0.0143 - val_loss: 0.0392 - val_mae: 0.0134\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0390 - mae: 0.0125 - val_loss: 0.0390 - val_mae: 0.0142\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0388 - mae: 0.0102 - val_loss: 0.0388 - val_mae: 0.0132\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0387 - mae: 0.0117 - val_loss: 0.0386 - val_mae: 0.0103\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0386 - mae: 0.0129 - val_loss: 0.0384 - val_mae: 0.0110\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0383 - mae: 0.0105 - val_loss: 0.0383 - val_mae: 0.0113\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0381 - mae: 0.0088 - val_loss: 0.0382 - val_mae: 0.0131\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0380 - mae: 0.0093 - val_loss: 0.0382 - val_mae: 0.0164\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0380 - mae: 0.0132 - val_loss: 0.0383 - val_mae: 0.0233\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0380 - mae: 0.0163 - val_loss: 0.0380 - val_mae: 0.0200\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0377 - mae: 0.0139 - val_loss: 0.0378 - val_mae: 0.0171\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0375 - mae: 0.0126 - val_loss: 0.0374 - val_mae: 0.0118\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0372 - mae: 0.0095 - val_loss: 0.0372 - val_mae: 0.0113\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0371 - mae: 0.0101 - val_loss: 0.0372 - val_mae: 0.0147\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0370 - mae: 0.0114 - val_loss: 0.0372 - val_mae: 0.0178\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0369 - mae: 0.0121 - val_loss: 0.0368 - val_mae: 0.0116\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0367 - mae: 0.0109 - val_loss: 0.0366 - val_mae: 0.0113\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0365 - mae: 0.0103 - val_loss: 0.0364 - val_mae: 0.0087\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0363 - mae: 0.0080 - val_loss: 0.0363 - val_mae: 0.0102\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0362 - mae: 0.0103 - val_loss: 0.0362 - val_mae: 0.0116\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0361 - mae: 0.0115 - val_loss: 0.0362 - val_mae: 0.0144\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0361 - mae: 0.0150 - val_loss: 0.0360 - val_mae: 0.0132\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0359 - mae: 0.0108 - val_loss: 0.0358 - val_mae: 0.0122\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0357 - mae: 0.0095 - val_loss: 0.0356 - val_mae: 0.0097\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0355 - mae: 0.0100 - val_loss: 0.0356 - val_mae: 0.0131\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0355 - mae: 0.0131 - val_loss: 0.0361 - val_mae: 0.0261\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0354 - mae: 0.0127 - val_loss: 0.0352 - val_mae: 0.0104\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0351 - mae: 0.0101 - val_loss: 0.0351 - val_mae: 0.0103\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0349 - mae: 0.0080 - val_loss: 0.0349 - val_mae: 0.0091\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0349 - mae: 0.0118 - val_loss: 0.0352 - val_mae: 0.0212\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0351 - mae: 0.0191 - val_loss: 0.0349 - val_mae: 0.0149\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0350 - mae: 0.0198 - val_loss: 0.0348 - val_mae: 0.0158\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0349 - mae: 0.0191 - val_loss: 0.0345 - val_mae: 0.0120\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0344 - mae: 0.0121 - val_loss: 0.0342 - val_mae: 0.0097\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0342 - mae: 0.0111 - val_loss: 0.0342 - val_mae: 0.0122\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0340 - mae: 0.0094 - val_loss: 0.0340 - val_mae: 0.0101\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0339 - mae: 0.0081 - val_loss: 0.0339 - val_mae: 0.0118\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0338 - mae: 0.0108 - val_loss: 0.0338 - val_mae: 0.0119\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0338 - mae: 0.0145 - val_loss: 0.0339 - val_mae: 0.0192\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0337 - mae: 0.0150 - val_loss: 0.0335 - val_mae: 0.0105\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0335 - mae: 0.0129 - val_loss: 0.0335 - val_mae: 0.0139\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0333 - mae: 0.0112 - val_loss: 0.0332 - val_mae: 0.0101\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0331 - mae: 0.0099 - val_loss: 0.0331 - val_mae: 0.0107\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0330 - mae: 0.0106 - val_loss: 0.0336 - val_mae: 0.0247\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0332 - mae: 0.0186 - val_loss: 0.0330 - val_mae: 0.0171\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0328 - mae: 0.0101 - val_loss: 0.0327 - val_mae: 0.0111\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0326 - mae: 0.0092 - val_loss: 0.0327 - val_mae: 0.0161\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0326 - mae: 0.0119 - val_loss: 0.0328 - val_mae: 0.0196\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0358 - mae: 0.0353 - val_loss: 0.0610 - val_mae: 0.1622\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0475 - mae: 0.1078 - val_loss: 0.0340 - val_mae: 0.0345\n",
      "Epoch 118/1000\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.0336 - mae: 0.0326Restoring model weights from the end of the best epoch: 113.\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0379 - mae: 0.0591 - val_loss: 0.0332 - val_mae: 0.0257\n",
      "Epoch 118: early stopping\n",
      "Training für Fold 3...\n",
      "Epoch 1/1000\n",
      "10/10 [==============================] - 1s 21ms/step - loss: 0.3339 - mae: 0.4238 - val_loss: 0.2905 - val_mae: 0.4186\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2316 - mae: 0.3462 - val_loss: 0.2240 - val_mae: 0.3287\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1940 - mae: 0.3020 - val_loss: 0.1929 - val_mae: 0.2920\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1746 - mae: 0.2764 - val_loss: 0.1824 - val_mae: 0.2924\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1615 - mae: 0.2655 - val_loss: 0.1646 - val_mae: 0.2649\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1495 - mae: 0.2509 - val_loss: 0.1609 - val_mae: 0.2590\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1382 - mae: 0.2335 - val_loss: 0.1502 - val_mae: 0.2503\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1367 - mae: 0.2357 - val_loss: 0.1320 - val_mae: 0.2343\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1268 - mae: 0.2262 - val_loss: 0.1243 - val_mae: 0.2304\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1125 - mae: 0.2006 - val_loss: 0.1169 - val_mae: 0.2221\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1063 - mae: 0.1955 - val_loss: 0.1059 - val_mae: 0.2050\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0926 - mae: 0.1753 - val_loss: 0.0929 - val_mae: 0.1772\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0881 - mae: 0.1642 - val_loss: 0.0866 - val_mae: 0.1722\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0799 - mae: 0.1495 - val_loss: 0.0756 - val_mae: 0.1473\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0713 - mae: 0.1333 - val_loss: 0.0718 - val_mae: 0.1283\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0641 - mae: 0.1124 - val_loss: 0.0645 - val_mae: 0.1107\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0575 - mae: 0.0956 - val_loss: 0.0579 - val_mae: 0.0961\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0540 - mae: 0.0846 - val_loss: 0.0578 - val_mae: 0.0897\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0517 - mae: 0.0795 - val_loss: 0.0501 - val_mae: 0.0714\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0505 - mae: 0.0726 - val_loss: 0.0586 - val_mae: 0.0941\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0492 - mae: 0.0697 - val_loss: 0.0511 - val_mae: 0.0784\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0480 - mae: 0.0638 - val_loss: 0.0464 - val_mae: 0.0597\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0439 - mae: 0.0464 - val_loss: 0.0465 - val_mae: 0.0596\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0449 - mae: 0.0541 - val_loss: 0.0464 - val_mae: 0.0677\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0430 - mae: 0.0448 - val_loss: 0.0420 - val_mae: 0.0407\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0412 - mae: 0.0354 - val_loss: 0.0417 - val_mae: 0.0382\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0407 - mae: 0.0317 - val_loss: 0.0409 - val_mae: 0.0359\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0402 - mae: 0.0289 - val_loss: 0.0408 - val_mae: 0.0336\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0399 - mae: 0.0290 - val_loss: 0.0398 - val_mae: 0.0292\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0394 - mae: 0.0245 - val_loss: 0.0396 - val_mae: 0.0300\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0396 - mae: 0.0293 - val_loss: 0.0427 - val_mae: 0.0481\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0395 - mae: 0.0305 - val_loss: 0.0388 - val_mae: 0.0249\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0385 - mae: 0.0218 - val_loss: 0.0385 - val_mae: 0.0226\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0382 - mae: 0.0200 - val_loss: 0.0382 - val_mae: 0.0223\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0379 - mae: 0.0186 - val_loss: 0.0379 - val_mae: 0.0204\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0377 - mae: 0.0175 - val_loss: 0.0378 - val_mae: 0.0204\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0375 - mae: 0.0174 - val_loss: 0.0375 - val_mae: 0.0189\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0373 - mae: 0.0176 - val_loss: 0.0378 - val_mae: 0.0257\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0373 - mae: 0.0198 - val_loss: 0.0370 - val_mae: 0.0168\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0369 - mae: 0.0174 - val_loss: 0.0373 - val_mae: 0.0250\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0368 - mae: 0.0174 - val_loss: 0.0367 - val_mae: 0.0175\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0366 - mae: 0.0164 - val_loss: 0.0365 - val_mae: 0.0164\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0364 - mae: 0.0153 - val_loss: 0.0363 - val_mae: 0.0159\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0361 - mae: 0.0141 - val_loss: 0.0364 - val_mae: 0.0188\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0360 - mae: 0.0152 - val_loss: 0.0359 - val_mae: 0.0140\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0359 - mae: 0.0153 - val_loss: 0.0357 - val_mae: 0.0150\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0357 - mae: 0.0160 - val_loss: 0.0356 - val_mae: 0.0150\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0355 - mae: 0.0150 - val_loss: 0.0358 - val_mae: 0.0217\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0354 - mae: 0.0161 - val_loss: 0.0353 - val_mae: 0.0162\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0352 - mae: 0.0147 - val_loss: 0.0350 - val_mae: 0.0132\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0350 - mae: 0.0132 - val_loss: 0.0349 - val_mae: 0.0143\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0348 - mae: 0.0129 - val_loss: 0.0348 - val_mae: 0.0142\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0347 - mae: 0.0133 - val_loss: 0.0347 - val_mae: 0.0151\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.0112 - val_loss: 0.0344 - val_mae: 0.0121\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0343 - mae: 0.0107 - val_loss: 0.0342 - val_mae: 0.0132\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0341 - mae: 0.0113 - val_loss: 0.0343 - val_mae: 0.0144\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0340 - mae: 0.0109 - val_loss: 0.0339 - val_mae: 0.0113\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0338 - mae: 0.0102 - val_loss: 0.0338 - val_mae: 0.0123\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0337 - mae: 0.0112 - val_loss: 0.0336 - val_mae: 0.0108\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0336 - mae: 0.0120 - val_loss: 0.0337 - val_mae: 0.0155\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0335 - mae: 0.0140 - val_loss: 0.0333 - val_mae: 0.0102\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0332 - mae: 0.0096 - val_loss: 0.0334 - val_mae: 0.0145\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0332 - mae: 0.0123 - val_loss: 0.0332 - val_mae: 0.0133\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0329 - mae: 0.0098 - val_loss: 0.0329 - val_mae: 0.0118\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0328 - mae: 0.0099 - val_loss: 0.0328 - val_mae: 0.0112\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0327 - mae: 0.0101 - val_loss: 0.0326 - val_mae: 0.0098\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0325 - mae: 0.0091 - val_loss: 0.0324 - val_mae: 0.0099\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0324 - mae: 0.0097 - val_loss: 0.0323 - val_mae: 0.0115\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0323 - mae: 0.0103 - val_loss: 0.0322 - val_mae: 0.0109\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0321 - mae: 0.0108 - val_loss: 0.0327 - val_mae: 0.0244\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0322 - mae: 0.0153 - val_loss: 0.0321 - val_mae: 0.0163\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0320 - mae: 0.0143 - val_loss: 0.0324 - val_mae: 0.0221\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0318 - mae: 0.0127 - val_loss: 0.0316 - val_mae: 0.0098\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0316 - mae: 0.0108 - val_loss: 0.0319 - val_mae: 0.0197\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0316 - mae: 0.0129 - val_loss: 0.0315 - val_mae: 0.0129\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0313 - mae: 0.0095 - val_loss: 0.0313 - val_mae: 0.0105\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.0312 - mae: 0.0100 - val_loss: 0.0312 - val_mae: 0.0110\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0311 - mae: 0.0107 - val_loss: 0.0311 - val_mae: 0.0140\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0310 - mae: 0.0113 - val_loss: 0.0309 - val_mae: 0.0122\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0308 - mae: 0.0111 - val_loss: 0.0312 - val_mae: 0.0198\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0309 - mae: 0.0152 - val_loss: 0.0308 - val_mae: 0.0140\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0306 - mae: 0.0109 - val_loss: 0.0306 - val_mae: 0.0120\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0305 - mae: 0.0121 - val_loss: 0.0304 - val_mae: 0.0114\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0304 - mae: 0.0127 - val_loss: 0.0305 - val_mae: 0.0159\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0302 - mae: 0.0109 - val_loss: 0.0302 - val_mae: 0.0116\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0301 - mae: 0.0098 - val_loss: 0.0300 - val_mae: 0.0101\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0299 - mae: 0.0085 - val_loss: 0.0301 - val_mae: 0.0142\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0299 - mae: 0.0116 - val_loss: 0.0298 - val_mae: 0.0101\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0297 - mae: 0.0094 - val_loss: 0.0296 - val_mae: 0.0097\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0295 - mae: 0.0079 - val_loss: 0.0295 - val_mae: 0.0100\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0294 - mae: 0.0082 - val_loss: 0.0294 - val_mae: 0.0096\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0293 - mae: 0.0079 - val_loss: 0.0293 - val_mae: 0.0097\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0293 - mae: 0.0103 - val_loss: 0.0295 - val_mae: 0.0174\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0292 - mae: 0.0114 - val_loss: 0.0297 - val_mae: 0.0253\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0291 - mae: 0.0133 - val_loss: 0.0289 - val_mae: 0.0103\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0289 - mae: 0.0089 - val_loss: 0.0289 - val_mae: 0.0110\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0288 - mae: 0.0101 - val_loss: 0.0288 - val_mae: 0.0121\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0287 - mae: 0.0099 - val_loss: 0.0289 - val_mae: 0.0164\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0288 - mae: 0.0154 - val_loss: 0.0287 - val_mae: 0.0162\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0287 - mae: 0.0163 - val_loss: 0.0284 - val_mae: 0.0102\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0284 - mae: 0.0117 - val_loss: 0.0285 - val_mae: 0.0160\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0282 - mae: 0.0103 - val_loss: 0.0282 - val_mae: 0.0104\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0281 - mae: 0.0099 - val_loss: 0.0284 - val_mae: 0.0171\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0282 - mae: 0.0145 - val_loss: 0.0283 - val_mae: 0.0180\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0280 - mae: 0.0129 - val_loss: 0.0281 - val_mae: 0.0169\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0278 - mae: 0.0110 - val_loss: 0.0279 - val_mae: 0.0137\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0278 - mae: 0.0119 - val_loss: 0.0280 - val_mae: 0.0197\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0277 - mae: 0.0127 - val_loss: 0.0275 - val_mae: 0.0097\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0275 - mae: 0.0102 - val_loss: 0.0275 - val_mae: 0.0123\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0275 - mae: 0.0123 - val_loss: 0.0277 - val_mae: 0.0186\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0274 - mae: 0.0122 - val_loss: 0.0280 - val_mae: 0.0254\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0274 - mae: 0.0157 - val_loss: 0.0277 - val_mae: 0.0217\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0273 - mae: 0.0147 - val_loss: 0.0270 - val_mae: 0.0097\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0270 - mae: 0.0100 - val_loss: 0.0270 - val_mae: 0.0123\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0269 - mae: 0.0102 - val_loss: 0.0270 - val_mae: 0.0144\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0268 - mae: 0.0112 - val_loss: 0.0270 - val_mae: 0.0172\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0268 - mae: 0.0142 - val_loss: 0.0271 - val_mae: 0.0195\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0267 - mae: 0.0129 - val_loss: 0.0266 - val_mae: 0.0123\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0265 - mae: 0.0093 - val_loss: 0.0265 - val_mae: 0.0119\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0263 - mae: 0.0082 - val_loss: 0.0263 - val_mae: 0.0095\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0263 - mae: 0.0089 - val_loss: 0.0265 - val_mae: 0.0152\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0262 - mae: 0.0097 - val_loss: 0.0262 - val_mae: 0.0120\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0261 - mae: 0.0083 - val_loss: 0.0260 - val_mae: 0.0102\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0259 - mae: 0.0075 - val_loss: 0.0259 - val_mae: 0.0092\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0259 - mae: 0.0081 - val_loss: 0.0259 - val_mae: 0.0109\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0258 - mae: 0.0092 - val_loss: 0.0259 - val_mae: 0.0145\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0257 - mae: 0.0108 - val_loss: 0.0259 - val_mae: 0.0172\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0257 - mae: 0.0117 - val_loss: 0.0256 - val_mae: 0.0118\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0256 - mae: 0.0105 - val_loss: 0.0255 - val_mae: 0.0096\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0254 - mae: 0.0079 - val_loss: 0.0255 - val_mae: 0.0118\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0254 - mae: 0.0102 - val_loss: 0.0256 - val_mae: 0.0165\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0253 - mae: 0.0124 - val_loss: 0.0254 - val_mae: 0.0139\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0252 - mae: 0.0117 - val_loss: 0.0254 - val_mae: 0.0168\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0251 - mae: 0.0115 - val_loss: 0.0253 - val_mae: 0.0167\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0251 - mae: 0.0121 - val_loss: 0.0250 - val_mae: 0.0108\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0249 - mae: 0.0085 - val_loss: 0.0249 - val_mae: 0.0097\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0248 - mae: 0.0090 - val_loss: 0.0248 - val_mae: 0.0092\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0247 - mae: 0.0093 - val_loss: 0.0247 - val_mae: 0.0098\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0246 - mae: 0.0097 - val_loss: 0.0247 - val_mae: 0.0110\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0246 - mae: 0.0105 - val_loss: 0.0245 - val_mae: 0.0103\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0245 - mae: 0.0091 - val_loss: 0.0245 - val_mae: 0.0126\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0245 - mae: 0.0125 - val_loss: 0.0265 - val_mae: 0.0387\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0250 - mae: 0.0221 - val_loss: 0.0244 - val_mae: 0.0129\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0245 - mae: 0.0165 - val_loss: 0.0246 - val_mae: 0.0193\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.0107 - val_loss: 0.0243 - val_mae: 0.0152\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.0103 - val_loss: 0.0240 - val_mae: 0.0104\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0239 - mae: 0.0088 - val_loss: 0.0241 - val_mae: 0.0140\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0240 - mae: 0.0130 - val_loss: 0.0239 - val_mae: 0.0116\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0238 - mae: 0.0099 - val_loss: 0.0238 - val_mae: 0.0101\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0237 - mae: 0.0087 - val_loss: 0.0237 - val_mae: 0.0112\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0236 - mae: 0.0085 - val_loss: 0.0236 - val_mae: 0.0087\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0236 - mae: 0.0099 - val_loss: 0.0249 - val_mae: 0.0312\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0239 - mae: 0.0197 - val_loss: 0.0237 - val_mae: 0.0145\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0236 - mae: 0.0151 - val_loss: 0.0235 - val_mae: 0.0135\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0233 - mae: 0.0093 - val_loss: 0.0236 - val_mae: 0.0172\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0232 - mae: 0.0093 - val_loss: 0.0233 - val_mae: 0.0121\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0231 - mae: 0.0088 - val_loss: 0.0231 - val_mae: 0.0085\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0230 - mae: 0.0075 - val_loss: 0.0231 - val_mae: 0.0097\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0230 - mae: 0.0074 - val_loss: 0.0231 - val_mae: 0.0143\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0230 - mae: 0.0101 - val_loss: 0.0229 - val_mae: 0.0096\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0229 - mae: 0.0120 - val_loss: 0.0234 - val_mae: 0.0231\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0228 - mae: 0.0111 - val_loss: 0.0230 - val_mae: 0.0160\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0228 - mae: 0.0135 - val_loss: 0.0229 - val_mae: 0.0165\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0228 - mae: 0.0140 - val_loss: 0.0227 - val_mae: 0.0127\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0226 - mae: 0.0114 - val_loss: 0.0228 - val_mae: 0.0167\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0227 - mae: 0.0141 - val_loss: 0.0226 - val_mae: 0.0119\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0224 - mae: 0.0098 - val_loss: 0.0224 - val_mae: 0.0096\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0223 - mae: 0.0083 - val_loss: 0.0229 - val_mae: 0.0220\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0225 - mae: 0.0156 - val_loss: 0.0223 - val_mae: 0.0101\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0222 - mae: 0.0101 - val_loss: 0.0224 - val_mae: 0.0171\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0224 - mae: 0.0163 - val_loss: 0.0221 - val_mae: 0.0115\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0221 - mae: 0.0121 - val_loss: 0.0222 - val_mae: 0.0149\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0220 - mae: 0.0100 - val_loss: 0.0220 - val_mae: 0.0110\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0219 - mae: 0.0096 - val_loss: 0.0219 - val_mae: 0.0096\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0219 - mae: 0.0108 - val_loss: 0.0223 - val_mae: 0.0195\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0219 - mae: 0.0124 - val_loss: 0.0217 - val_mae: 0.0092\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0216 - mae: 0.0074 - val_loss: 0.0216 - val_mae: 0.0083\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0216 - mae: 0.0074 - val_loss: 0.0218 - val_mae: 0.0130\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0215 - mae: 0.0089 - val_loss: 0.0217 - val_mae: 0.0128\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0216 - mae: 0.0118 - val_loss: 0.0218 - val_mae: 0.0183\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0215 - mae: 0.0127 - val_loss: 0.0217 - val_mae: 0.0159\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0214 - mae: 0.0111 - val_loss: 0.0214 - val_mae: 0.0137\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0213 - mae: 0.0103 - val_loss: 0.0220 - val_mae: 0.0236\n",
      "Epoch 184/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0214 - mae: 0.0146 - val_loss: 0.0213 - val_mae: 0.0147\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0213 - mae: 0.0126 - val_loss: 0.0211 - val_mae: 0.0090\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0211 - mae: 0.0099 - val_loss: 0.0212 - val_mae: 0.0128\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0210 - mae: 0.0100 - val_loss: 0.0211 - val_mae: 0.0121\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0210 - mae: 0.0119 - val_loss: 0.0209 - val_mae: 0.0102\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0208 - mae: 0.0082 - val_loss: 0.0210 - val_mae: 0.0139\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0208 - mae: 0.0095 - val_loss: 0.0210 - val_mae: 0.0162\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0209 - mae: 0.0145 - val_loss: 0.0223 - val_mae: 0.0357\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0212 - mae: 0.0194 - val_loss: 0.0207 - val_mae: 0.0095\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0206 - mae: 0.0091 - val_loss: 0.0208 - val_mae: 0.0149\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0206 - mae: 0.0105 - val_loss: 0.0206 - val_mae: 0.0109\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0206 - mae: 0.0122 - val_loss: 0.0207 - val_mae: 0.0174\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0205 - mae: 0.0126 - val_loss: 0.0204 - val_mae: 0.0097\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0203 - mae: 0.0088 - val_loss: 0.0205 - val_mae: 0.0136\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0205 - mae: 0.0148 - val_loss: 0.0205 - val_mae: 0.0165\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0205 - mae: 0.0159 - val_loss: 0.0208 - val_mae: 0.0233\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0205 - mae: 0.0177 - val_loss: 0.0206 - val_mae: 0.0188\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0203 - mae: 0.0152 - val_loss: 0.0204 - val_mae: 0.0182\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0202 - mae: 0.0146 - val_loss: 0.0203 - val_mae: 0.0170\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0205 - mae: 0.0203 - val_loss: 0.0216 - val_mae: 0.0350\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0209 - mae: 0.0261 - val_loss: 0.0206 - val_mae: 0.0259\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0205 - mae: 0.0232 - val_loss: 0.0204 - val_mae: 0.0204\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0200 - mae: 0.0151 - val_loss: 0.0199 - val_mae: 0.0138\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0199 - mae: 0.0130 - val_loss: 0.0199 - val_mae: 0.0150\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0198 - mae: 0.0123 - val_loss: 0.0199 - val_mae: 0.0145\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0196 - mae: 0.0105 - val_loss: 0.0197 - val_mae: 0.0120\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0195 - mae: 0.0089 - val_loss: 0.0196 - val_mae: 0.0105\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0196 - mae: 0.0126 - val_loss: 0.0204 - val_mae: 0.0263\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0198 - mae: 0.0166 - val_loss: 0.0195 - val_mae: 0.0113\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0195 - mae: 0.0140 - val_loss: 0.0198 - val_mae: 0.0182\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0194 - mae: 0.0127 - val_loss: 0.0197 - val_mae: 0.0191\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0194 - mae: 0.0141 - val_loss: 0.0193 - val_mae: 0.0108\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0192 - mae: 0.0108 - val_loss: 0.0192 - val_mae: 0.0105\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0192 - mae: 0.0106 - val_loss: 0.0194 - val_mae: 0.0178\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0193 - mae: 0.0156 - val_loss: 0.0192 - val_mae: 0.0128\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0197 - mae: 0.0216 - val_loss: 0.0204 - val_mae: 0.0316\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0195 - mae: 0.0196 - val_loss: 0.0192 - val_mae: 0.0153\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0190 - mae: 0.0131 - val_loss: 0.0189 - val_mae: 0.0104\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0189 - mae: 0.0117 - val_loss: 0.0192 - val_mae: 0.0202\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0191 - mae: 0.0161 - val_loss: 0.0189 - val_mae: 0.0130\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0189 - mae: 0.0140 - val_loss: 0.0188 - val_mae: 0.0116\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0188 - mae: 0.0120 - val_loss: 0.0188 - val_mae: 0.0137\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0187 - mae: 0.0099 - val_loss: 0.0193 - val_mae: 0.0216\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0187 - mae: 0.0132 - val_loss: 0.0187 - val_mae: 0.0131\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0187 - mae: 0.0130 - val_loss: 0.0186 - val_mae: 0.0126\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0185 - mae: 0.0111 - val_loss: 0.0185 - val_mae: 0.0091\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0184 - mae: 0.0086 - val_loss: 0.0186 - val_mae: 0.0154\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0110 - val_loss: 0.0187 - val_mae: 0.0176\n",
      "Epoch 232/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0184 - mae: 0.0124 - val_loss: 0.0184 - val_mae: 0.0124\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0184 - mae: 0.0128 - val_loss: 0.0184 - val_mae: 0.0137\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0183 - mae: 0.0120 - val_loss: 0.0183 - val_mae: 0.0133\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0183 - mae: 0.0118 - val_loss: 0.0182 - val_mae: 0.0110\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0181 - mae: 0.0090 - val_loss: 0.0182 - val_mae: 0.0113\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0180 - mae: 0.0092 - val_loss: 0.0186 - val_mae: 0.0220\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0184 - mae: 0.0172 - val_loss: 0.0187 - val_mae: 0.0241\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0183 - mae: 0.0169 - val_loss: 0.0182 - val_mae: 0.0153\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0180 - mae: 0.0132 - val_loss: 0.0180 - val_mae: 0.0142\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0178 - mae: 0.0098 - val_loss: 0.0179 - val_mae: 0.0118\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0178 - mae: 0.0091 - val_loss: 0.0179 - val_mae: 0.0124\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0098 - val_loss: 0.0177 - val_mae: 0.0084\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0178 - mae: 0.0125 - val_loss: 0.0178 - val_mae: 0.0140\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0178 - mae: 0.0133 - val_loss: 0.0177 - val_mae: 0.0112\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0092 - val_loss: 0.0176 - val_mae: 0.0099\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0176 - mae: 0.0105 - val_loss: 0.0185 - val_mae: 0.0255\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0179 - mae: 0.0180 - val_loss: 0.0176 - val_mae: 0.0122\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0139 - val_loss: 0.0176 - val_mae: 0.0139\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0174 - mae: 0.0105 - val_loss: 0.0175 - val_mae: 0.0117\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0174 - mae: 0.0123 - val_loss: 0.0177 - val_mae: 0.0199\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0137 - val_loss: 0.0174 - val_mae: 0.0133\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0108 - val_loss: 0.0172 - val_mae: 0.0095\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0086 - val_loss: 0.0172 - val_mae: 0.0114\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0171 - mae: 0.0081 - val_loss: 0.0171 - val_mae: 0.0107\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0079 - val_loss: 0.0173 - val_mae: 0.0156\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0172 - mae: 0.0140 - val_loss: 0.0174 - val_mae: 0.0172\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0174 - mae: 0.0189 - val_loss: 0.0176 - val_mae: 0.0240\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0121 - val_loss: 0.0170 - val_mae: 0.0113\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0172 - mae: 0.0173 - val_loss: 0.0175 - val_mae: 0.0234\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0134 - val_loss: 0.0169 - val_mae: 0.0116\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0169 - mae: 0.0120 - val_loss: 0.0168 - val_mae: 0.0109\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0100 - val_loss: 0.0169 - val_mae: 0.0140\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0169 - mae: 0.0139 - val_loss: 0.0170 - val_mae: 0.0165\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0169 - mae: 0.0151 - val_loss: 0.0168 - val_mae: 0.0173\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0166 - mae: 0.0100 - val_loss: 0.0168 - val_mae: 0.0124\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0168 - mae: 0.0148 - val_loss: 0.0169 - val_mae: 0.0183\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0240 - val_loss: 0.0166 - val_mae: 0.0121\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0171 - mae: 0.0221 - val_loss: 0.0173 - val_mae: 0.0270\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0215 - val_loss: 0.0194 - val_mae: 0.0450\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0188 - mae: 0.0406 - val_loss: 0.0203 - val_mae: 0.0558\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0343 - val_loss: 0.0170 - val_mae: 0.0244\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0168 - mae: 0.0195 - val_loss: 0.0166 - val_mae: 0.0171\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0164 - mae: 0.0144 - val_loss: 0.0163 - val_mae: 0.0113\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0163 - mae: 0.0120 - val_loss: 0.0165 - val_mae: 0.0177\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0163 - mae: 0.0143 - val_loss: 0.0162 - val_mae: 0.0129\n",
      "Epoch 277/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0094 - val_loss: 0.0161 - val_mae: 0.0094\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0098 - val_loss: 0.0160 - val_mae: 0.0083\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0109 - val_loss: 0.0160 - val_mae: 0.0097\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0159 - mae: 0.0078 - val_loss: 0.0165 - val_mae: 0.0216\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0161 - mae: 0.0130 - val_loss: 0.0159 - val_mae: 0.0100\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0158 - mae: 0.0080 - val_loss: 0.0159 - val_mae: 0.0101\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0158 - mae: 0.0083 - val_loss: 0.0158 - val_mae: 0.0090\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0100 - val_loss: 0.0160 - val_mae: 0.0157\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0158 - mae: 0.0120 - val_loss: 0.0157 - val_mae: 0.0086\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0157 - mae: 0.0091 - val_loss: 0.0157 - val_mae: 0.0095\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0157 - mae: 0.0107 - val_loss: 0.0156 - val_mae: 0.0095\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0156 - mae: 0.0087 - val_loss: 0.0158 - val_mae: 0.0140\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0156 - mae: 0.0094 - val_loss: 0.0156 - val_mae: 0.0098\n",
      "Epoch 290/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0155 - mae: 0.0103 - val_loss: 0.0157 - val_mae: 0.0137\n",
      "Epoch 291/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0155 - mae: 0.0094 - val_loss: 0.0157 - val_mae: 0.0143\n",
      "Epoch 292/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0156 - mae: 0.0121 - val_loss: 0.0155 - val_mae: 0.0115\n",
      "Epoch 293/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0154 - mae: 0.0093 - val_loss: 0.0154 - val_mae: 0.0083\n",
      "Epoch 294/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0153 - mae: 0.0086 - val_loss: 0.0153 - val_mae: 0.0092\n",
      "Epoch 295/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0153 - mae: 0.0082 - val_loss: 0.0153 - val_mae: 0.0098\n",
      "Epoch 296/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0152 - mae: 0.0082 - val_loss: 0.0152 - val_mae: 0.0085\n",
      "Epoch 297/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0152 - mae: 0.0071 - val_loss: 0.0154 - val_mae: 0.0146\n",
      "Epoch 298/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0152 - mae: 0.0101 - val_loss: 0.0155 - val_mae: 0.0161\n",
      "Epoch 299/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0152 - mae: 0.0121 - val_loss: 0.0152 - val_mae: 0.0119\n",
      "Epoch 300/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0152 - mae: 0.0112 - val_loss: 0.0151 - val_mae: 0.0084\n",
      "Epoch 301/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0150 - mae: 0.0077 - val_loss: 0.0150 - val_mae: 0.0072\n",
      "Epoch 302/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0149 - mae: 0.0066 - val_loss: 0.0149 - val_mae: 0.0074\n",
      "Epoch 303/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0149 - mae: 0.0065 - val_loss: 0.0150 - val_mae: 0.0107\n",
      "Epoch 304/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0149 - mae: 0.0091 - val_loss: 0.0149 - val_mae: 0.0107\n",
      "Epoch 305/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0149 - mae: 0.0090 - val_loss: 0.0148 - val_mae: 0.0082\n",
      "Epoch 306/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0149 - mae: 0.0108 - val_loss: 0.0149 - val_mae: 0.0134\n",
      "Epoch 307/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0149 - mae: 0.0131 - val_loss: 0.0148 - val_mae: 0.0099\n",
      "Epoch 308/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0148 - mae: 0.0108 - val_loss: 0.0151 - val_mae: 0.0165\n",
      "Epoch 309/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0148 - mae: 0.0110 - val_loss: 0.0150 - val_mae: 0.0178\n",
      "Epoch 310/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0147 - mae: 0.0112 - val_loss: 0.0147 - val_mae: 0.0090\n",
      "Epoch 311/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0148 - mae: 0.0135 - val_loss: 0.0159 - val_mae: 0.0282\n",
      "Epoch 312/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0218 - val_loss: 0.0150 - val_mae: 0.0195\n",
      "Epoch 313/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0152 - mae: 0.0208 - val_loss: 0.0147 - val_mae: 0.0133\n",
      "Epoch 314/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0147 - mae: 0.0144 - val_loss: 0.0147 - val_mae: 0.0134\n",
      "Epoch 315/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0147 - mae: 0.0155 - val_loss: 0.0148 - val_mae: 0.0164\n",
      "Epoch 316/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0145 - mae: 0.0114 - val_loss: 0.0144 - val_mae: 0.0085\n",
      "Epoch 317/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0144 - mae: 0.0083 - val_loss: 0.0147 - val_mae: 0.0170\n",
      "Epoch 318/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0144 - mae: 0.0096 - val_loss: 0.0143 - val_mae: 0.0093\n",
      "Epoch 319/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0143 - mae: 0.0081 - val_loss: 0.0143 - val_mae: 0.0098\n",
      "Epoch 320/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0142 - mae: 0.0079 - val_loss: 0.0143 - val_mae: 0.0093\n",
      "Epoch 321/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0142 - mae: 0.0084 - val_loss: 0.0143 - val_mae: 0.0128\n",
      "Epoch 322/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0071 - val_loss: 0.0141 - val_mae: 0.0069\n",
      "Epoch 323/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0063 - val_loss: 0.0141 - val_mae: 0.0077\n",
      "Epoch 324/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0140 - mae: 0.0065 - val_loss: 0.0141 - val_mae: 0.0079\n",
      "Epoch 325/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0140 - mae: 0.0070 - val_loss: 0.0140 - val_mae: 0.0072\n",
      "Epoch 326/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0140 - mae: 0.0071 - val_loss: 0.0141 - val_mae: 0.0116\n",
      "Epoch 327/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0140 - mae: 0.0077 - val_loss: 0.0140 - val_mae: 0.0091\n",
      "Epoch 328/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0139 - mae: 0.0073 - val_loss: 0.0140 - val_mae: 0.0100\n",
      "Epoch 329/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0140 - mae: 0.0120 - val_loss: 0.0145 - val_mae: 0.0214\n",
      "Epoch 330/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0141 - mae: 0.0142 - val_loss: 0.0145 - val_mae: 0.0212\n",
      "Epoch 331/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0142 - mae: 0.0172 - val_loss: 0.0140 - val_mae: 0.0160\n",
      "Epoch 332/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0170 - val_loss: 0.0138 - val_mae: 0.0119\n",
      "Epoch 333/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0141 - mae: 0.0160 - val_loss: 0.0137 - val_mae: 0.0097\n",
      "Epoch 334/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0140 - mae: 0.0158 - val_loss: 0.0142 - val_mae: 0.0203\n",
      "Epoch 335/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0140 - mae: 0.0177 - val_loss: 0.0142 - val_mae: 0.0199\n",
      "Epoch 336/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0138 - mae: 0.0138 - val_loss: 0.0145 - val_mae: 0.0286\n",
      "Epoch 337/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0140 - mae: 0.0182 - val_loss: 0.0136 - val_mae: 0.0092\n",
      "Epoch 338/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0137 - mae: 0.0127 - val_loss: 0.0138 - val_mae: 0.0157\n",
      "Epoch 339/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0136 - mae: 0.0106 - val_loss: 0.0138 - val_mae: 0.0176\n",
      "Epoch 340/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0135 - mae: 0.0109 - val_loss: 0.0135 - val_mae: 0.0095\n",
      "Epoch 341/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0134 - mae: 0.0077 - val_loss: 0.0137 - val_mae: 0.0166\n",
      "Epoch 342/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0135 - mae: 0.0115 - val_loss: 0.0133 - val_mae: 0.0068\n",
      "Epoch 343/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0134 - mae: 0.0087 - val_loss: 0.0134 - val_mae: 0.0096\n",
      "Epoch 344/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0133 - mae: 0.0072 - val_loss: 0.0133 - val_mae: 0.0072\n",
      "Epoch 345/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0132 - mae: 0.0072 - val_loss: 0.0135 - val_mae: 0.0159\n",
      "Epoch 346/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0133 - mae: 0.0093 - val_loss: 0.0133 - val_mae: 0.0127\n",
      "Epoch 347/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0134 - mae: 0.0123 - val_loss: 0.0134 - val_mae: 0.0146\n",
      "Epoch 348/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0133 - mae: 0.0125 - val_loss: 0.0134 - val_mae: 0.0161\n",
      "Epoch 349/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0132 - mae: 0.0110 - val_loss: 0.0132 - val_mae: 0.0092\n",
      "Epoch 350/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0112 - val_loss: 0.0136 - val_mae: 0.0238\n",
      "Epoch 351/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0133 - mae: 0.0145 - val_loss: 0.0132 - val_mae: 0.0123\n",
      "Epoch 352/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0132 - mae: 0.0145 - val_loss: 0.0135 - val_mae: 0.0205\n",
      "Epoch 353/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0135 - mae: 0.0189 - val_loss: 0.0133 - val_mae: 0.0195\n",
      "Epoch 354/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0130 - mae: 0.0118 - val_loss: 0.0129 - val_mae: 0.0081\n",
      "Epoch 355/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0129 - mae: 0.0082 - val_loss: 0.0130 - val_mae: 0.0110\n",
      "Epoch 356/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0131 - mae: 0.0147 - val_loss: 0.0130 - val_mae: 0.0123\n",
      "Epoch 357/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0129 - mae: 0.0095 - val_loss: 0.0128 - val_mae: 0.0084\n",
      "Epoch 358/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0128 - mae: 0.0082 - val_loss: 0.0128 - val_mae: 0.0080\n",
      "Epoch 359/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0074 - val_loss: 0.0127 - val_mae: 0.0081\n",
      "Epoch 360/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0066 - val_loss: 0.0127 - val_mae: 0.0074\n",
      "Epoch 361/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0090 - val_loss: 0.0128 - val_mae: 0.0142\n",
      "Epoch 362/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0101 - val_loss: 0.0126 - val_mae: 0.0073\n",
      "Epoch 363/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0126 - mae: 0.0087 - val_loss: 0.0128 - val_mae: 0.0121\n",
      "Epoch 364/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0127 - mae: 0.0124 - val_loss: 0.0128 - val_mae: 0.0155\n",
      "Epoch 365/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0129 - val_loss: 0.0130 - val_mae: 0.0206\n",
      "Epoch 366/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0130 - mae: 0.0192 - val_loss: 0.0125 - val_mae: 0.0080\n",
      "Epoch 367/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0127 - mae: 0.0129 - val_loss: 0.0126 - val_mae: 0.0123\n",
      "Epoch 368/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0125 - mae: 0.0107 - val_loss: 0.0124 - val_mae: 0.0081\n",
      "Epoch 369/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0124 - mae: 0.0082 - val_loss: 0.0125 - val_mae: 0.0110\n",
      "Epoch 370/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0124 - mae: 0.0086 - val_loss: 0.0124 - val_mae: 0.0089\n",
      "Epoch 371/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0123 - mae: 0.0082 - val_loss: 0.0123 - val_mae: 0.0080\n",
      "Epoch 372/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0122 - mae: 0.0062 - val_loss: 0.0124 - val_mae: 0.0105\n",
      "Epoch 373/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0123 - mae: 0.0086 - val_loss: 0.0123 - val_mae: 0.0099\n",
      "Epoch 374/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0122 - mae: 0.0079 - val_loss: 0.0122 - val_mae: 0.0088\n",
      "Epoch 375/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0122 - mae: 0.0070 - val_loss: 0.0122 - val_mae: 0.0081\n",
      "Epoch 376/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0121 - mae: 0.0067 - val_loss: 0.0124 - val_mae: 0.0153\n",
      "Epoch 377/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0122 - mae: 0.0096 - val_loss: 0.0125 - val_mae: 0.0180\n",
      "Epoch 378/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0122 - mae: 0.0129 - val_loss: 0.0123 - val_mae: 0.0143\n",
      "Epoch 379/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0122 - mae: 0.0121 - val_loss: 0.0121 - val_mae: 0.0104\n",
      "Epoch 380/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0141 - val_loss: 0.0124 - val_mae: 0.0167\n",
      "Epoch 381/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0122 - mae: 0.0155 - val_loss: 0.0131 - val_mae: 0.0267\n",
      "Epoch 382/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0125 - mae: 0.0197 - val_loss: 0.0128 - val_mae: 0.0243\n",
      "Epoch 383/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0122 - mae: 0.0156 - val_loss: 0.0123 - val_mae: 0.0161\n",
      "Epoch 384/1000\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.0121 - mae: 0.0136Restoring model weights from the end of the best epoch: 379.\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0120 - mae: 0.0123 - val_loss: 0.0129 - val_mae: 0.0278\n",
      "Epoch 384: early stopping\n",
      "Training für Fold 4...\n",
      "Epoch 1/1000\n",
      "10/10 [==============================] - 1s 20ms/step - loss: 0.5183 - mae: 0.5515 - val_loss: 0.2380 - val_mae: 0.3470\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2467 - mae: 0.3628 - val_loss: 0.2126 - val_mae: 0.3254\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2045 - mae: 0.3097 - val_loss: 0.1794 - val_mae: 0.2893\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1867 - mae: 0.2866 - val_loss: 0.1647 - val_mae: 0.2595\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1775 - mae: 0.2766 - val_loss: 0.1478 - val_mae: 0.2440\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1673 - mae: 0.2709 - val_loss: 0.1406 - val_mae: 0.2359\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1525 - mae: 0.2522 - val_loss: 0.1316 - val_mae: 0.2255\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1413 - mae: 0.2401 - val_loss: 0.1237 - val_mae: 0.2160\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1330 - mae: 0.2318 - val_loss: 0.1135 - val_mae: 0.2020\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1228 - mae: 0.2195 - val_loss: 0.1085 - val_mae: 0.1913\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1134 - mae: 0.2076 - val_loss: 0.0963 - val_mae: 0.1766\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1046 - mae: 0.1963 - val_loss: 0.0863 - val_mae: 0.1586\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0948 - mae: 0.1803 - val_loss: 0.0794 - val_mae: 0.1494\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0884 - mae: 0.1675 - val_loss: 0.0765 - val_mae: 0.1459\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0801 - mae: 0.1573 - val_loss: 0.0691 - val_mae: 0.1287\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0727 - mae: 0.1402 - val_loss: 0.0700 - val_mae: 0.1249\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0712 - mae: 0.1343 - val_loss: 0.0692 - val_mae: 0.1326\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0663 - mae: 0.1255 - val_loss: 0.0550 - val_mae: 0.0987\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0591 - mae: 0.1044 - val_loss: 0.0513 - val_mae: 0.0832\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0525 - mae: 0.0843 - val_loss: 0.0533 - val_mae: 0.0883\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0523 - mae: 0.0832 - val_loss: 0.0461 - val_mae: 0.0614\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0480 - mae: 0.0693 - val_loss: 0.0458 - val_mae: 0.0604\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0465 - mae: 0.0628 - val_loss: 0.0427 - val_mae: 0.0482\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0440 - mae: 0.0542 - val_loss: 0.0425 - val_mae: 0.0479\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0433 - mae: 0.0503 - val_loss: 0.0447 - val_mae: 0.0566\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0435 - mae: 0.0534 - val_loss: 0.0419 - val_mae: 0.0483\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0416 - mae: 0.0450 - val_loss: 0.0396 - val_mae: 0.0325\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0399 - mae: 0.0350 - val_loss: 0.0391 - val_mae: 0.0315\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0393 - mae: 0.0338 - val_loss: 0.0387 - val_mae: 0.0299\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0390 - mae: 0.0321 - val_loss: 0.0380 - val_mae: 0.0256\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0384 - mae: 0.0310 - val_loss: 0.0376 - val_mae: 0.0239\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0377 - mae: 0.0250 - val_loss: 0.0386 - val_mae: 0.0346\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0379 - mae: 0.0303 - val_loss: 0.0380 - val_mae: 0.0331\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0373 - mae: 0.0270 - val_loss: 0.0370 - val_mae: 0.0237\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0367 - mae: 0.0208 - val_loss: 0.0364 - val_mae: 0.0187\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0365 - mae: 0.0213 - val_loss: 0.0362 - val_mae: 0.0183\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0363 - mae: 0.0214 - val_loss: 0.0359 - val_mae: 0.0185\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0360 - mae: 0.0197 - val_loss: 0.0356 - val_mae: 0.0169\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0357 - mae: 0.0192 - val_loss: 0.0355 - val_mae: 0.0174\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0355 - mae: 0.0183 - val_loss: 0.0352 - val_mae: 0.0152\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0353 - mae: 0.0172 - val_loss: 0.0350 - val_mae: 0.0135\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0350 - mae: 0.0156 - val_loss: 0.0349 - val_mae: 0.0175\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0347 - mae: 0.0148 - val_loss: 0.0345 - val_mae: 0.0124\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0346 - mae: 0.0159 - val_loss: 0.0344 - val_mae: 0.0140\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.0142 - val_loss: 0.0342 - val_mae: 0.0119\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0341 - mae: 0.0134 - val_loss: 0.0340 - val_mae: 0.0117\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0340 - mae: 0.0130 - val_loss: 0.0339 - val_mae: 0.0135\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0339 - mae: 0.0154 - val_loss: 0.0338 - val_mae: 0.0151\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0337 - mae: 0.0158 - val_loss: 0.0334 - val_mae: 0.0111\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0334 - mae: 0.0126 - val_loss: 0.0333 - val_mae: 0.0122\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0333 - mae: 0.0128 - val_loss: 0.0331 - val_mae: 0.0109\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0331 - mae: 0.0134 - val_loss: 0.0331 - val_mae: 0.0147\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0330 - mae: 0.0132 - val_loss: 0.0329 - val_mae: 0.0153\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0329 - mae: 0.0152 - val_loss: 0.0326 - val_mae: 0.0109\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0326 - mae: 0.0125 - val_loss: 0.0326 - val_mae: 0.0127\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0326 - mae: 0.0157 - val_loss: 0.0323 - val_mae: 0.0103\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0324 - mae: 0.0131 - val_loss: 0.0322 - val_mae: 0.0111\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0322 - mae: 0.0128 - val_loss: 0.0320 - val_mae: 0.0116\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0320 - mae: 0.0112 - val_loss: 0.0318 - val_mae: 0.0103\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0318 - mae: 0.0110 - val_loss: 0.0317 - val_mae: 0.0099\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0317 - mae: 0.0116 - val_loss: 0.0316 - val_mae: 0.0104\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0316 - mae: 0.0124 - val_loss: 0.0315 - val_mae: 0.0140\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0314 - mae: 0.0126 - val_loss: 0.0314 - val_mae: 0.0138\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0313 - mae: 0.0137 - val_loss: 0.0312 - val_mae: 0.0130\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0311 - mae: 0.0119 - val_loss: 0.0310 - val_mae: 0.0128\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0309 - mae: 0.0110 - val_loss: 0.0308 - val_mae: 0.0105\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0308 - mae: 0.0108 - val_loss: 0.0309 - val_mae: 0.0161\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0307 - mae: 0.0121 - val_loss: 0.0306 - val_mae: 0.0114\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0305 - mae: 0.0101 - val_loss: 0.0304 - val_mae: 0.0109\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0304 - mae: 0.0102 - val_loss: 0.0302 - val_mae: 0.0091\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0303 - mae: 0.0118 - val_loss: 0.0301 - val_mae: 0.0107\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0301 - mae: 0.0100 - val_loss: 0.0300 - val_mae: 0.0092\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0299 - mae: 0.0092 - val_loss: 0.0299 - val_mae: 0.0097\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0298 - mae: 0.0106 - val_loss: 0.0298 - val_mae: 0.0125\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0297 - mae: 0.0096 - val_loss: 0.0296 - val_mae: 0.0092\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0295 - mae: 0.0096 - val_loss: 0.0295 - val_mae: 0.0094\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0294 - mae: 0.0097 - val_loss: 0.0295 - val_mae: 0.0144\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0294 - mae: 0.0116 - val_loss: 0.0293 - val_mae: 0.0127\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0293 - mae: 0.0125 - val_loss: 0.0293 - val_mae: 0.0156\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0292 - mae: 0.0141 - val_loss: 0.0290 - val_mae: 0.0104\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0289 - mae: 0.0105 - val_loss: 0.0292 - val_mae: 0.0201\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0290 - mae: 0.0144 - val_loss: 0.0287 - val_mae: 0.0106\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0287 - mae: 0.0115 - val_loss: 0.0291 - val_mae: 0.0209\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0288 - mae: 0.0152 - val_loss: 0.0285 - val_mae: 0.0113\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0285 - mae: 0.0111 - val_loss: 0.0284 - val_mae: 0.0097\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0284 - mae: 0.0132 - val_loss: 0.0284 - val_mae: 0.0132\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0283 - mae: 0.0121 - val_loss: 0.0285 - val_mae: 0.0198\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0282 - mae: 0.0133 - val_loss: 0.0282 - val_mae: 0.0138\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0281 - mae: 0.0122 - val_loss: 0.0282 - val_mae: 0.0171\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0280 - mae: 0.0131 - val_loss: 0.0281 - val_mae: 0.0182\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0280 - mae: 0.0160 - val_loss: 0.0277 - val_mae: 0.0109\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0279 - mae: 0.0148 - val_loss: 0.0279 - val_mae: 0.0167\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0276 - mae: 0.0113 - val_loss: 0.0275 - val_mae: 0.0110\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0275 - mae: 0.0109 - val_loss: 0.0274 - val_mae: 0.0099\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0274 - mae: 0.0113 - val_loss: 0.0273 - val_mae: 0.0115\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0272 - mae: 0.0097 - val_loss: 0.0271 - val_mae: 0.0100\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0271 - mae: 0.0096 - val_loss: 0.0271 - val_mae: 0.0104\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0270 - mae: 0.0092 - val_loss: 0.0270 - val_mae: 0.0130\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0269 - mae: 0.0109 - val_loss: 0.0269 - val_mae: 0.0116\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0268 - mae: 0.0096 - val_loss: 0.0268 - val_mae: 0.0107\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0267 - mae: 0.0095 - val_loss: 0.0267 - val_mae: 0.0105\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0266 - mae: 0.0097 - val_loss: 0.0268 - val_mae: 0.0166\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0265 - mae: 0.0112 - val_loss: 0.0264 - val_mae: 0.0096\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0264 - mae: 0.0099 - val_loss: 0.0265 - val_mae: 0.0167\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0264 - mae: 0.0130 - val_loss: 0.0264 - val_mae: 0.0158\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0263 - mae: 0.0121 - val_loss: 0.0262 - val_mae: 0.0133\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0261 - mae: 0.0106 - val_loss: 0.0262 - val_mae: 0.0143\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0261 - mae: 0.0118 - val_loss: 0.0263 - val_mae: 0.0184\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0260 - mae: 0.0127 - val_loss: 0.0260 - val_mae: 0.0152\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0259 - mae: 0.0108 - val_loss: 0.0258 - val_mae: 0.0095\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0257 - mae: 0.0082 - val_loss: 0.0256 - val_mae: 0.0074\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0256 - mae: 0.0074 - val_loss: 0.0255 - val_mae: 0.0080\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0255 - mae: 0.0083 - val_loss: 0.0255 - val_mae: 0.0116\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0255 - mae: 0.0117 - val_loss: 0.0255 - val_mae: 0.0124\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0254 - mae: 0.0107 - val_loss: 0.0253 - val_mae: 0.0090\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0253 - mae: 0.0107 - val_loss: 0.0253 - val_mae: 0.0119\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0253 - mae: 0.0133 - val_loss: 0.0252 - val_mae: 0.0131\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0251 - mae: 0.0105 - val_loss: 0.0251 - val_mae: 0.0123\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0250 - mae: 0.0085 - val_loss: 0.0249 - val_mae: 0.0091\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0249 - mae: 0.0104 - val_loss: 0.0248 - val_mae: 0.0096\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0249 - mae: 0.0114 - val_loss: 0.0248 - val_mae: 0.0107\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0248 - mae: 0.0108 - val_loss: 0.0249 - val_mae: 0.0159\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0249 - mae: 0.0148 - val_loss: 0.0247 - val_mae: 0.0123\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0246 - mae: 0.0114 - val_loss: 0.0245 - val_mae: 0.0087\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.0080 - val_loss: 0.0244 - val_mae: 0.0082\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.0089 - val_loss: 0.0243 - val_mae: 0.0083\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.0091 - val_loss: 0.0243 - val_mae: 0.0107\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.0091 - val_loss: 0.0242 - val_mae: 0.0094\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.0096 - val_loss: 0.0244 - val_mae: 0.0155\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0241 - mae: 0.0114 - val_loss: 0.0241 - val_mae: 0.0121\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0240 - mae: 0.0102 - val_loss: 0.0241 - val_mae: 0.0147\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0239 - mae: 0.0108 - val_loss: 0.0240 - val_mae: 0.0130\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0238 - mae: 0.0096 - val_loss: 0.0237 - val_mae: 0.0076\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0237 - mae: 0.0084 - val_loss: 0.0236 - val_mae: 0.0075\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0237 - mae: 0.0096 - val_loss: 0.0235 - val_mae: 0.0065\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0235 - mae: 0.0076 - val_loss: 0.0238 - val_mae: 0.0161\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0235 - mae: 0.0109 - val_loss: 0.0234 - val_mae: 0.0076\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0234 - mae: 0.0079 - val_loss: 0.0233 - val_mae: 0.0074\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0233 - mae: 0.0077 - val_loss: 0.0233 - val_mae: 0.0086\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0233 - mae: 0.0091 - val_loss: 0.0232 - val_mae: 0.0089\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0232 - mae: 0.0104 - val_loss: 0.0233 - val_mae: 0.0155\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0232 - mae: 0.0131 - val_loss: 0.0231 - val_mae: 0.0101\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0230 - mae: 0.0086 - val_loss: 0.0229 - val_mae: 0.0066\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0230 - mae: 0.0097 - val_loss: 0.0230 - val_mae: 0.0116\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0229 - mae: 0.0098 - val_loss: 0.0230 - val_mae: 0.0144\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0228 - mae: 0.0102 - val_loss: 0.0228 - val_mae: 0.0109\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0228 - mae: 0.0112 - val_loss: 0.0227 - val_mae: 0.0095\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0227 - mae: 0.0107 - val_loss: 0.0226 - val_mae: 0.0080\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0226 - mae: 0.0082 - val_loss: 0.0229 - val_mae: 0.0161\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0229 - mae: 0.0173 - val_loss: 0.0238 - val_mae: 0.0341\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0231 - mae: 0.0225 - val_loss: 0.0233 - val_mae: 0.0278\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0227 - mae: 0.0163 - val_loss: 0.0223 - val_mae: 0.0080\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0225 - mae: 0.0137 - val_loss: 0.0230 - val_mae: 0.0247\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0225 - mae: 0.0157 - val_loss: 0.0221 - val_mae: 0.0065\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0222 - mae: 0.0097 - val_loss: 0.0221 - val_mae: 0.0084\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0221 - mae: 0.0079 - val_loss: 0.0220 - val_mae: 0.0080\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0220 - mae: 0.0089 - val_loss: 0.0225 - val_mae: 0.0203\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0222 - mae: 0.0156 - val_loss: 0.0219 - val_mae: 0.0078\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0220 - mae: 0.0117 - val_loss: 0.0220 - val_mae: 0.0146\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0219 - mae: 0.0114 - val_loss: 0.0218 - val_mae: 0.0093\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0218 - mae: 0.0099 - val_loss: 0.0217 - val_mae: 0.0070\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0216 - mae: 0.0071 - val_loss: 0.0216 - val_mae: 0.0080\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0217 - mae: 0.0108 - val_loss: 0.0215 - val_mae: 0.0059\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0216 - mae: 0.0108 - val_loss: 0.0216 - val_mae: 0.0117\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0215 - mae: 0.0098 - val_loss: 0.0214 - val_mae: 0.0090\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0214 - mae: 0.0087 - val_loss: 0.0214 - val_mae: 0.0114\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0214 - mae: 0.0096 - val_loss: 0.0214 - val_mae: 0.0118\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0213 - mae: 0.0102 - val_loss: 0.0212 - val_mae: 0.0076\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0212 - mae: 0.0077 - val_loss: 0.0215 - val_mae: 0.0168\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0212 - mae: 0.0112 - val_loss: 0.0212 - val_mae: 0.0104\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0212 - mae: 0.0118 - val_loss: 0.0211 - val_mae: 0.0095\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0212 - mae: 0.0136 - val_loss: 0.0210 - val_mae: 0.0089\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0210 - mae: 0.0102 - val_loss: 0.0209 - val_mae: 0.0073\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0209 - mae: 0.0096 - val_loss: 0.0209 - val_mae: 0.0104\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0208 - mae: 0.0082 - val_loss: 0.0209 - val_mae: 0.0119\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0209 - mae: 0.0123 - val_loss: 0.0208 - val_mae: 0.0117\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0208 - mae: 0.0098 - val_loss: 0.0207 - val_mae: 0.0106\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0208 - mae: 0.0130 - val_loss: 0.0207 - val_mae: 0.0109\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0208 - mae: 0.0142 - val_loss: 0.0209 - val_mae: 0.0181\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0207 - mae: 0.0142 - val_loss: 0.0207 - val_mae: 0.0151\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0207 - mae: 0.0145 - val_loss: 0.0208 - val_mae: 0.0175\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0207 - mae: 0.0152 - val_loss: 0.0209 - val_mae: 0.0238\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0206 - mae: 0.0161 - val_loss: 0.0211 - val_mae: 0.0248\n",
      "Epoch 184/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0206 - mae: 0.0167 - val_loss: 0.0202 - val_mae: 0.0082\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0203 - mae: 0.0092 - val_loss: 0.0202 - val_mae: 0.0106\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0202 - mae: 0.0100 - val_loss: 0.0202 - val_mae: 0.0117\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0201 - mae: 0.0095 - val_loss: 0.0200 - val_mae: 0.0062\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0200 - mae: 0.0069 - val_loss: 0.0200 - val_mae: 0.0070\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0200 - mae: 0.0072 - val_loss: 0.0199 - val_mae: 0.0078\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0199 - mae: 0.0083 - val_loss: 0.0199 - val_mae: 0.0091\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0199 - mae: 0.0105 - val_loss: 0.0198 - val_mae: 0.0090\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0200 - mae: 0.0130 - val_loss: 0.0207 - val_mae: 0.0286\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0203 - mae: 0.0206 - val_loss: 0.0197 - val_mae: 0.0085\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0202 - mae: 0.0199 - val_loss: 0.0202 - val_mae: 0.0217\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0198 - mae: 0.0127 - val_loss: 0.0204 - val_mae: 0.0238\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0200 - mae: 0.0173 - val_loss: 0.0202 - val_mae: 0.0206\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0200 - mae: 0.0198 - val_loss: 0.0203 - val_mae: 0.0226\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0199 - mae: 0.0189 - val_loss: 0.0195 - val_mae: 0.0108\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0198 - mae: 0.0180 - val_loss: 0.0197 - val_mae: 0.0182\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0195 - mae: 0.0140 - val_loss: 0.0203 - val_mae: 0.0301\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0197 - mae: 0.0181 - val_loss: 0.0194 - val_mae: 0.0119\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0194 - mae: 0.0125 - val_loss: 0.0194 - val_mae: 0.0127\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0193 - mae: 0.0114 - val_loss: 0.0194 - val_mae: 0.0145\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0192 - mae: 0.0110 - val_loss: 0.0192 - val_mae: 0.0113\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0195 - mae: 0.0170 - val_loss: 0.0204 - val_mae: 0.0335\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0197 - mae: 0.0230 - val_loss: 0.0190 - val_mae: 0.0097\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0192 - mae: 0.0145 - val_loss: 0.0191 - val_mae: 0.0124\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0190 - mae: 0.0116 - val_loss: 0.0190 - val_mae: 0.0098\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0189 - mae: 0.0085 - val_loss: 0.0188 - val_mae: 0.0061\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0188 - mae: 0.0068 - val_loss: 0.0188 - val_mae: 0.0064\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0187 - mae: 0.0066 - val_loss: 0.0187 - val_mae: 0.0061\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0187 - mae: 0.0069 - val_loss: 0.0187 - val_mae: 0.0081\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0187 - mae: 0.0076 - val_loss: 0.0186 - val_mae: 0.0072\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0076 - val_loss: 0.0186 - val_mae: 0.0089\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0185 - mae: 0.0070 - val_loss: 0.0186 - val_mae: 0.0095\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0096 - val_loss: 0.0186 - val_mae: 0.0124\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0083 - val_loss: 0.0185 - val_mae: 0.0108\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0184 - mae: 0.0083 - val_loss: 0.0184 - val_mae: 0.0091\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0184 - mae: 0.0082 - val_loss: 0.0184 - val_mae: 0.0099\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0183 - mae: 0.0090 - val_loss: 0.0184 - val_mae: 0.0117\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0183 - mae: 0.0089 - val_loss: 0.0182 - val_mae: 0.0060\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0082 - val_loss: 0.0182 - val_mae: 0.0094\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0099 - val_loss: 0.0183 - val_mae: 0.0135\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0099 - val_loss: 0.0186 - val_mae: 0.0228\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0168 - val_loss: 0.0181 - val_mae: 0.0108\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0183 - mae: 0.0151 - val_loss: 0.0184 - val_mae: 0.0193\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0181 - mae: 0.0113 - val_loss: 0.0179 - val_mae: 0.0084\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0179 - mae: 0.0087 - val_loss: 0.0180 - val_mae: 0.0109\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0179 - mae: 0.0098 - val_loss: 0.0189 - val_mae: 0.0292\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0185 - mae: 0.0218 - val_loss: 0.0178 - val_mae: 0.0102\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0179 - mae: 0.0121 - val_loss: 0.0179 - val_mae: 0.0156\n",
      "Epoch 232/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0178 - mae: 0.0109 - val_loss: 0.0178 - val_mae: 0.0113\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0177 - mae: 0.0082 - val_loss: 0.0178 - val_mae: 0.0138\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0100 - val_loss: 0.0179 - val_mae: 0.0172\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0177 - mae: 0.0133 - val_loss: 0.0175 - val_mae: 0.0088\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0176 - mae: 0.0116 - val_loss: 0.0175 - val_mae: 0.0071\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0175 - mae: 0.0078 - val_loss: 0.0175 - val_mae: 0.0098\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0175 - mae: 0.0116 - val_loss: 0.0175 - val_mae: 0.0125\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0176 - mae: 0.0140 - val_loss: 0.0174 - val_mae: 0.0113\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0098 - val_loss: 0.0174 - val_mae: 0.0116\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0100 - val_loss: 0.0173 - val_mae: 0.0098\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0172 - mae: 0.0086 - val_loss: 0.0176 - val_mae: 0.0170\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0175 - mae: 0.0154 - val_loss: 0.0172 - val_mae: 0.0119\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0173 - mae: 0.0130 - val_loss: 0.0174 - val_mae: 0.0170\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0173 - mae: 0.0142 - val_loss: 0.0183 - val_mae: 0.0314\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0174 - mae: 0.0161 - val_loss: 0.0171 - val_mae: 0.0094\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0171 - mae: 0.0114 - val_loss: 0.0170 - val_mae: 0.0082\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0169 - mae: 0.0084 - val_loss: 0.0171 - val_mae: 0.0131\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0170 - mae: 0.0106 - val_loss: 0.0170 - val_mae: 0.0116\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0170 - mae: 0.0114 - val_loss: 0.0169 - val_mae: 0.0097\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0168 - mae: 0.0086 - val_loss: 0.0167 - val_mae: 0.0069\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0167 - mae: 0.0074 - val_loss: 0.0167 - val_mae: 0.0065\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0167 - mae: 0.0081 - val_loss: 0.0167 - val_mae: 0.0098\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0167 - mae: 0.0099 - val_loss: 0.0178 - val_mae: 0.0273\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0171 - mae: 0.0202 - val_loss: 0.0166 - val_mae: 0.0093\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0172 - mae: 0.0199 - val_loss: 0.0177 - val_mae: 0.0274\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0243 - val_loss: 0.0167 - val_mae: 0.0154\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0168 - mae: 0.0158 - val_loss: 0.0166 - val_mae: 0.0143\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0166 - mae: 0.0140 - val_loss: 0.0168 - val_mae: 0.0165\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0166 - mae: 0.0145 - val_loss: 0.0165 - val_mae: 0.0113\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0166 - mae: 0.0157 - val_loss: 0.0169 - val_mae: 0.0220\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0166 - mae: 0.0160 - val_loss: 0.0163 - val_mae: 0.0098\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0163 - mae: 0.0093 - val_loss: 0.0164 - val_mae: 0.0139\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0163 - mae: 0.0096 - val_loss: 0.0162 - val_mae: 0.0068\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0162 - mae: 0.0076 - val_loss: 0.0161 - val_mae: 0.0063\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0161 - mae: 0.0079 - val_loss: 0.0161 - val_mae: 0.0087\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0160 - mae: 0.0063 - val_loss: 0.0161 - val_mae: 0.0081\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0160 - mae: 0.0069 - val_loss: 0.0160 - val_mae: 0.0073\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0160 - mae: 0.0095 - val_loss: 0.0160 - val_mae: 0.0102\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0160 - mae: 0.0090 - val_loss: 0.0160 - val_mae: 0.0104\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0160 - mae: 0.0117 - val_loss: 0.0159 - val_mae: 0.0108\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0076 - val_loss: 0.0158 - val_mae: 0.0063\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0158 - mae: 0.0074 - val_loss: 0.0158 - val_mae: 0.0063\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0158 - mae: 0.0069 - val_loss: 0.0157 - val_mae: 0.0063\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0157 - mae: 0.0067 - val_loss: 0.0157 - val_mae: 0.0073\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0157 - mae: 0.0069 - val_loss: 0.0157 - val_mae: 0.0109\n",
      "Epoch 277/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0076 - val_loss: 0.0156 - val_mae: 0.0089\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0156 - mae: 0.0080 - val_loss: 0.0156 - val_mae: 0.0110\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0156 - mae: 0.0087 - val_loss: 0.0156 - val_mae: 0.0094\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0114 - val_loss: 0.0157 - val_mae: 0.0158\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0156 - mae: 0.0124 - val_loss: 0.0155 - val_mae: 0.0104\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0155 - mae: 0.0093 - val_loss: 0.0160 - val_mae: 0.0202\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0156 - mae: 0.0130 - val_loss: 0.0155 - val_mae: 0.0131\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0155 - mae: 0.0119 - val_loss: 0.0154 - val_mae: 0.0107\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0153 - mae: 0.0085 - val_loss: 0.0154 - val_mae: 0.0118\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0154 - mae: 0.0115 - val_loss: 0.0155 - val_mae: 0.0140\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0156 - mae: 0.0160 - val_loss: 0.0174 - val_mae: 0.0401\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0390 - val_loss: 0.0162 - val_mae: 0.0283\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0166 - mae: 0.0303 - val_loss: 0.0181 - val_mae: 0.0423\n",
      "Epoch 290/1000\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.0195 - mae: 0.0534Restoring model weights from the end of the best epoch: 285.\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0167 - mae: 0.0320 - val_loss: 0.0164 - val_mae: 0.0313\n",
      "Epoch 290: early stopping\n",
      "Training für Fold 5...\n",
      "Epoch 1/1000\n",
      "10/10 [==============================] - 1s 19ms/step - loss: 0.7584 - mae: 0.6666 - val_loss: 0.3335 - val_mae: 0.4277\n",
      "Epoch 2/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2686 - mae: 0.3760 - val_loss: 0.2735 - val_mae: 0.3755\n",
      "Epoch 3/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2024 - mae: 0.2987 - val_loss: 0.2036 - val_mae: 0.3104\n",
      "Epoch 4/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1927 - mae: 0.2962 - val_loss: 0.2061 - val_mae: 0.3133\n",
      "Epoch 5/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1797 - mae: 0.2796 - val_loss: 0.1881 - val_mae: 0.2997\n",
      "Epoch 6/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1705 - mae: 0.2700 - val_loss: 0.1857 - val_mae: 0.2961\n",
      "Epoch 7/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1616 - mae: 0.2601 - val_loss: 0.1717 - val_mae: 0.2804\n",
      "Epoch 8/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1505 - mae: 0.2481 - val_loss: 0.1640 - val_mae: 0.2719\n",
      "Epoch 9/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1402 - mae: 0.2326 - val_loss: 0.1427 - val_mae: 0.2468\n",
      "Epoch 10/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1289 - mae: 0.2184 - val_loss: 0.1282 - val_mae: 0.2275\n",
      "Epoch 11/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1152 - mae: 0.2004 - val_loss: 0.1146 - val_mae: 0.2045\n",
      "Epoch 12/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1024 - mae: 0.1788 - val_loss: 0.0987 - val_mae: 0.1769\n",
      "Epoch 13/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0932 - mae: 0.1645 - val_loss: 0.0878 - val_mae: 0.1574\n",
      "Epoch 14/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0853 - mae: 0.1514 - val_loss: 0.0838 - val_mae: 0.1489\n",
      "Epoch 15/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0804 - mae: 0.1417 - val_loss: 0.0815 - val_mae: 0.1532\n",
      "Epoch 16/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0732 - mae: 0.1268 - val_loss: 0.0721 - val_mae: 0.1258\n",
      "Epoch 17/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0661 - mae: 0.1099 - val_loss: 0.0647 - val_mae: 0.1088\n",
      "Epoch 18/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0616 - mae: 0.0975 - val_loss: 0.0669 - val_mae: 0.1138\n",
      "Epoch 19/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0599 - mae: 0.0929 - val_loss: 0.0573 - val_mae: 0.0891\n",
      "Epoch 20/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0568 - mae: 0.0816 - val_loss: 0.0547 - val_mae: 0.0757\n",
      "Epoch 21/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0532 - mae: 0.0687 - val_loss: 0.0615 - val_mae: 0.0962\n",
      "Epoch 22/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0552 - mae: 0.0766 - val_loss: 0.0552 - val_mae: 0.0674\n",
      "Epoch 23/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0533 - mae: 0.0679 - val_loss: 0.0566 - val_mae: 0.0765\n",
      "Epoch 24/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0533 - mae: 0.0692 - val_loss: 0.0545 - val_mae: 0.0742\n",
      "Epoch 25/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0535 - mae: 0.0707 - val_loss: 0.0508 - val_mae: 0.0564\n",
      "Epoch 26/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0518 - mae: 0.0624 - val_loss: 0.0471 - val_mae: 0.0426\n",
      "Epoch 27/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0495 - mae: 0.0531 - val_loss: 0.0462 - val_mae: 0.0379\n",
      "Epoch 28/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0464 - mae: 0.0394 - val_loss: 0.0456 - val_mae: 0.0338\n",
      "Epoch 29/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0456 - mae: 0.0337 - val_loss: 0.0453 - val_mae: 0.0329\n",
      "Epoch 30/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0452 - mae: 0.0320 - val_loss: 0.0448 - val_mae: 0.0293\n",
      "Epoch 31/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0449 - mae: 0.0309 - val_loss: 0.0444 - val_mae: 0.0275\n",
      "Epoch 32/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0445 - mae: 0.0294 - val_loss: 0.0443 - val_mae: 0.0274\n",
      "Epoch 33/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0442 - mae: 0.0269 - val_loss: 0.0439 - val_mae: 0.0253\n",
      "Epoch 34/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0439 - mae: 0.0257 - val_loss: 0.0435 - val_mae: 0.0235\n",
      "Epoch 35/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0436 - mae: 0.0253 - val_loss: 0.0432 - val_mae: 0.0210\n",
      "Epoch 36/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0434 - mae: 0.0239 - val_loss: 0.0437 - val_mae: 0.0277\n",
      "Epoch 37/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0432 - mae: 0.0230 - val_loss: 0.0428 - val_mae: 0.0191\n",
      "Epoch 38/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0430 - mae: 0.0226 - val_loss: 0.0431 - val_mae: 0.0247\n",
      "Epoch 39/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0428 - mae: 0.0223 - val_loss: 0.0432 - val_mae: 0.0273\n",
      "Epoch 40/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0428 - mae: 0.0242 - val_loss: 0.0425 - val_mae: 0.0227\n",
      "Epoch 41/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0424 - mae: 0.0217 - val_loss: 0.0423 - val_mae: 0.0214\n",
      "Epoch 42/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0422 - mae: 0.0209 - val_loss: 0.0420 - val_mae: 0.0182\n",
      "Epoch 43/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0420 - mae: 0.0199 - val_loss: 0.0431 - val_mae: 0.0324\n",
      "Epoch 44/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0423 - mae: 0.0261 - val_loss: 0.0417 - val_mae: 0.0187\n",
      "Epoch 45/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0418 - mae: 0.0209 - val_loss: 0.0416 - val_mae: 0.0191\n",
      "Epoch 46/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0416 - mae: 0.0199 - val_loss: 0.0415 - val_mae: 0.0208\n",
      "Epoch 47/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0414 - mae: 0.0195 - val_loss: 0.0411 - val_mae: 0.0168\n",
      "Epoch 48/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0411 - mae: 0.0172 - val_loss: 0.0411 - val_mae: 0.0183\n",
      "Epoch 49/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0410 - mae: 0.0177 - val_loss: 0.0408 - val_mae: 0.0154\n",
      "Epoch 50/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0408 - mae: 0.0154 - val_loss: 0.0408 - val_mae: 0.0163\n",
      "Epoch 51/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0407 - mae: 0.0157 - val_loss: 0.0406 - val_mae: 0.0174\n",
      "Epoch 52/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0405 - mae: 0.0160 - val_loss: 0.0403 - val_mae: 0.0145\n",
      "Epoch 53/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0403 - mae: 0.0149 - val_loss: 0.0402 - val_mae: 0.0143\n",
      "Epoch 54/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0402 - mae: 0.0144 - val_loss: 0.0403 - val_mae: 0.0177\n",
      "Epoch 55/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0402 - mae: 0.0172 - val_loss: 0.0403 - val_mae: 0.0210\n",
      "Epoch 56/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0400 - mae: 0.0171 - val_loss: 0.0404 - val_mae: 0.0236\n",
      "Epoch 57/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0400 - mae: 0.0176 - val_loss: 0.0397 - val_mae: 0.0156\n",
      "Epoch 58/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0396 - mae: 0.0140 - val_loss: 0.0395 - val_mae: 0.0144\n",
      "Epoch 59/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0395 - mae: 0.0143 - val_loss: 0.0396 - val_mae: 0.0173\n",
      "Epoch 60/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0394 - mae: 0.0152 - val_loss: 0.0393 - val_mae: 0.0149\n",
      "Epoch 61/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0393 - mae: 0.0152 - val_loss: 0.0391 - val_mae: 0.0137\n",
      "Epoch 62/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0391 - mae: 0.0149 - val_loss: 0.0390 - val_mae: 0.0137\n",
      "Epoch 63/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0389 - mae: 0.0132 - val_loss: 0.0391 - val_mae: 0.0163\n",
      "Epoch 64/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0388 - mae: 0.0130 - val_loss: 0.0388 - val_mae: 0.0138\n",
      "Epoch 65/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0387 - mae: 0.0133 - val_loss: 0.0388 - val_mae: 0.0165\n",
      "Epoch 66/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0386 - mae: 0.0138 - val_loss: 0.0385 - val_mae: 0.0132\n",
      "Epoch 67/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0384 - mae: 0.0128 - val_loss: 0.0383 - val_mae: 0.0138\n",
      "Epoch 68/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0383 - mae: 0.0136 - val_loss: 0.0382 - val_mae: 0.0142\n",
      "Epoch 69/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0381 - mae: 0.0127 - val_loss: 0.0381 - val_mae: 0.0126\n",
      "Epoch 70/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0380 - mae: 0.0117 - val_loss: 0.0380 - val_mae: 0.0133\n",
      "Epoch 71/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0378 - mae: 0.0112 - val_loss: 0.0379 - val_mae: 0.0146\n",
      "Epoch 72/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0377 - mae: 0.0124 - val_loss: 0.0377 - val_mae: 0.0139\n",
      "Epoch 73/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0377 - mae: 0.0145 - val_loss: 0.0378 - val_mae: 0.0166\n",
      "Epoch 74/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0377 - mae: 0.0155 - val_loss: 0.0376 - val_mae: 0.0163\n",
      "Epoch 75/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0375 - mae: 0.0155 - val_loss: 0.0380 - val_mae: 0.0221\n",
      "Epoch 76/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0376 - mae: 0.0192 - val_loss: 0.0378 - val_mae: 0.0229\n",
      "Epoch 77/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0376 - mae: 0.0206 - val_loss: 0.0383 - val_mae: 0.0316\n",
      "Epoch 78/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0376 - mae: 0.0210 - val_loss: 0.0371 - val_mae: 0.0148\n",
      "Epoch 79/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0370 - mae: 0.0146 - val_loss: 0.0369 - val_mae: 0.0124\n",
      "Epoch 80/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0368 - mae: 0.0117 - val_loss: 0.0369 - val_mae: 0.0153\n",
      "Epoch 81/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0368 - mae: 0.0139 - val_loss: 0.0367 - val_mae: 0.0130\n",
      "Epoch 82/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0365 - mae: 0.0119 - val_loss: 0.0366 - val_mae: 0.0140\n",
      "Epoch 83/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0364 - mae: 0.0115 - val_loss: 0.0364 - val_mae: 0.0122\n",
      "Epoch 84/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0363 - mae: 0.0123 - val_loss: 0.0366 - val_mae: 0.0202\n",
      "Epoch 85/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0364 - mae: 0.0155 - val_loss: 0.0362 - val_mae: 0.0140\n",
      "Epoch 86/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0362 - mae: 0.0140 - val_loss: 0.0366 - val_mae: 0.0202\n",
      "Epoch 87/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0361 - mae: 0.0159 - val_loss: 0.0360 - val_mae: 0.0132\n",
      "Epoch 88/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0359 - mae: 0.0126 - val_loss: 0.0358 - val_mae: 0.0117\n",
      "Epoch 89/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0357 - mae: 0.0108 - val_loss: 0.0359 - val_mae: 0.0158\n",
      "Epoch 90/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0356 - mae: 0.0113 - val_loss: 0.0356 - val_mae: 0.0127\n",
      "Epoch 91/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0355 - mae: 0.0112 - val_loss: 0.0357 - val_mae: 0.0169\n",
      "Epoch 92/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0355 - mae: 0.0129 - val_loss: 0.0357 - val_mae: 0.0171\n",
      "Epoch 93/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0354 - mae: 0.0127 - val_loss: 0.0353 - val_mae: 0.0129\n",
      "Epoch 94/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0352 - mae: 0.0115 - val_loss: 0.0352 - val_mae: 0.0119\n",
      "Epoch 95/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0351 - mae: 0.0108 - val_loss: 0.0350 - val_mae: 0.0115\n",
      "Epoch 96/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0350 - mae: 0.0109 - val_loss: 0.0349 - val_mae: 0.0119\n",
      "Epoch 97/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0348 - mae: 0.0103 - val_loss: 0.0348 - val_mae: 0.0125\n",
      "Epoch 98/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0347 - mae: 0.0108 - val_loss: 0.0347 - val_mae: 0.0128\n",
      "Epoch 99/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0347 - mae: 0.0115 - val_loss: 0.0348 - val_mae: 0.0159\n",
      "Epoch 100/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0347 - mae: 0.0142 - val_loss: 0.0348 - val_mae: 0.0172\n",
      "Epoch 101/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0347 - mae: 0.0164 - val_loss: 0.0345 - val_mae: 0.0148\n",
      "Epoch 102/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0344 - mae: 0.0119 - val_loss: 0.0345 - val_mae: 0.0149\n",
      "Epoch 103/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0342 - mae: 0.0119 - val_loss: 0.0344 - val_mae: 0.0172\n",
      "Epoch 104/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0342 - mae: 0.0141 - val_loss: 0.0343 - val_mae: 0.0152\n",
      "Epoch 105/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0341 - mae: 0.0139 - val_loss: 0.0344 - val_mae: 0.0181\n",
      "Epoch 106/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0341 - mae: 0.0150 - val_loss: 0.0339 - val_mae: 0.0140\n",
      "Epoch 107/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0340 - mae: 0.0157 - val_loss: 0.0339 - val_mae: 0.0143\n",
      "Epoch 108/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0337 - mae: 0.0113 - val_loss: 0.0337 - val_mae: 0.0132\n",
      "Epoch 109/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0336 - mae: 0.0117 - val_loss: 0.0337 - val_mae: 0.0144\n",
      "Epoch 110/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0336 - mae: 0.0120 - val_loss: 0.0335 - val_mae: 0.0114\n",
      "Epoch 111/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0334 - mae: 0.0099 - val_loss: 0.0334 - val_mae: 0.0134\n",
      "Epoch 112/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0333 - mae: 0.0114 - val_loss: 0.0333 - val_mae: 0.0113\n",
      "Epoch 113/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0332 - mae: 0.0095 - val_loss: 0.0332 - val_mae: 0.0129\n",
      "Epoch 114/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0331 - mae: 0.0108 - val_loss: 0.0331 - val_mae: 0.0128\n",
      "Epoch 115/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0330 - mae: 0.0110 - val_loss: 0.0331 - val_mae: 0.0154\n",
      "Epoch 116/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0330 - mae: 0.0128 - val_loss: 0.0329 - val_mae: 0.0122\n",
      "Epoch 117/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0328 - mae: 0.0119 - val_loss: 0.0328 - val_mae: 0.0134\n",
      "Epoch 118/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0327 - mae: 0.0116 - val_loss: 0.0327 - val_mae: 0.0127\n",
      "Epoch 119/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0327 - mae: 0.0136 - val_loss: 0.0331 - val_mae: 0.0207\n",
      "Epoch 120/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0329 - mae: 0.0188 - val_loss: 0.0327 - val_mae: 0.0160\n",
      "Epoch 121/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0327 - mae: 0.0164 - val_loss: 0.0325 - val_mae: 0.0149\n",
      "Epoch 122/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0325 - mae: 0.0167 - val_loss: 0.0328 - val_mae: 0.0217\n",
      "Epoch 123/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0324 - mae: 0.0154 - val_loss: 0.0323 - val_mae: 0.0133\n",
      "Epoch 124/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0322 - mae: 0.0124 - val_loss: 0.0322 - val_mae: 0.0149\n",
      "Epoch 125/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0321 - mae: 0.0129 - val_loss: 0.0321 - val_mae: 0.0148\n",
      "Epoch 126/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0320 - mae: 0.0125 - val_loss: 0.0319 - val_mae: 0.0108\n",
      "Epoch 127/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0320 - mae: 0.0131 - val_loss: 0.0320 - val_mae: 0.0159\n",
      "Epoch 128/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0318 - mae: 0.0114 - val_loss: 0.0318 - val_mae: 0.0145\n",
      "Epoch 129/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0317 - mae: 0.0108 - val_loss: 0.0317 - val_mae: 0.0129\n",
      "Epoch 130/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0316 - mae: 0.0114 - val_loss: 0.0317 - val_mae: 0.0145\n",
      "Epoch 131/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0315 - mae: 0.0117 - val_loss: 0.0316 - val_mae: 0.0140\n",
      "Epoch 132/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0315 - mae: 0.0128 - val_loss: 0.0314 - val_mae: 0.0122\n",
      "Epoch 133/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0313 - mae: 0.0115 - val_loss: 0.0313 - val_mae: 0.0138\n",
      "Epoch 134/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0312 - mae: 0.0119 - val_loss: 0.0312 - val_mae: 0.0112\n",
      "Epoch 135/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0311 - mae: 0.0103 - val_loss: 0.0311 - val_mae: 0.0121\n",
      "Epoch 136/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0312 - mae: 0.0139 - val_loss: 0.0317 - val_mae: 0.0234\n",
      "Epoch 137/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0318 - mae: 0.0249 - val_loss: 0.0312 - val_mae: 0.0200\n",
      "Epoch 138/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0315 - mae: 0.0226 - val_loss: 0.0311 - val_mae: 0.0182\n",
      "Epoch 139/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0311 - mae: 0.0180 - val_loss: 0.0308 - val_mae: 0.0139\n",
      "Epoch 140/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0308 - mae: 0.0132 - val_loss: 0.0311 - val_mae: 0.0186\n",
      "Epoch 141/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0307 - mae: 0.0138 - val_loss: 0.0305 - val_mae: 0.0102\n",
      "Epoch 142/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0306 - mae: 0.0132 - val_loss: 0.0307 - val_mae: 0.0145\n",
      "Epoch 143/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0306 - mae: 0.0147 - val_loss: 0.0303 - val_mae: 0.0103\n",
      "Epoch 144/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0304 - mae: 0.0128 - val_loss: 0.0305 - val_mae: 0.0162\n",
      "Epoch 145/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0303 - mae: 0.0130 - val_loss: 0.0302 - val_mae: 0.0103\n",
      "Epoch 146/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0302 - mae: 0.0115 - val_loss: 0.0302 - val_mae: 0.0118\n",
      "Epoch 147/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0301 - mae: 0.0106 - val_loss: 0.0302 - val_mae: 0.0154\n",
      "Epoch 148/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0301 - mae: 0.0130 - val_loss: 0.0300 - val_mae: 0.0131\n",
      "Epoch 149/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0299 - mae: 0.0109 - val_loss: 0.0299 - val_mae: 0.0121\n",
      "Epoch 150/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0298 - mae: 0.0118 - val_loss: 0.0298 - val_mae: 0.0109\n",
      "Epoch 151/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0297 - mae: 0.0108 - val_loss: 0.0297 - val_mae: 0.0109\n",
      "Epoch 152/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0297 - mae: 0.0112 - val_loss: 0.0297 - val_mae: 0.0140\n",
      "Epoch 153/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0298 - mae: 0.0150 - val_loss: 0.0295 - val_mae: 0.0110\n",
      "Epoch 154/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0295 - mae: 0.0115 - val_loss: 0.0312 - val_mae: 0.0339\n",
      "Epoch 155/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0300 - mae: 0.0213 - val_loss: 0.0299 - val_mae: 0.0198\n",
      "Epoch 156/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0296 - mae: 0.0160 - val_loss: 0.0296 - val_mae: 0.0164\n",
      "Epoch 157/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0295 - mae: 0.0156 - val_loss: 0.0292 - val_mae: 0.0111\n",
      "Epoch 158/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0292 - mae: 0.0127 - val_loss: 0.0291 - val_mae: 0.0107\n",
      "Epoch 159/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0291 - mae: 0.0102 - val_loss: 0.0293 - val_mae: 0.0161\n",
      "Epoch 160/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0292 - mae: 0.0159 - val_loss: 0.0289 - val_mae: 0.0110\n",
      "Epoch 161/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0290 - mae: 0.0126 - val_loss: 0.0292 - val_mae: 0.0179\n",
      "Epoch 162/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0290 - mae: 0.0157 - val_loss: 0.0288 - val_mae: 0.0122\n",
      "Epoch 163/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0288 - mae: 0.0116 - val_loss: 0.0287 - val_mae: 0.0119\n",
      "Epoch 164/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0287 - mae: 0.0116 - val_loss: 0.0286 - val_mae: 0.0114\n",
      "Epoch 165/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0286 - mae: 0.0104 - val_loss: 0.0290 - val_mae: 0.0180\n",
      "Epoch 166/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0286 - mae: 0.0136 - val_loss: 0.0285 - val_mae: 0.0114\n",
      "Epoch 167/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0285 - mae: 0.0128 - val_loss: 0.0285 - val_mae: 0.0134\n",
      "Epoch 168/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0284 - mae: 0.0124 - val_loss: 0.0298 - val_mae: 0.0314\n",
      "Epoch 169/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0295 - mae: 0.0283 - val_loss: 0.0293 - val_mae: 0.0279\n",
      "Epoch 170/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0288 - mae: 0.0224 - val_loss: 0.0284 - val_mae: 0.0171\n",
      "Epoch 171/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0284 - mae: 0.0166 - val_loss: 0.0290 - val_mae: 0.0290\n",
      "Epoch 172/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0284 - mae: 0.0179 - val_loss: 0.0284 - val_mae: 0.0172\n",
      "Epoch 173/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0282 - mae: 0.0159 - val_loss: 0.0280 - val_mae: 0.0133\n",
      "Epoch 174/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0281 - mae: 0.0165 - val_loss: 0.0299 - val_mae: 0.0355\n",
      "Epoch 175/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0294 - mae: 0.0306 - val_loss: 0.0278 - val_mae: 0.0127\n",
      "Epoch 176/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0283 - mae: 0.0193 - val_loss: 0.0279 - val_mae: 0.0156\n",
      "Epoch 177/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0277 - mae: 0.0119 - val_loss: 0.0276 - val_mae: 0.0109\n",
      "Epoch 178/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0276 - mae: 0.0107 - val_loss: 0.0276 - val_mae: 0.0124\n",
      "Epoch 179/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0275 - mae: 0.0104 - val_loss: 0.0275 - val_mae: 0.0107\n",
      "Epoch 180/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0274 - mae: 0.0103 - val_loss: 0.0275 - val_mae: 0.0128\n",
      "Epoch 181/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0274 - mae: 0.0109 - val_loss: 0.0274 - val_mae: 0.0118\n",
      "Epoch 182/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0273 - mae: 0.0103 - val_loss: 0.0275 - val_mae: 0.0163\n",
      "Epoch 183/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0274 - mae: 0.0144 - val_loss: 0.0273 - val_mae: 0.0126\n",
      "Epoch 184/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0272 - mae: 0.0125 - val_loss: 0.0271 - val_mae: 0.0107\n",
      "Epoch 185/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0271 - mae: 0.0106 - val_loss: 0.0275 - val_mae: 0.0183\n",
      "Epoch 186/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0271 - mae: 0.0132 - val_loss: 0.0270 - val_mae: 0.0121\n",
      "Epoch 187/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0269 - mae: 0.0101 - val_loss: 0.0270 - val_mae: 0.0123\n",
      "Epoch 188/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0269 - mae: 0.0112 - val_loss: 0.0268 - val_mae: 0.0103\n",
      "Epoch 189/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0268 - mae: 0.0097 - val_loss: 0.0267 - val_mae: 0.0103\n",
      "Epoch 190/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0268 - mae: 0.0122 - val_loss: 0.0267 - val_mae: 0.0112\n",
      "Epoch 191/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0266 - mae: 0.0108 - val_loss: 0.0269 - val_mae: 0.0168\n",
      "Epoch 192/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0270 - mae: 0.0184 - val_loss: 0.0286 - val_mae: 0.0391\n",
      "Epoch 193/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0272 - mae: 0.0225 - val_loss: 0.0266 - val_mae: 0.0152\n",
      "Epoch 194/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0265 - mae: 0.0131 - val_loss: 0.0267 - val_mae: 0.0182\n",
      "Epoch 195/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0266 - mae: 0.0168 - val_loss: 0.0266 - val_mae: 0.0163\n",
      "Epoch 196/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0269 - mae: 0.0231 - val_loss: 0.0263 - val_mae: 0.0136\n",
      "Epoch 197/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0264 - mae: 0.0162 - val_loss: 0.0262 - val_mae: 0.0112\n",
      "Epoch 198/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0262 - mae: 0.0125 - val_loss: 0.0267 - val_mae: 0.0224\n",
      "Epoch 199/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0263 - mae: 0.0154 - val_loss: 0.0261 - val_mae: 0.0115\n",
      "Epoch 200/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0262 - mae: 0.0149 - val_loss: 0.0259 - val_mae: 0.0101\n",
      "Epoch 201/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0261 - mae: 0.0140 - val_loss: 0.0260 - val_mae: 0.0133\n",
      "Epoch 202/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0259 - mae: 0.0127 - val_loss: 0.0261 - val_mae: 0.0162\n",
      "Epoch 203/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0260 - mae: 0.0150 - val_loss: 0.0259 - val_mae: 0.0133\n",
      "Epoch 204/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0259 - mae: 0.0136 - val_loss: 0.0258 - val_mae: 0.0147\n",
      "Epoch 205/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0257 - mae: 0.0117 - val_loss: 0.0256 - val_mae: 0.0113\n",
      "Epoch 206/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0256 - mae: 0.0106 - val_loss: 0.0256 - val_mae: 0.0138\n",
      "Epoch 207/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0255 - mae: 0.0098 - val_loss: 0.0255 - val_mae: 0.0101\n",
      "Epoch 208/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0254 - mae: 0.0095 - val_loss: 0.0254 - val_mae: 0.0110\n",
      "Epoch 209/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0254 - mae: 0.0116 - val_loss: 0.0254 - val_mae: 0.0126\n",
      "Epoch 210/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0255 - mae: 0.0156 - val_loss: 0.0256 - val_mae: 0.0174\n",
      "Epoch 211/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0254 - mae: 0.0150 - val_loss: 0.0252 - val_mae: 0.0120\n",
      "Epoch 212/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0254 - mae: 0.0153 - val_loss: 0.0256 - val_mae: 0.0195\n",
      "Epoch 213/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0253 - mae: 0.0148 - val_loss: 0.0250 - val_mae: 0.0100\n",
      "Epoch 214/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0252 - mae: 0.0127 - val_loss: 0.0250 - val_mae: 0.0132\n",
      "Epoch 215/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0250 - mae: 0.0126 - val_loss: 0.0253 - val_mae: 0.0169\n",
      "Epoch 216/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0251 - mae: 0.0155 - val_loss: 0.0251 - val_mae: 0.0152\n",
      "Epoch 217/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0250 - mae: 0.0139 - val_loss: 0.0248 - val_mae: 0.0098\n",
      "Epoch 218/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0248 - mae: 0.0099 - val_loss: 0.0248 - val_mae: 0.0135\n",
      "Epoch 219/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0247 - mae: 0.0104 - val_loss: 0.0246 - val_mae: 0.0103\n",
      "Epoch 220/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0246 - mae: 0.0090 - val_loss: 0.0246 - val_mae: 0.0102\n",
      "Epoch 221/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0246 - mae: 0.0118 - val_loss: 0.0248 - val_mae: 0.0163\n",
      "Epoch 222/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0246 - mae: 0.0129 - val_loss: 0.0246 - val_mae: 0.0124\n",
      "Epoch 223/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0245 - mae: 0.0130 - val_loss: 0.0244 - val_mae: 0.0102\n",
      "Epoch 224/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.0101 - val_loss: 0.0243 - val_mae: 0.0095\n",
      "Epoch 225/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.0096 - val_loss: 0.0243 - val_mae: 0.0112\n",
      "Epoch 226/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.0109 - val_loss: 0.0246 - val_mae: 0.0195\n",
      "Epoch 227/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.0129 - val_loss: 0.0242 - val_mae: 0.0110\n",
      "Epoch 228/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.0114 - val_loss: 0.0241 - val_mae: 0.0104\n",
      "Epoch 229/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0240 - mae: 0.0099 - val_loss: 0.0244 - val_mae: 0.0175\n",
      "Epoch 230/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.0172 - val_loss: 0.0245 - val_mae: 0.0199\n",
      "Epoch 231/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0243 - mae: 0.0180 - val_loss: 0.0241 - val_mae: 0.0136\n",
      "Epoch 232/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0242 - mae: 0.0183 - val_loss: 0.0240 - val_mae: 0.0144\n",
      "Epoch 233/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0240 - mae: 0.0144 - val_loss: 0.0239 - val_mae: 0.0142\n",
      "Epoch 234/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0240 - mae: 0.0170 - val_loss: 0.0240 - val_mae: 0.0175\n",
      "Epoch 235/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0239 - mae: 0.0158 - val_loss: 0.0237 - val_mae: 0.0125\n",
      "Epoch 236/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0238 - mae: 0.0166 - val_loss: 0.0237 - val_mae: 0.0140\n",
      "Epoch 237/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0236 - mae: 0.0118 - val_loss: 0.0236 - val_mae: 0.0136\n",
      "Epoch 238/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0235 - mae: 0.0115 - val_loss: 0.0234 - val_mae: 0.0103\n",
      "Epoch 239/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0236 - mae: 0.0155 - val_loss: 0.0242 - val_mae: 0.0252\n",
      "Epoch 240/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0237 - mae: 0.0180 - val_loss: 0.0233 - val_mae: 0.0108\n",
      "Epoch 241/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0244 - mae: 0.0269 - val_loss: 0.0292 - val_mae: 0.0664\n",
      "Epoch 242/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0261 - mae: 0.0408 - val_loss: 0.0278 - val_mae: 0.0518\n",
      "Epoch 243/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0257 - mae: 0.0392 - val_loss: 0.0243 - val_mae: 0.0275\n",
      "Epoch 244/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0241 - mae: 0.0253 - val_loss: 0.0236 - val_mae: 0.0197\n",
      "Epoch 245/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0233 - mae: 0.0159 - val_loss: 0.0231 - val_mae: 0.0118\n",
      "Epoch 246/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0231 - mae: 0.0126 - val_loss: 0.0229 - val_mae: 0.0105\n",
      "Epoch 247/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0229 - mae: 0.0095 - val_loss: 0.0229 - val_mae: 0.0121\n",
      "Epoch 248/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0228 - mae: 0.0095 - val_loss: 0.0230 - val_mae: 0.0135\n",
      "Epoch 249/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0228 - mae: 0.0110 - val_loss: 0.0227 - val_mae: 0.0097\n",
      "Epoch 250/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0227 - mae: 0.0088 - val_loss: 0.0229 - val_mae: 0.0133\n",
      "Epoch 251/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0227 - mae: 0.0107 - val_loss: 0.0226 - val_mae: 0.0099\n",
      "Epoch 252/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0226 - mae: 0.0095 - val_loss: 0.0226 - val_mae: 0.0103\n",
      "Epoch 253/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0225 - mae: 0.0091 - val_loss: 0.0226 - val_mae: 0.0131\n",
      "Epoch 254/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0226 - mae: 0.0118 - val_loss: 0.0226 - val_mae: 0.0136\n",
      "Epoch 255/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0225 - mae: 0.0104 - val_loss: 0.0226 - val_mae: 0.0148\n",
      "Epoch 256/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0226 - mae: 0.0163 - val_loss: 0.0225 - val_mae: 0.0143\n",
      "Epoch 257/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0225 - mae: 0.0158 - val_loss: 0.0229 - val_mae: 0.0217\n",
      "Epoch 258/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0226 - mae: 0.0170 - val_loss: 0.0226 - val_mae: 0.0167\n",
      "Epoch 259/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0224 - mae: 0.0140 - val_loss: 0.0222 - val_mae: 0.0097\n",
      "Epoch 260/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0222 - mae: 0.0115 - val_loss: 0.0226 - val_mae: 0.0170\n",
      "Epoch 261/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0223 - mae: 0.0150 - val_loss: 0.0230 - val_mae: 0.0265\n",
      "Epoch 262/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0226 - mae: 0.0201 - val_loss: 0.0235 - val_mae: 0.0299\n",
      "Epoch 263/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0230 - mae: 0.0277 - val_loss: 0.0224 - val_mae: 0.0189\n",
      "Epoch 264/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0224 - mae: 0.0194 - val_loss: 0.0219 - val_mae: 0.0105\n",
      "Epoch 265/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0219 - mae: 0.0118 - val_loss: 0.0221 - val_mae: 0.0159\n",
      "Epoch 266/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0221 - mae: 0.0153 - val_loss: 0.0224 - val_mae: 0.0192\n",
      "Epoch 267/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0223 - mae: 0.0196 - val_loss: 0.0217 - val_mae: 0.0095\n",
      "Epoch 268/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0218 - mae: 0.0127 - val_loss: 0.0220 - val_mae: 0.0184\n",
      "Epoch 269/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0219 - mae: 0.0164 - val_loss: 0.0217 - val_mae: 0.0129\n",
      "Epoch 270/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0217 - mae: 0.0126 - val_loss: 0.0216 - val_mae: 0.0117\n",
      "Epoch 271/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0215 - mae: 0.0103 - val_loss: 0.0217 - val_mae: 0.0134\n",
      "Epoch 272/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0215 - mae: 0.0118 - val_loss: 0.0215 - val_mae: 0.0113\n",
      "Epoch 273/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0214 - mae: 0.0111 - val_loss: 0.0217 - val_mae: 0.0172\n",
      "Epoch 274/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0216 - mae: 0.0154 - val_loss: 0.0219 - val_mae: 0.0232\n",
      "Epoch 275/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0216 - mae: 0.0170 - val_loss: 0.0214 - val_mae: 0.0142\n",
      "Epoch 276/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0213 - mae: 0.0107 - val_loss: 0.0216 - val_mae: 0.0194\n",
      "Epoch 277/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0214 - mae: 0.0138 - val_loss: 0.0214 - val_mae: 0.0163\n",
      "Epoch 278/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0213 - mae: 0.0134 - val_loss: 0.0213 - val_mae: 0.0136\n",
      "Epoch 279/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0211 - mae: 0.0108 - val_loss: 0.0211 - val_mae: 0.0101\n",
      "Epoch 280/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0210 - mae: 0.0091 - val_loss: 0.0210 - val_mae: 0.0093\n",
      "Epoch 281/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0209 - mae: 0.0095 - val_loss: 0.0209 - val_mae: 0.0087\n",
      "Epoch 282/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0209 - mae: 0.0088 - val_loss: 0.0209 - val_mae: 0.0090\n",
      "Epoch 283/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0208 - mae: 0.0082 - val_loss: 0.0208 - val_mae: 0.0088\n",
      "Epoch 284/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0208 - mae: 0.0091 - val_loss: 0.0207 - val_mae: 0.0096\n",
      "Epoch 285/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0207 - mae: 0.0089 - val_loss: 0.0207 - val_mae: 0.0085\n",
      "Epoch 286/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0207 - mae: 0.0102 - val_loss: 0.0207 - val_mae: 0.0111\n",
      "Epoch 287/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0207 - mae: 0.0126 - val_loss: 0.0207 - val_mae: 0.0127\n",
      "Epoch 288/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0207 - mae: 0.0135 - val_loss: 0.0205 - val_mae: 0.0091\n",
      "Epoch 289/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0206 - mae: 0.0119 - val_loss: 0.0205 - val_mae: 0.0116\n",
      "Epoch 290/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0205 - mae: 0.0115 - val_loss: 0.0205 - val_mae: 0.0122\n",
      "Epoch 291/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0204 - mae: 0.0096 - val_loss: 0.0203 - val_mae: 0.0084\n",
      "Epoch 292/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0203 - mae: 0.0084 - val_loss: 0.0203 - val_mae: 0.0088\n",
      "Epoch 293/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0202 - mae: 0.0082 - val_loss: 0.0203 - val_mae: 0.0108\n",
      "Epoch 294/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0202 - mae: 0.0087 - val_loss: 0.0202 - val_mae: 0.0097\n",
      "Epoch 295/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0202 - mae: 0.0093 - val_loss: 0.0201 - val_mae: 0.0084\n",
      "Epoch 296/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0201 - mae: 0.0085 - val_loss: 0.0201 - val_mae: 0.0086\n",
      "Epoch 297/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0200 - mae: 0.0090 - val_loss: 0.0200 - val_mae: 0.0088\n",
      "Epoch 298/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0200 - mae: 0.0087 - val_loss: 0.0199 - val_mae: 0.0085\n",
      "Epoch 299/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0199 - mae: 0.0086 - val_loss: 0.0199 - val_mae: 0.0095\n",
      "Epoch 300/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0199 - mae: 0.0083 - val_loss: 0.0199 - val_mae: 0.0098\n",
      "Epoch 301/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0198 - mae: 0.0083 - val_loss: 0.0198 - val_mae: 0.0093\n",
      "Epoch 302/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0198 - mae: 0.0081 - val_loss: 0.0197 - val_mae: 0.0084\n",
      "Epoch 303/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0197 - mae: 0.0096 - val_loss: 0.0197 - val_mae: 0.0090\n",
      "Epoch 304/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0197 - mae: 0.0084 - val_loss: 0.0196 - val_mae: 0.0094\n",
      "Epoch 305/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0196 - mae: 0.0081 - val_loss: 0.0196 - val_mae: 0.0097\n",
      "Epoch 306/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0195 - mae: 0.0078 - val_loss: 0.0195 - val_mae: 0.0083\n",
      "Epoch 307/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0195 - mae: 0.0086 - val_loss: 0.0197 - val_mae: 0.0133\n",
      "Epoch 308/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0196 - mae: 0.0122 - val_loss: 0.0195 - val_mae: 0.0099\n",
      "Epoch 309/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0194 - mae: 0.0089 - val_loss: 0.0195 - val_mae: 0.0128\n",
      "Epoch 310/1000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0194 - mae: 0.0094 - val_loss: 0.0193 - val_mae: 0.0080\n",
      "Epoch 311/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0193 - mae: 0.0084 - val_loss: 0.0199 - val_mae: 0.0226\n",
      "Epoch 312/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0199 - mae: 0.0215 - val_loss: 0.0203 - val_mae: 0.0251\n",
      "Epoch 313/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0208 - mae: 0.0335 - val_loss: 0.0194 - val_mae: 0.0147\n",
      "Epoch 314/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0200 - mae: 0.0234 - val_loss: 0.0199 - val_mae: 0.0256\n",
      "Epoch 315/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0196 - mae: 0.0186 - val_loss: 0.0191 - val_mae: 0.0114\n",
      "Epoch 316/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0191 - mae: 0.0102 - val_loss: 0.0190 - val_mae: 0.0103\n",
      "Epoch 317/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0190 - mae: 0.0096 - val_loss: 0.0189 - val_mae: 0.0082\n",
      "Epoch 318/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0189 - mae: 0.0081 - val_loss: 0.0189 - val_mae: 0.0078\n",
      "Epoch 319/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0188 - mae: 0.0075 - val_loss: 0.0189 - val_mae: 0.0119\n",
      "Epoch 320/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0189 - mae: 0.0107 - val_loss: 0.0188 - val_mae: 0.0081\n",
      "Epoch 321/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0188 - mae: 0.0095 - val_loss: 0.0195 - val_mae: 0.0243\n",
      "Epoch 322/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0190 - mae: 0.0147 - val_loss: 0.0191 - val_mae: 0.0170\n",
      "Epoch 323/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0189 - mae: 0.0158 - val_loss: 0.0187 - val_mae: 0.0104\n",
      "Epoch 324/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0187 - mae: 0.0112 - val_loss: 0.0186 - val_mae: 0.0101\n",
      "Epoch 325/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0186 - mae: 0.0119 - val_loss: 0.0185 - val_mae: 0.0094\n",
      "Epoch 326/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0185 - mae: 0.0091 - val_loss: 0.0184 - val_mae: 0.0077\n",
      "Epoch 327/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0184 - mae: 0.0083 - val_loss: 0.0184 - val_mae: 0.0080\n",
      "Epoch 328/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0087 - val_loss: 0.0185 - val_mae: 0.0119\n",
      "Epoch 329/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0184 - mae: 0.0111 - val_loss: 0.0183 - val_mae: 0.0085\n",
      "Epoch 330/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0183 - mae: 0.0097 - val_loss: 0.0183 - val_mae: 0.0088\n",
      "Epoch 331/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0183 - mae: 0.0108 - val_loss: 0.0183 - val_mae: 0.0103\n",
      "Epoch 332/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0183 - mae: 0.0118 - val_loss: 0.0181 - val_mae: 0.0085\n",
      "Epoch 333/1000\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0183 - mae: 0.0137 - val_loss: 0.0183 - val_mae: 0.0126\n",
      "Epoch 334/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0184 - mae: 0.0168 - val_loss: 0.0185 - val_mae: 0.0169\n",
      "Epoch 335/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0182 - mae: 0.0141 - val_loss: 0.0181 - val_mae: 0.0118\n",
      "Epoch 336/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0182 - mae: 0.0140 - val_loss: 0.0180 - val_mae: 0.0108\n",
      "Epoch 337/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0180 - mae: 0.0116 - val_loss: 0.0180 - val_mae: 0.0113\n",
      "Epoch 338/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0148 - val_loss: 0.0185 - val_mae: 0.0207\n",
      "Epoch 339/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0182 - mae: 0.0163 - val_loss: 0.0180 - val_mae: 0.0134\n",
      "Epoch 340/1000\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0179 - mae: 0.0115 - val_loss: 0.0178 - val_mae: 0.0092\n",
      "Epoch 341/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0178 - mae: 0.0114 - val_loss: 0.0177 - val_mae: 0.0090\n",
      "Epoch 342/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0177 - mae: 0.0089 - val_loss: 0.0177 - val_mae: 0.0102\n",
      "Epoch 343/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0177 - mae: 0.0094 - val_loss: 0.0176 - val_mae: 0.0078\n",
      "Epoch 344/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0083 - val_loss: 0.0175 - val_mae: 0.0079\n",
      "Epoch 345/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0175 - mae: 0.0069 - val_loss: 0.0178 - val_mae: 0.0159\n",
      "Epoch 346/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0176 - mae: 0.0111 - val_loss: 0.0175 - val_mae: 0.0094\n",
      "Epoch 347/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0174 - mae: 0.0081 - val_loss: 0.0177 - val_mae: 0.0161\n",
      "Epoch 348/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0176 - mae: 0.0139 - val_loss: 0.0174 - val_mae: 0.0104\n",
      "Epoch 349/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0101 - val_loss: 0.0175 - val_mae: 0.0126\n",
      "Epoch 350/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0122 - val_loss: 0.0176 - val_mae: 0.0166\n",
      "Epoch 351/1000\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0176 - mae: 0.0172 - val_loss: 0.0176 - val_mae: 0.0193\n",
      "Epoch 352/1000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0174 - mae: 0.0149 - val_loss: 0.0179 - val_mae: 0.0232\n",
      "Epoch 353/1000\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.0179 - mae: 0.0255Restoring model weights from the end of the best epoch: 348.\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0176 - mae: 0.0177 - val_loss: 0.0176 - val_mae: 0.0225\n",
      "Epoch 353: early stopping\n",
      "Durchschnittlicher Validation Loss: 0.020762568898499013\n",
      "Durchschnittlicher Validation MAE: 0.007318311743438244\n"
     ]
    }
   ],
   "source": [
    "# Initialisiere Listen, um Ergebnisse zu speichern\n",
    "val_loss_results = []\n",
    "val_mae_results = []\n",
    "\n",
    "# Funktion, um das Modell zu erstellen\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "                Dense(136, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "            \n",
    "                Dense(168, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "                \n",
    "                Dense(24, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "                \n",
    "                Dense(1 , activation = 'linear')\n",
    "\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-Validation Konfiguration\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Leistungsüberwachung\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_scaled[train_index], y_train_scaled[val_index]\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "    print(f'Training für Fold {fold_no}...')\n",
    "    history = model.fit(X_train_fold, y_train_fold, batch_size=75, epochs=1000, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    # Speichere die Ergebnisse des aktuellen Folds\n",
    "    val_loss_results.append(min(history.history['val_loss']))\n",
    "    val_mae_results.append(min(history.history['val_mae']))\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "# Berechne den Durchschnitt über alle Folds\n",
    "average_val_loss = np.mean(val_loss_results)\n",
    "average_val_mae = np.mean(val_mae_results)\n",
    "\n",
    "# Gib die durchschnittlichen Ergebnisse aus\n",
    "print(f'Durchschnittlicher Validation Loss: {average_val_loss}')\n",
    "print(f'Durchschnittlicher Validation MAE: {average_val_mae}')\n",
    "\n",
    "# Umwandeln der Listen in Pandas DataFrames\n",
    "val_loss_df = pd.DataFrame(val_loss_results, columns=['Validation Loss'])\n",
    "val_mae_df = pd.DataFrame(val_mae_results, columns=['Validation MAE'])\n",
    "\n",
    "# Speichern der DataFrames in CSV-Dateien\n",
    "val_loss_df.to_csv('val_loss_results_D4_3.csv', index=False)\n",
    "val_mae_df.to_csv('val_mae_results_D4_3.csv', index=False)\n",
    "\n",
    "# Gib die durchschnittlichen Ergebnisse aus\n",
    "print(f'Durchschnittlicher Validation Loss: {average_val_loss}')\n",
    "print(f'Durchschnittlicher Validation MAE: {average_val_mae}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T10:24:05.873831700Z",
     "start_time": "2024-03-18T10:22:55.663004300Z"
    }
   },
   "id": "9e0eeb55ab86239f"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 0s - loss: 0.0325 - mae: 0.0137 - 28ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.03253410384058952, 0.013650050386786461]"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test_scaled, y_test_scaled, verbose=2)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T11:10:50.925141500Z",
     "start_time": "2024-03-15T11:10:50.856659Z"
    }
   },
   "id": "4b02697cbcecd185"
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bsp. Predicted: [1133.3977] Actual: [1148.8] \n",
      "Durchschnittliche Abweichung (MAE): [12.90284485]\n"
     ]
    }
   ],
   "source": [
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Führen Sie die Rücktransformation der skalierten Werte durch\n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[100]} Actual: {original_actual_values[100]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "\n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "\n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")\n",
    "\n",
    "# Berechnung des MAPE\n",
    "errors = np.abs((original_actual_values - original_predicted_values) / original_actual_values)\n",
    "mape = np.mean(errors) * 100\n",
    "print(mape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T11:10:51.053476900Z",
     "start_time": "2024-03-15T11:10:50.925141500Z"
    }
   },
   "id": "a402d28abbd82f60"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: [0.9970775]\n"
     ]
    }
   ],
   "source": [
    "def calculate_r_squared(predicted, actual):\n",
    "    # Berechnung des Mittelwerts der tatsächlichen Werte\n",
    "    mean_actual = sum(actual) / len(actual)\n",
    "    \n",
    "    # Berechnung der totalen Summe der Quadrate (SST)\n",
    "    sst = sum((x - mean_actual) ** 2 for x in actual)\n",
    "    \n",
    "    # Berechnung der Summe der Quadrate der Residuen (SSE)\n",
    "    sse = sum((actual[i] - predicted[i]) ** 2 for i in range(len(actual)))\n",
    "    \n",
    "    # Berechnung des R^2-Wertes\n",
    "    r_squared = 1 - (sse / sst)\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "# Berechnung von R^2 mit den bereitgestellten Listen\n",
    "r_squared = calculate_r_squared(list1, list2)\n",
    "\n",
    "print(f\"R^2: {r_squared}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T11:10:51.063181600Z",
     "start_time": "2024-03-15T11:10:51.052476200Z"
    }
   },
   "id": "2820df0b9f03b675"
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse = history.history['loss']\n",
    "val_mse = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(mse) + 1)\n",
    "\n",
    "# MSE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mse, 'r', label='Training MSE')\n",
    "plt.plot(epochs, val_mse, 'b', label='Validation MSE')\n",
    "plt.title('Training and Validation MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.savefig('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Bilder Diplomarbeit/MSE_NeuroNetz/MSE_NeuroNetz_D4_3')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T11:10:51.265221500Z",
     "start_time": "2024-03-15T11:10:51.057754300Z"
    }
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def build_model(learning_rate=0.001, activation='relu', regularization=0.0001, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(136, activation=activation, input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(168, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(24, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Verwenden Sie eine Funktion, um das Modell zu instanziieren, für scikit-learn Wrapper\n",
    "model = KerasRegressor(model=build_model, verbose=2)\n",
    "\n",
    "# Anpassung der Parameter im param_grid\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'model__regularization': [0.001, 0.0001, 0.00001],\n",
    "    'fit__batch_size': [10, 25, 50, 75],\n",
    "    'fit__epochs': [50],\n",
    "    'model__dropout_rate' : [0.0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "# Hinweis: Stellen Sie sicher, dass Ihre Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "# Beste Parameter und Score ausgeben\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "print(\"Beste Genauigkeit:\", grid_search.best_score_)\n",
    "\n",
    "with open(\"Gridsearch_D4_3.txt\", \"w\") as f:\n",
    "    f.write(f\"Beste Parameter: {grid_search.best_params_}\\n\")\n",
    "    f.write(f\"Beste Genauigkeit: {grid_search.best_score_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-15T11:10:51.270610300Z",
     "start_time": "2024-03-15T11:10:51.265221500Z"
    }
   },
   "id": "7464a951f44a07ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b0518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.091462200Z",
     "start_time": "2024-03-14T16:01:30.403218800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ff61b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.111461300Z",
     "start_time": "2024-03-14T16:01:35.093461200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n0           0.0000      -0.00200        500   7000   9000      669.05\n1           0.0000      -0.00192        500   7000   9000      724.42\n2           0.0000      -0.00184        500   7000   9000      779.83\n3           0.0000      -0.00176        500   7000   9000      835.21\n4           0.0000      -0.00168        500   7000   9000      890.44\n...            ...           ...        ...    ...    ...         ...\n1066        0.0024       0.00168        500   7000   9000      775.40\n1067        0.0024       0.00176        500   7000   9000      715.43\n1068        0.0024       0.00184        500   7000   9000      645.85\n1069        0.0024       0.00192        500   7000   9000      585.87\n1070        0.0024       0.00200        500   7000   9000      574.64\n\n[1071 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00192</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>724.42</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00184</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>779.83</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00176</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>835.21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00168</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>890.44</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1066</th>\n      <td>0.0024</td>\n      <td>0.00168</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>775.40</td>\n    </tr>\n    <tr>\n      <th>1067</th>\n      <td>0.0024</td>\n      <td>0.00176</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>715.43</td>\n    </tr>\n    <tr>\n      <th>1068</th>\n      <td>0.0024</td>\n      <td>0.00184</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>645.85</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.0024</td>\n      <td>0.00192</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>585.87</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.0024</td>\n      <td>0.00200</td>\n      <td>500</td>\n      <td>7000</td>\n      <td>9000</td>\n      <td>574.64</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/TPath_300_finish_data.pkl')\n",
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/Finish_D4_I7000_F9000/TPath_500_finish_data_D4.pkl')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966e3c74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.124559700Z",
     "start_time": "2024-03-14T16:01:35.110460800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Temperatur\n0           0.0000      -0.00200      669.05\n1           0.0000      -0.00192      724.42\n2           0.0000      -0.00184      779.83\n3           0.0000      -0.00176      835.21\n4           0.0000      -0.00168      890.44\n...            ...           ...         ...\n1066        0.0024       0.00168      775.40\n1067        0.0024       0.00176      715.43\n1068        0.0024       0.00184      645.85\n1069        0.0024       0.00192      585.87\n1070        0.0024       0.00200      574.64\n\n[1071 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>669.05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00192</td>\n      <td>724.42</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00184</td>\n      <td>779.83</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00176</td>\n      <td>835.21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00168</td>\n      <td>890.44</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1066</th>\n      <td>0.0024</td>\n      <td>0.00168</td>\n      <td>775.40</td>\n    </tr>\n    <tr>\n      <th>1067</th>\n      <td>0.0024</td>\n      <td>0.00176</td>\n      <td>715.43</td>\n    </tr>\n    <tr>\n      <th>1068</th>\n      <td>0.0024</td>\n      <td>0.00184</td>\n      <td>645.85</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.0024</td>\n      <td>0.00192</td>\n      <td>585.87</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.0024</td>\n      <td>0.00200</td>\n      <td>574.64</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.drop(data.columns[2:5], axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8783d1d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.133266300Z",
     "start_time": "2024-03-14T16:01:35.119556500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      X-Koordinate  Y-Koordinate  Temperatur\n",
      "184        0.00036       0.00048     1441.10\n",
      "572        0.00132      -0.00112     1225.70\n",
      "309        0.00072      -0.00176      832.88\n",
      "930        0.00216      -0.00104     1158.00\n",
      "711        0.00156       0.00184      642.96\n",
      "...            ...           ...         ...\n",
      "330        0.00072      -0.00008     1500.20\n",
      "466        0.00108      -0.00144     1045.00\n",
      "121        0.00024      -0.00048     1484.90\n",
      "1044       0.00240      -0.00008     1259.10\n",
      "860        0.00192       0.00152      861.75\n",
      "\n",
      "[1071 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate  Temperatur\n0          0.00036       0.00048     1441.10\n1          0.00132      -0.00112     1225.70\n2          0.00072      -0.00176      832.88\n3          0.00216      -0.00104     1158.00\n4          0.00156       0.00184      642.96\n...            ...           ...         ...\n1066       0.00072      -0.00008     1500.20\n1067       0.00108      -0.00144     1045.00\n1068       0.00024      -0.00048     1484.90\n1069       0.00240      -0.00008     1259.10\n1070       0.00192       0.00152      861.75\n\n[1071 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00036</td>\n      <td>0.00048</td>\n      <td>1441.10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00132</td>\n      <td>-0.00112</td>\n      <td>1225.70</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00072</td>\n      <td>-0.00176</td>\n      <td>832.88</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00216</td>\n      <td>-0.00104</td>\n      <td>1158.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00156</td>\n      <td>0.00184</td>\n      <td>642.96</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1066</th>\n      <td>0.00072</td>\n      <td>-0.00008</td>\n      <td>1500.20</td>\n    </tr>\n    <tr>\n      <th>1067</th>\n      <td>0.00108</td>\n      <td>-0.00144</td>\n      <td>1045.00</td>\n    </tr>\n    <tr>\n      <th>1068</th>\n      <td>0.00024</td>\n      <td>-0.00048</td>\n      <td>1484.90</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.00240</td>\n      <td>-0.00008</td>\n      <td>1259.10</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.00192</td>\n      <td>0.00152</td>\n      <td>861.75</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "\n",
    "print(df1)\n",
    "df_reset = df1.reset_index(drop=True)\n",
    "df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e72a16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.158992400Z",
     "start_time": "2024-03-14T16:01:35.130747900Z"
    }
   },
   "outputs": [],
   "source": [
    "label = df_reset[\"Temperatur\"]\n",
    "# Korrektur: Verwenden Sie den Spaltennamen direkt, ohne Indexierung der columns-Eigenschaft\n",
    "df1 = df_reset.drop(\"Temperatur\", axis=1)\n",
    "X = df1\n",
    "y = label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7fa289a50d87423"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e694a236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.212719200Z",
     "start_time": "2024-03-14T16:01:35.135264800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      X-Koordinate  Y-Koordinate\n0          0.00036       0.00048\n1          0.00132      -0.00112\n2          0.00072      -0.00176\n3          0.00216      -0.00104\n4          0.00156       0.00184\n...            ...           ...\n1066       0.00072      -0.00008\n1067       0.00108      -0.00144\n1068       0.00024      -0.00048\n1069       0.00240      -0.00008\n1070       0.00192       0.00152\n\n[1071 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00036</td>\n      <td>0.00048</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00132</td>\n      <td>-0.00112</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00072</td>\n      <td>-0.00176</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00216</td>\n      <td>-0.00104</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00156</td>\n      <td>0.00184</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1066</th>\n      <td>0.00072</td>\n      <td>-0.00008</td>\n    </tr>\n    <tr>\n      <th>1067</th>\n      <td>0.00108</td>\n      <td>-0.00144</td>\n    </tr>\n    <tr>\n      <th>1068</th>\n      <td>0.00024</td>\n      <td>-0.00048</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>0.00240</td>\n      <td>-0.00008</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>0.00192</td>\n      <td>0.00152</td>\n    </tr>\n  </tbody>\n</table>\n<p>1071 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f3303b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.272719100Z",
     "start_time": "2024-03-14T16:01:35.142917600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0       1441.10\n1       1225.70\n2        832.88\n3       1158.00\n4        642.96\n         ...   \n1066    1500.20\n1067    1045.00\n1068    1484.90\n1069    1259.10\n1070     861.75\nName: Temperatur, Length: 1071, dtype: float64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ad8da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.275719300Z",
     "start_time": "2024-03-14T16:01:35.146469500Z"
    }
   },
   "outputs": [],
   "source": [
    " # train_df enthält 80% der Daten, test_df enthält 20% der Daten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c705edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.277718700Z",
     "start_time": "2024-03-14T16:01:35.153526800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_features2 = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X_train)\n",
    "X_test_scaled = scaler_features.transform(X_test)  # Nutze unterschiedliche Skalierungsparameter\n",
    "\n",
    "# Initialisiere einen SEPARATEN MinMaxScaler für das Ziel, wenn nötig\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Skaliere y_train und y_test. Beachte, dass y_train.reshape(-1, 1) verwendet wird, da MinMaxScaler \n",
    "# erwartet, dass die Eingaben als 2D-Arrays kommen, und Ziele normalerweise als 1D-Arrays vorliegen.\n",
    "y_train_scaled = scaler_target.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_test.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbefe631e495b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.283719200Z",
     "start_time": "2024-03-14T16:01:35.159993300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.35, 0.88],\n       [0.75, 0.88],\n       [0.85, 0.02],\n       ...,\n       [0.2 , 0.86],\n       [1.  , 0.52],\n       [0.8 , 0.04]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_scaled.max()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:01:35.342719Z",
     "start_time": "2024-03-14T16:01:35.165145800Z"
    }
   },
   "id": "ce04ce43aac2242f"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "Epoch 1/1000\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "28/28 [==============================] - 2s 10ms/step - loss: 0.5297 - mae: 0.3740 - val_loss: 0.3942 - val_mae: 0.2722\n",
      "Epoch 2/1000\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.3673 - mae: 0.2765 - val_loss: 0.3166 - val_mae: 0.2433\n",
      "Epoch 3/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.3121 - mae: 0.2450 - val_loss: 0.3312 - val_mae: 0.2546\n",
      "Epoch 4/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2821 - mae: 0.2235 - val_loss: 0.2474 - val_mae: 0.1839\n",
      "Epoch 5/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.2235 - mae: 0.1419 - val_loss: 0.1963 - val_mae: 0.0765\n",
      "Epoch 6/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1966 - mae: 0.0823 - val_loss: 0.1853 - val_mae: 0.0536\n",
      "Epoch 7/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1839 - mae: 0.0581 - val_loss: 0.1786 - val_mae: 0.0444\n",
      "Epoch 8/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1776 - mae: 0.0487 - val_loss: 0.1878 - val_mae: 0.1011\n",
      "Epoch 9/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1963 - mae: 0.1175 - val_loss: 0.1730 - val_mae: 0.0613\n",
      "Epoch 10/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1731 - mae: 0.0623 - val_loss: 0.1787 - val_mae: 0.0936\n",
      "Epoch 11/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1706 - mae: 0.0632 - val_loss: 0.1679 - val_mae: 0.0576\n",
      "Epoch 12/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1628 - mae: 0.0331 - val_loss: 0.1599 - val_mae: 0.0202\n",
      "Epoch 13/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1588 - mae: 0.0204 - val_loss: 0.1586 - val_mae: 0.0317\n",
      "Epoch 14/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1563 - mae: 0.0167 - val_loss: 0.1548 - val_mae: 0.0089\n",
      "Epoch 15/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1546 - mae: 0.0209 - val_loss: 0.1531 - val_mae: 0.0183\n",
      "Epoch 16/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1521 - mae: 0.0179 - val_loss: 0.1509 - val_mae: 0.0171\n",
      "Epoch 17/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1500 - mae: 0.0169 - val_loss: 0.1496 - val_mae: 0.0226\n",
      "Epoch 18/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1489 - mae: 0.0264 - val_loss: 0.1469 - val_mae: 0.0150\n",
      "Epoch 19/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1459 - mae: 0.0141 - val_loss: 0.1447 - val_mae: 0.0109\n",
      "Epoch 20/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1439 - mae: 0.0130 - val_loss: 0.1436 - val_mae: 0.0257\n",
      "Epoch 21/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1421 - mae: 0.0143 - val_loss: 0.1410 - val_mae: 0.0135\n",
      "Epoch 22/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1402 - mae: 0.0116 - val_loss: 0.1394 - val_mae: 0.0159\n",
      "Epoch 23/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1385 - mae: 0.0131 - val_loss: 0.1377 - val_mae: 0.0155\n",
      "Epoch 24/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1370 - mae: 0.0170 - val_loss: 0.1367 - val_mae: 0.0259\n",
      "Epoch 25/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1370 - mae: 0.0315 - val_loss: 0.1384 - val_mae: 0.0508\n",
      "Epoch 26/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1356 - mae: 0.0349 - val_loss: 0.1330 - val_mae: 0.0202\n",
      "Epoch 27/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1319 - mae: 0.0143 - val_loss: 0.1308 - val_mae: 0.0085\n",
      "Epoch 28/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1303 - mae: 0.0146 - val_loss: 0.1299 - val_mae: 0.0198\n",
      "Epoch 29/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1291 - mae: 0.0178 - val_loss: 0.1278 - val_mae: 0.0138\n",
      "Epoch 30/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1270 - mae: 0.0098 - val_loss: 0.1261 - val_mae: 0.0072\n",
      "Epoch 31/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1255 - mae: 0.0102 - val_loss: 0.1247 - val_mae: 0.0111\n",
      "Epoch 32/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1240 - mae: 0.0100 - val_loss: 0.1231 - val_mae: 0.0061\n",
      "Epoch 33/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1225 - mae: 0.0078 - val_loss: 0.1218 - val_mae: 0.0109\n",
      "Epoch 34/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1212 - mae: 0.0120 - val_loss: 0.1206 - val_mae: 0.0162\n",
      "Epoch 35/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1218 - mae: 0.0333 - val_loss: 0.1250 - val_mae: 0.0588\n",
      "Epoch 36/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1212 - mae: 0.0409 - val_loss: 0.1214 - val_mae: 0.0529\n",
      "Epoch 37/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1192 - mae: 0.0372 - val_loss: 0.1179 - val_mae: 0.0289\n",
      "Epoch 38/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1171 - mae: 0.0277 - val_loss: 0.1155 - val_mae: 0.0186\n",
      "Epoch 39/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1181 - mae: 0.0430 - val_loss: 0.1153 - val_mae: 0.0325\n",
      "Epoch 40/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1147 - mae: 0.0303 - val_loss: 0.1180 - val_mae: 0.0592\n",
      "Epoch 41/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1137 - mae: 0.0308 - val_loss: 0.1120 - val_mae: 0.0264\n",
      "Epoch 42/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1110 - mae: 0.0185 - val_loss: 0.1099 - val_mae: 0.0094\n",
      "Epoch 43/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1096 - mae: 0.0163 - val_loss: 0.1088 - val_mae: 0.0119\n",
      "Epoch 44/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1083 - mae: 0.0159 - val_loss: 0.1074 - val_mae: 0.0092\n",
      "Epoch 45/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1079 - mae: 0.0257 - val_loss: 0.1085 - val_mae: 0.0417\n",
      "Epoch 46/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1071 - mae: 0.0298 - val_loss: 0.1055 - val_mae: 0.0186\n",
      "Epoch 47/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1048 - mae: 0.0168 - val_loss: 0.1040 - val_mae: 0.0135\n",
      "Epoch 48/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1033 - mae: 0.0080 - val_loss: 0.1027 - val_mae: 0.0052\n",
      "Epoch 49/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1022 - mae: 0.0072 - val_loss: 0.1015 - val_mae: 0.0074\n",
      "Epoch 50/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.1010 - mae: 0.0056 - val_loss: 0.1004 - val_mae: 0.0049\n",
      "Epoch 51/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0999 - mae: 0.0087 - val_loss: 0.0995 - val_mae: 0.0121\n",
      "Epoch 52/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0989 - mae: 0.0101 - val_loss: 0.0989 - val_mae: 0.0199\n",
      "Epoch 53/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0983 - mae: 0.0173 - val_loss: 0.0985 - val_mae: 0.0271\n",
      "Epoch 54/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0982 - mae: 0.0278 - val_loss: 0.1004 - val_mae: 0.0465\n",
      "Epoch 55/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0993 - mae: 0.0416 - val_loss: 0.0996 - val_mae: 0.0580\n",
      "Epoch 56/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0971 - mae: 0.0380 - val_loss: 0.0942 - val_mae: 0.0121\n",
      "Epoch 57/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0938 - mae: 0.0133 - val_loss: 0.0930 - val_mae: 0.0084\n",
      "Epoch 58/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0926 - mae: 0.0094 - val_loss: 0.0920 - val_mae: 0.0067\n",
      "Epoch 59/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0915 - mae: 0.0065 - val_loss: 0.0910 - val_mae: 0.0077\n",
      "Epoch 60/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0906 - mae: 0.0092 - val_loss: 0.0900 - val_mae: 0.0061\n",
      "Epoch 61/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0895 - mae: 0.0061 - val_loss: 0.0892 - val_mae: 0.0106\n",
      "Epoch 62/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0889 - mae: 0.0143 - val_loss: 0.0886 - val_mae: 0.0181\n",
      "Epoch 63/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0877 - mae: 0.0088 - val_loss: 0.0871 - val_mae: 0.0070\n",
      "Epoch 64/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0866 - mae: 0.0073 - val_loss: 0.0862 - val_mae: 0.0084\n",
      "Epoch 65/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0857 - mae: 0.0064 - val_loss: 0.0852 - val_mae: 0.0085\n",
      "Epoch 66/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0848 - mae: 0.0094 - val_loss: 0.0842 - val_mae: 0.0062\n",
      "Epoch 67/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0838 - mae: 0.0064 - val_loss: 0.0835 - val_mae: 0.0128\n",
      "Epoch 68/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0830 - mae: 0.0105 - val_loss: 0.0828 - val_mae: 0.0160\n",
      "Epoch 69/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0823 - mae: 0.0151 - val_loss: 0.0818 - val_mae: 0.0163\n",
      "Epoch 70/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0813 - mae: 0.0135 - val_loss: 0.0816 - val_mae: 0.0248\n",
      "Epoch 71/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0810 - mae: 0.0207 - val_loss: 0.0799 - val_mae: 0.0118\n",
      "Epoch 72/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0795 - mae: 0.0115 - val_loss: 0.0790 - val_mae: 0.0122\n",
      "Epoch 73/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0786 - mae: 0.0122 - val_loss: 0.0784 - val_mae: 0.0176\n",
      "Epoch 74/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0776 - mae: 0.0097 - val_loss: 0.0773 - val_mae: 0.0123\n",
      "Epoch 75/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0768 - mae: 0.0111 - val_loss: 0.0764 - val_mae: 0.0145\n",
      "Epoch 76/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0759 - mae: 0.0099 - val_loss: 0.0754 - val_mae: 0.0077\n",
      "Epoch 77/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0752 - mae: 0.0117 - val_loss: 0.0747 - val_mae: 0.0115\n",
      "Epoch 78/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0743 - mae: 0.0110 - val_loss: 0.0738 - val_mae: 0.0109\n",
      "Epoch 79/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0734 - mae: 0.0088 - val_loss: 0.0731 - val_mae: 0.0139\n",
      "Epoch 80/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0726 - mae: 0.0100 - val_loss: 0.0725 - val_mae: 0.0165\n",
      "Epoch 81/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0720 - mae: 0.0148 - val_loss: 0.0722 - val_mae: 0.0277\n",
      "Epoch 82/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0713 - mae: 0.0173 - val_loss: 0.0706 - val_mae: 0.0118\n",
      "Epoch 83/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0704 - mae: 0.0154 - val_loss: 0.0699 - val_mae: 0.0163\n",
      "Epoch 84/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0696 - mae: 0.0162 - val_loss: 0.0695 - val_mae: 0.0176\n",
      "Epoch 85/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0691 - mae: 0.0184 - val_loss: 0.0682 - val_mae: 0.0104\n",
      "Epoch 86/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0678 - mae: 0.0112 - val_loss: 0.0673 - val_mae: 0.0075\n",
      "Epoch 87/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0670 - mae: 0.0090 - val_loss: 0.0667 - val_mae: 0.0153\n",
      "Epoch 88/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0665 - mae: 0.0149 - val_loss: 0.0703 - val_mae: 0.0480\n",
      "Epoch 89/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0690 - mae: 0.0417 - val_loss: 0.0673 - val_mae: 0.0456\n",
      "Epoch 90/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0673 - mae: 0.0376 - val_loss: 0.0660 - val_mae: 0.0306\n",
      "Epoch 91/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0644 - mae: 0.0174 - val_loss: 0.0643 - val_mae: 0.0235\n",
      "Epoch 92/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0635 - mae: 0.0158 - val_loss: 0.0638 - val_mae: 0.0273\n",
      "Epoch 93/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0632 - mae: 0.0202 - val_loss: 0.0627 - val_mae: 0.0187\n",
      "Epoch 94/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0620 - mae: 0.0143 - val_loss: 0.0617 - val_mae: 0.0164\n",
      "Epoch 95/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0612 - mae: 0.0124 - val_loss: 0.0607 - val_mae: 0.0081\n",
      "Epoch 96/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0603 - mae: 0.0078 - val_loss: 0.0599 - val_mae: 0.0047\n",
      "Epoch 97/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0596 - mae: 0.0062 - val_loss: 0.0592 - val_mae: 0.0070\n",
      "Epoch 98/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0591 - mae: 0.0109 - val_loss: 0.0587 - val_mae: 0.0137\n",
      "Epoch 99/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0582 - mae: 0.0076 - val_loss: 0.0578 - val_mae: 0.0038\n",
      "Epoch 100/1000\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0575 - mae: 0.0068 - val_loss: 0.0571 - val_mae: 0.0062\n",
      "Epoch 101/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0568 - mae: 0.0063 - val_loss: 0.0564 - val_mae: 0.0054\n",
      "Epoch 102/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0561 - mae: 0.0060 - val_loss: 0.0559 - val_mae: 0.0114\n",
      "Epoch 103/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0575 - mae: 0.0264 - val_loss: 0.0775 - val_mae: 0.1085\n",
      "Epoch 104/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0587 - mae: 0.0409 - val_loss: 0.0550 - val_mae: 0.0189\n",
      "Epoch 105/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0556 - mae: 0.0266 - val_loss: 0.0542 - val_mae: 0.0162\n",
      "Epoch 106/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0545 - mae: 0.0238 - val_loss: 0.0555 - val_mae: 0.0371\n",
      "Epoch 107/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0542 - mae: 0.0254 - val_loss: 0.0527 - val_mae: 0.0106\n",
      "Epoch 108/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0526 - mae: 0.0156 - val_loss: 0.0521 - val_mae: 0.0102\n",
      "Epoch 109/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0518 - mae: 0.0117 - val_loss: 0.0519 - val_mae: 0.0202\n",
      "Epoch 110/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0513 - mae: 0.0142 - val_loss: 0.0506 - val_mae: 0.0059\n",
      "Epoch 111/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0505 - mae: 0.0097 - val_loss: 0.0505 - val_mae: 0.0175\n",
      "Epoch 112/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0500 - mae: 0.0136 - val_loss: 0.0512 - val_mae: 0.0342\n",
      "Epoch 113/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0504 - mae: 0.0242 - val_loss: 0.0498 - val_mae: 0.0243\n",
      "Epoch 114/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0492 - mae: 0.0183 - val_loss: 0.0487 - val_mae: 0.0183\n",
      "Epoch 115/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0482 - mae: 0.0152 - val_loss: 0.0480 - val_mae: 0.0188\n",
      "Epoch 116/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0475 - mae: 0.0111 - val_loss: 0.0470 - val_mae: 0.0061\n",
      "Epoch 117/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0468 - mae: 0.0083 - val_loss: 0.0466 - val_mae: 0.0123\n",
      "Epoch 118/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0463 - mae: 0.0114 - val_loss: 0.0465 - val_mae: 0.0209\n",
      "Epoch 119/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0459 - mae: 0.0155 - val_loss: 0.0453 - val_mae: 0.0070\n",
      "Epoch 120/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0450 - mae: 0.0075 - val_loss: 0.0449 - val_mae: 0.0120\n",
      "Epoch 121/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0444 - mae: 0.0071 - val_loss: 0.0442 - val_mae: 0.0098\n",
      "Epoch 122/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0439 - mae: 0.0092 - val_loss: 0.0436 - val_mae: 0.0086\n",
      "Epoch 123/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0433 - mae: 0.0085 - val_loss: 0.0430 - val_mae: 0.0088\n",
      "Epoch 124/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0431 - mae: 0.0146 - val_loss: 0.0426 - val_mae: 0.0129\n",
      "Epoch 125/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0425 - mae: 0.0140 - val_loss: 0.0421 - val_mae: 0.0130\n",
      "Epoch 126/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0417 - mae: 0.0101 - val_loss: 0.0414 - val_mae: 0.0095\n",
      "Epoch 127/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0411 - mae: 0.0075 - val_loss: 0.0408 - val_mae: 0.0081\n",
      "Epoch 128/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0406 - mae: 0.0069 - val_loss: 0.0405 - val_mae: 0.0136\n",
      "Epoch 129/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0401 - mae: 0.0097 - val_loss: 0.0397 - val_mae: 0.0068\n",
      "Epoch 130/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0395 - mae: 0.0081 - val_loss: 0.0392 - val_mae: 0.0068\n",
      "Epoch 131/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0389 - mae: 0.0064 - val_loss: 0.0387 - val_mae: 0.0074\n",
      "Epoch 132/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0385 - mae: 0.0082 - val_loss: 0.0382 - val_mae: 0.0098\n",
      "Epoch 133/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0379 - mae: 0.0076 - val_loss: 0.0377 - val_mae: 0.0073\n",
      "Epoch 134/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0374 - mae: 0.0079 - val_loss: 0.0374 - val_mae: 0.0165\n",
      "Epoch 135/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0370 - mae: 0.0107 - val_loss: 0.0369 - val_mae: 0.0150\n",
      "Epoch 136/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0365 - mae: 0.0105 - val_loss: 0.0363 - val_mae: 0.0137\n",
      "Epoch 137/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0362 - mae: 0.0141 - val_loss: 0.0357 - val_mae: 0.0089\n",
      "Epoch 138/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0356 - mae: 0.0124 - val_loss: 0.0352 - val_mae: 0.0098\n",
      "Epoch 139/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0349 - mae: 0.0079 - val_loss: 0.0347 - val_mae: 0.0084\n",
      "Epoch 140/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0344 - mae: 0.0075 - val_loss: 0.0342 - val_mae: 0.0075\n",
      "Epoch 141/1000\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0340 - mae: 0.0092 - val_loss: 0.0344 - val_mae: 0.0224\n",
      "Epoch 142/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0337 - mae: 0.0135 - val_loss: 0.0339 - val_mae: 0.0210\n",
      "Epoch 143/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0334 - mae: 0.0172 - val_loss: 0.0334 - val_mae: 0.0189\n",
      "Epoch 144/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0327 - mae: 0.0132 - val_loss: 0.0323 - val_mae: 0.0094\n",
      "Epoch 145/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0322 - mae: 0.0096 - val_loss: 0.0319 - val_mae: 0.0075\n",
      "Epoch 146/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0319 - mae: 0.0140 - val_loss: 0.0317 - val_mae: 0.0158\n",
      "Epoch 147/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0313 - mae: 0.0108 - val_loss: 0.0310 - val_mae: 0.0101\n",
      "Epoch 148/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0307 - mae: 0.0081 - val_loss: 0.0305 - val_mae: 0.0055\n",
      "Epoch 149/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0304 - mae: 0.0100 - val_loss: 0.0300 - val_mae: 0.0063\n",
      "Epoch 150/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0300 - mae: 0.0101 - val_loss: 0.0298 - val_mae: 0.0117\n",
      "Epoch 151/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0297 - mae: 0.0127 - val_loss: 0.0294 - val_mae: 0.0173\n",
      "Epoch 152/1000\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0302 - mae: 0.0244 - val_loss: 0.0345 - val_mae: 0.0599\n",
      "Epoch 153/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0317 - mae: 0.0383 - val_loss: 0.0288 - val_mae: 0.0185\n",
      "Epoch 154/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0285 - mae: 0.0155 - val_loss: 0.0281 - val_mae: 0.0117\n",
      "Epoch 155/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0282 - mae: 0.0157 - val_loss: 0.0282 - val_mae: 0.0208\n",
      "Epoch 156/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0279 - mae: 0.0193 - val_loss: 0.0272 - val_mae: 0.0087\n",
      "Epoch 157/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0272 - mae: 0.0134 - val_loss: 0.0269 - val_mae: 0.0125\n",
      "Epoch 158/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0267 - mae: 0.0101 - val_loss: 0.0266 - val_mae: 0.0133\n",
      "Epoch 159/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0265 - mae: 0.0145 - val_loss: 0.0263 - val_mae: 0.0142\n",
      "Epoch 160/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0266 - mae: 0.0212 - val_loss: 0.0261 - val_mae: 0.0202\n",
      "Epoch 161/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0260 - mae: 0.0187 - val_loss: 0.0253 - val_mae: 0.0094\n",
      "Epoch 162/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0252 - mae: 0.0113 - val_loss: 0.0250 - val_mae: 0.0132\n",
      "Epoch 163/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0247 - mae: 0.0087 - val_loss: 0.0247 - val_mae: 0.0139\n",
      "Epoch 164/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0244 - mae: 0.0094 - val_loss: 0.0241 - val_mae: 0.0069\n",
      "Epoch 165/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0240 - mae: 0.0075 - val_loss: 0.0238 - val_mae: 0.0100\n",
      "Epoch 166/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0237 - mae: 0.0091 - val_loss: 0.0234 - val_mae: 0.0076\n",
      "Epoch 167/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0233 - mae: 0.0090 - val_loss: 0.0230 - val_mae: 0.0061\n",
      "Epoch 168/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0230 - mae: 0.0099 - val_loss: 0.0231 - val_mae: 0.0173\n",
      "Epoch 169/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0239 - mae: 0.0249 - val_loss: 0.0235 - val_mae: 0.0254\n",
      "Epoch 170/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0224 - mae: 0.0124 - val_loss: 0.0220 - val_mae: 0.0047\n",
      "Epoch 171/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0219 - mae: 0.0063 - val_loss: 0.0218 - val_mae: 0.0106\n",
      "Epoch 172/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0215 - mae: 0.0065 - val_loss: 0.0214 - val_mae: 0.0081\n",
      "Epoch 173/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0212 - mae: 0.0076 - val_loss: 0.0211 - val_mae: 0.0098\n",
      "Epoch 174/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0209 - mae: 0.0086 - val_loss: 0.0212 - val_mae: 0.0189\n",
      "Epoch 175/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0217 - mae: 0.0263 - val_loss: 0.0223 - val_mae: 0.0358\n",
      "Epoch 176/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0219 - mae: 0.0331 - val_loss: 0.0209 - val_mae: 0.0220\n",
      "Epoch 177/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0202 - mae: 0.0114 - val_loss: 0.0199 - val_mae: 0.0085\n",
      "Epoch 178/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0198 - mae: 0.0113 - val_loss: 0.0198 - val_mae: 0.0136\n",
      "Epoch 179/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0195 - mae: 0.0113 - val_loss: 0.0197 - val_mae: 0.0162\n",
      "Epoch 180/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0192 - mae: 0.0120 - val_loss: 0.0189 - val_mae: 0.0091\n",
      "Epoch 181/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0188 - mae: 0.0089 - val_loss: 0.0188 - val_mae: 0.0120\n",
      "Epoch 182/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0187 - mae: 0.0122 - val_loss: 0.0191 - val_mae: 0.0194\n",
      "Epoch 183/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0190 - mae: 0.0214 - val_loss: 0.0260 - val_mae: 0.0686\n",
      "Epoch 184/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0232 - mae: 0.0568 - val_loss: 0.0189 - val_mae: 0.0240\n",
      "Epoch 185/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0189 - mae: 0.0270 - val_loss: 0.0190 - val_mae: 0.0330\n",
      "Epoch 186/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0179 - mae: 0.0176 - val_loss: 0.0175 - val_mae: 0.0130\n",
      "Epoch 187/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0172 - mae: 0.0075 - val_loss: 0.0170 - val_mae: 0.0055\n",
      "Epoch 188/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0169 - mae: 0.0066 - val_loss: 0.0168 - val_mae: 0.0060\n",
      "Epoch 189/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0167 - mae: 0.0097 - val_loss: 0.0165 - val_mae: 0.0071\n",
      "Epoch 190/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0165 - mae: 0.0097 - val_loss: 0.0165 - val_mae: 0.0147\n",
      "Epoch 191/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0162 - mae: 0.0092 - val_loss: 0.0160 - val_mae: 0.0046\n",
      "Epoch 192/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0074 - val_loss: 0.0160 - val_mae: 0.0131\n",
      "Epoch 193/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0160 - mae: 0.0151 - val_loss: 0.0157 - val_mae: 0.0123\n",
      "Epoch 194/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0159 - mae: 0.0165 - val_loss: 0.0153 - val_mae: 0.0090\n",
      "Epoch 195/1000\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 0.0152 - mae: 0.0069 - val_loss: 0.0151 - val_mae: 0.0072\n",
      "Epoch 196/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0149 - mae: 0.0061 - val_loss: 0.0148 - val_mae: 0.0070\n",
      "Epoch 197/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0147 - mae: 0.0083 - val_loss: 0.0146 - val_mae: 0.0076\n",
      "Epoch 198/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0147 - mae: 0.0129 - val_loss: 0.0146 - val_mae: 0.0140\n",
      "Epoch 199/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0143 - mae: 0.0079 - val_loss: 0.0142 - val_mae: 0.0079\n",
      "Epoch 200/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0141 - mae: 0.0078 - val_loss: 0.0139 - val_mae: 0.0064\n",
      "Epoch 201/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0075 - val_loss: 0.0138 - val_mae: 0.0105\n",
      "Epoch 202/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0138 - mae: 0.0129 - val_loss: 0.0138 - val_mae: 0.0164\n",
      "Epoch 203/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0134 - mae: 0.0076 - val_loss: 0.0133 - val_mae: 0.0086\n",
      "Epoch 204/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0079 - val_loss: 0.0132 - val_mae: 0.0133\n",
      "Epoch 205/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0082 - val_loss: 0.0129 - val_mae: 0.0072\n",
      "Epoch 206/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0130 - mae: 0.0149 - val_loss: 0.0127 - val_mae: 0.0080\n",
      "Epoch 207/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0126 - mae: 0.0078 - val_loss: 0.0124 - val_mae: 0.0050\n",
      "Epoch 208/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0084 - val_loss: 0.0124 - val_mae: 0.0114\n",
      "Epoch 209/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0123 - mae: 0.0099 - val_loss: 0.0123 - val_mae: 0.0131\n",
      "Epoch 210/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0080 - val_loss: 0.0119 - val_mae: 0.0060\n",
      "Epoch 211/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0118 - mae: 0.0064 - val_loss: 0.0118 - val_mae: 0.0122\n",
      "Epoch 212/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0124 - mae: 0.0210 - val_loss: 0.0135 - val_mae: 0.0380\n",
      "Epoch 213/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0199 - val_loss: 0.0114 - val_mae: 0.0098\n",
      "Epoch 214/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0115 - mae: 0.0144 - val_loss: 0.0113 - val_mae: 0.0126\n",
      "Epoch 215/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0314 - val_loss: 0.0169 - val_mae: 0.0683\n",
      "Epoch 216/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0132 - mae: 0.0376 - val_loss: 0.0139 - val_mae: 0.0498\n",
      "Epoch 217/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0120 - mae: 0.0278 - val_loss: 0.0116 - val_mae: 0.0273\n",
      "Epoch 218/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0109 - mae: 0.0134 - val_loss: 0.0106 - val_mae: 0.0100\n",
      "Epoch 219/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0106 - mae: 0.0094 - val_loss: 0.0107 - val_mae: 0.0163\n",
      "Epoch 220/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0105 - mae: 0.0136 - val_loss: 0.0102 - val_mae: 0.0072\n",
      "Epoch 221/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0073 - val_loss: 0.0100 - val_mae: 0.0050\n",
      "Epoch 222/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0096 - val_loss: 0.0106 - val_mae: 0.0218\n",
      "Epoch 223/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0101 - mae: 0.0136 - val_loss: 0.0109 - val_mae: 0.0294\n",
      "Epoch 224/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0099 - mae: 0.0145 - val_loss: 0.0103 - val_mae: 0.0237\n",
      "Epoch 225/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0102 - mae: 0.0203 - val_loss: 0.0098 - val_mae: 0.0173\n",
      "Epoch 226/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0095 - mae: 0.0117 - val_loss: 0.0094 - val_mae: 0.0092\n",
      "Epoch 227/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0124 - val_loss: 0.0093 - val_mae: 0.0140\n",
      "Epoch 228/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0094 - mae: 0.0147 - val_loss: 0.0095 - val_mae: 0.0193\n",
      "Epoch 229/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0091 - mae: 0.0119 - val_loss: 0.0090 - val_mae: 0.0100\n",
      "Epoch 230/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0074 - val_loss: 0.0088 - val_mae: 0.0061\n",
      "Epoch 231/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0089 - mae: 0.0120 - val_loss: 0.0088 - val_mae: 0.0131\n",
      "Epoch 232/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0096 - mae: 0.0238 - val_loss: 0.0093 - val_mae: 0.0265\n",
      "Epoch 233/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0086 - mae: 0.0124 - val_loss: 0.0085 - val_mae: 0.0097\n",
      "Epoch 234/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0069 - val_loss: 0.0082 - val_mae: 0.0044\n",
      "Epoch 235/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0107 - val_loss: 0.0084 - val_mae: 0.0160\n",
      "Epoch 236/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0084 - mae: 0.0145 - val_loss: 0.0081 - val_mae: 0.0074\n",
      "Epoch 237/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0082 - mae: 0.0135 - val_loss: 0.0079 - val_mae: 0.0068\n",
      "Epoch 238/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0079 - mae: 0.0089 - val_loss: 0.0078 - val_mae: 0.0049\n",
      "Epoch 239/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0089 - val_loss: 0.0083 - val_mae: 0.0217\n",
      "Epoch 240/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0083 - mae: 0.0198 - val_loss: 0.0078 - val_mae: 0.0160\n",
      "Epoch 241/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0078 - mae: 0.0156 - val_loss: 0.0075 - val_mae: 0.0089\n",
      "Epoch 242/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0075 - mae: 0.0101 - val_loss: 0.0080 - val_mae: 0.0203\n",
      "Epoch 243/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0076 - mae: 0.0153 - val_loss: 0.0072 - val_mae: 0.0056\n",
      "Epoch 244/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0072 - mae: 0.0056 - val_loss: 0.0071 - val_mae: 0.0043\n",
      "Epoch 245/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0071 - mae: 0.0075 - val_loss: 0.0071 - val_mae: 0.0113\n",
      "Epoch 246/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0071 - mae: 0.0094 - val_loss: 0.0070 - val_mae: 0.0092\n",
      "Epoch 247/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0070 - mae: 0.0109 - val_loss: 0.0068 - val_mae: 0.0044\n",
      "Epoch 248/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0068 - mae: 0.0070 - val_loss: 0.0068 - val_mae: 0.0081\n",
      "Epoch 249/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0067 - mae: 0.0069 - val_loss: 0.0068 - val_mae: 0.0115\n",
      "Epoch 250/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0066 - mae: 0.0067 - val_loss: 0.0065 - val_mae: 0.0054\n",
      "Epoch 251/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0065 - mae: 0.0074 - val_loss: 0.0067 - val_mae: 0.0153\n",
      "Epoch 252/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0067 - mae: 0.0155 - val_loss: 0.0064 - val_mae: 0.0075\n",
      "Epoch 253/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0064 - mae: 0.0103 - val_loss: 0.0064 - val_mae: 0.0102\n",
      "Epoch 254/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0065 - mae: 0.0143 - val_loss: 0.0066 - val_mae: 0.0180\n",
      "Epoch 255/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0062 - mae: 0.0099 - val_loss: 0.0061 - val_mae: 0.0069\n",
      "Epoch 256/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0063 - mae: 0.0112 - val_loss: 0.0148 - val_mae: 0.0696\n",
      "Epoch 257/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0128 - mae: 0.0641 - val_loss: 0.0072 - val_mae: 0.0284\n",
      "Epoch 258/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0070 - mae: 0.0235 - val_loss: 0.0062 - val_mae: 0.0121\n",
      "Epoch 259/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0061 - mae: 0.0124 - val_loss: 0.0064 - val_mae: 0.0203\n",
      "Epoch 260/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0061 - mae: 0.0129 - val_loss: 0.0059 - val_mae: 0.0095\n",
      "Epoch 261/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0058 - mae: 0.0083 - val_loss: 0.0057 - val_mae: 0.0074\n",
      "Epoch 262/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0061 - mae: 0.0155 - val_loss: 0.0057 - val_mae: 0.0080\n",
      "Epoch 263/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0059 - mae: 0.0125 - val_loss: 0.0059 - val_mae: 0.0145\n",
      "Epoch 264/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0056 - mae: 0.0081 - val_loss: 0.0055 - val_mae: 0.0078\n",
      "Epoch 265/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0055 - mae: 0.0069 - val_loss: 0.0054 - val_mae: 0.0051\n",
      "Epoch 266/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0054 - mae: 0.0059 - val_loss: 0.0054 - val_mae: 0.0081\n",
      "Epoch 267/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0058 - mae: 0.0162 - val_loss: 0.0059 - val_mae: 0.0195\n",
      "Epoch 268/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0059 - mae: 0.0210 - val_loss: 0.0054 - val_mae: 0.0138\n",
      "Epoch 269/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0054 - mae: 0.0117 - val_loss: 0.0054 - val_mae: 0.0117\n",
      "Epoch 270/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0053 - mae: 0.0110 - val_loss: 0.0052 - val_mae: 0.0082\n",
      "Epoch 271/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0051 - mae: 0.0066 - val_loss: 0.0050 - val_mae: 0.0051\n",
      "Epoch 272/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0051 - mae: 0.0078 - val_loss: 0.0052 - val_mae: 0.0123\n",
      "Epoch 273/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0050 - mae: 0.0080 - val_loss: 0.0050 - val_mae: 0.0076\n",
      "Epoch 274/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0051 - mae: 0.0126 - val_loss: 0.0049 - val_mae: 0.0056\n",
      "Epoch 275/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0049 - mae: 0.0078 - val_loss: 0.0049 - val_mae: 0.0107\n",
      "Epoch 276/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0049 - mae: 0.0095 - val_loss: 0.0051 - val_mae: 0.0181\n",
      "Epoch 277/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0049 - mae: 0.0109 - val_loss: 0.0049 - val_mae: 0.0141\n",
      "Epoch 278/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0048 - mae: 0.0109 - val_loss: 0.0047 - val_mae: 0.0088\n",
      "Epoch 279/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0046 - mae: 0.0072 - val_loss: 0.0046 - val_mae: 0.0074\n",
      "Epoch 280/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0046 - mae: 0.0071 - val_loss: 0.0047 - val_mae: 0.0122\n",
      "Epoch 281/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0050 - mae: 0.0161 - val_loss: 0.0046 - val_mae: 0.0108\n",
      "Epoch 282/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0047 - mae: 0.0126 - val_loss: 0.0045 - val_mae: 0.0067\n",
      "Epoch 283/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0045 - mae: 0.0090 - val_loss: 0.0046 - val_mae: 0.0114\n",
      "Epoch 284/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0044 - mae: 0.0062 - val_loss: 0.0046 - val_mae: 0.0123\n",
      "Epoch 285/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0044 - mae: 0.0093 - val_loss: 0.0043 - val_mae: 0.0058\n",
      "Epoch 286/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0043 - mae: 0.0058 - val_loss: 0.0042 - val_mae: 0.0065\n",
      "Epoch 287/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0043 - mae: 0.0083 - val_loss: 0.0043 - val_mae: 0.0097\n",
      "Epoch 288/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0081 - val_loss: 0.0042 - val_mae: 0.0066\n",
      "Epoch 289/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0078 - val_loss: 0.0043 - val_mae: 0.0122\n",
      "Epoch 290/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0041 - mae: 0.0083 - val_loss: 0.0042 - val_mae: 0.0103\n",
      "Epoch 291/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0048 - mae: 0.0228 - val_loss: 0.0052 - val_mae: 0.0316\n",
      "Epoch 292/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0045 - mae: 0.0190 - val_loss: 0.0041 - val_mae: 0.0104\n",
      "Epoch 293/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0047 - mae: 0.0226 - val_loss: 0.0046 - val_mae: 0.0235\n",
      "Epoch 294/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0041 - mae: 0.0110 - val_loss: 0.0040 - val_mae: 0.0109\n",
      "Epoch 295/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0041 - mae: 0.0101 - val_loss: 0.0066 - val_mae: 0.0381\n",
      "Epoch 296/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0050 - mae: 0.0248 - val_loss: 0.0042 - val_mae: 0.0173\n",
      "Epoch 297/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0042 - mae: 0.0153 - val_loss: 0.0039 - val_mae: 0.0089\n",
      "Epoch 298/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0040 - mae: 0.0116 - val_loss: 0.0038 - val_mae: 0.0083\n",
      "Epoch 299/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0069 - val_loss: 0.0038 - val_mae: 0.0095\n",
      "Epoch 300/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0100 - val_loss: 0.0037 - val_mae: 0.0067\n",
      "Epoch 301/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0037 - mae: 0.0059 - val_loss: 0.0037 - val_mae: 0.0080\n",
      "Epoch 302/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0037 - mae: 0.0084 - val_loss: 0.0040 - val_mae: 0.0161\n",
      "Epoch 303/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0128 - val_loss: 0.0044 - val_mae: 0.0240\n",
      "Epoch 304/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0123 - val_loss: 0.0037 - val_mae: 0.0100\n",
      "Epoch 305/1000\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0037 - mae: 0.0122 - val_loss: 0.0039 - val_mae: 0.0143\n",
      "Epoch 306/1000\n",
      " 1/28 [>.............................] - ETA: 0s - loss: 0.0039 - mae: 0.0169Restoring model weights from the end of the best epoch: 301.\n",
      "28/28 [==============================] - 0s 3ms/step - loss: 0.0038 - mae: 0.0147 - val_loss: 0.0037 - val_mae: 0.0131\n",
      "Epoch 306: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "\n",
    "    Dense(264, activation='relu', input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "\n",
    "    Dense(232, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(280, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    Dense(264, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    Dense(216, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    Dense(72, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    Dense(216, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    Dense(88, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0001)),\n",
    "    \n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 25\n",
    "epochs = 1000\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:02:03.640027400Z",
     "start_time": "2024-03-14T16:01:35.171718600Z"
    }
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 - 0s - loss: 0.0037 - mae: 0.0065 - 30ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.0037017916329205036, 0.006507318001240492]"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(X_test_scaled, y_test_scaled, verbose=2)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:02:03.708829500Z",
     "start_time": "2024-03-14T16:02:03.640027400Z"
    }
   },
   "id": "4b02697cbcecd185"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bsp. Predicted: [1156.5292] Actual: [1148.8] \n",
      "Durchschnittliche Abweichung (MAE): [6.15110788]\n"
     ]
    }
   ],
   "source": [
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Führen Sie die Rücktransformation der skalierten Werte durch\n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[100]} Actual: {original_actual_values[100]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "\n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "\n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:02:03.845753800Z",
     "start_time": "2024-03-14T16:02:03.704300Z"
    }
   },
   "id": "a402d28abbd82f60"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: [0.99890102]\n"
     ]
    }
   ],
   "source": [
    "def calculate_r_squared(predicted, actual):\n",
    "    # Berechnung des Mittelwerts der tatsächlichen Werte\n",
    "    mean_actual = sum(actual) / len(actual)\n",
    "    \n",
    "    # Berechnung der totalen Summe der Quadrate (SST)\n",
    "    sst = sum((x - mean_actual) ** 2 for x in actual)\n",
    "    \n",
    "    # Berechnung der Summe der Quadrate der Residuen (SSE)\n",
    "    sse = sum((actual[i] - predicted[i]) ** 2 for i in range(len(actual)))\n",
    "    \n",
    "    # Berechnung des R^2-Wertes\n",
    "    r_squared = 1 - (sse / sst)\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "# Berechnung von R^2 mit den bereitgestellten Listen\n",
    "r_squared = calculate_r_squared(list1, list2)\n",
    "\n",
    "print(f\"R^2: {r_squared}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:02:03.851239200Z",
     "start_time": "2024-03-14T16:02:03.845753800Z"
    }
   },
   "id": "2820df0b9f03b675"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae = history.history['loss']\n",
    "val_mae = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(mae) + 1)\n",
    "\n",
    "# MAE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mae, 'r', label='Training MSE')\n",
    "plt.plot(epochs, val_mae, 'b', label='Validation MSE')\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:02:03.990691700Z",
     "start_time": "2024-03-14T16:02:03.851239200Z"
    }
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# def build_model(learning_rate=0.001, activation='relu', regularization=0.0001, dropout_rate=0.0):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(448, activation=activation, input_shape=(2,), kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(384, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(96, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(128, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(320, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "# \n",
    "#     model.add(Dense(416, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     \n",
    "#     model.add(Dense(416, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "#     \n",
    "#     model.add(Dense(256, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "#     model.add(Dropout(dropout_rate))    \n",
    "#     \n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "#     return model\n",
    "# \n",
    "# # Verwenden Sie eine Funktion, um das Modell zu instanziieren, für scikit-learn Wrapper\n",
    "# model = KerasRegressor(model=build_model, verbose=2)\n",
    "# \n",
    "# # Anpassung der Parameter im param_grid\n",
    "# param_grid = {\n",
    "#     'model__learning_rate': [0.01, 0.001, 0.0001],\n",
    "#     'model__regularization': [0.001, 0.0001],\n",
    "#     'fit__batch_size': [10, 25, 50, 75],\n",
    "#     'fit__epochs': [50],\n",
    "#     'model__dropout_rate' : [0.0, 0.1, 0.2]\n",
    "# }\n",
    "# \n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "# # Hinweis: Stellen Sie sicher, dass Ihre Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\n",
    "# grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "# # Beste Parameter und Score ausgeben\n",
    "# print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "# print(\"Beste Genauigkeit:\", grid_search.best_score_)\n",
    "# \n",
    "# with open(\"Gridsearch_D4.txt\", \"w\") as f:\n",
    "#     f.write(f\"Beste Parameter: {grid_search.best_params_}\\n\")\n",
    "#     f.write(f\"Beste Genauigkeit: {grid_search.best_score_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:02:04.001524500Z",
     "start_time": "2024-03-14T16:02:03.990691700Z"
    }
   },
   "id": "7464a951f44a07ee"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# # Funktion zum Erstellen des Modells\n",
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(hp.Int('input_units', min_value=8, max_value=328, step=16), input_shape=(2,), activation='relu'))\n",
    "#     for i in range(hp.Int('n_layers', 1, 10)):\n",
    "#         model.add(Dense(hp.Int(f'units_{i}', min_value=8, max_value=328, step=16), activation='relu'))\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#     return model\n",
    "# \n",
    "# # Durchführung der Random Search dreimal\n",
    "# for run in range(1, 4):\n",
    "#     # Anpassen des Verzeichnisses und des Projektnamens für jeden Durchlauf\n",
    "#     directory = 'random_search'\n",
    "#     project_name = f'random_search_D4_{run}'\n",
    "#     \n",
    "#     tuner = RandomSearch(\n",
    "#         build_model,\n",
    "#         objective='val_loss',\n",
    "#         max_trials=100,\n",
    "#         executions_per_trial=1,\n",
    "#         directory=directory,\n",
    "#         project_name=project_name\n",
    "#     )\n",
    "#     \n",
    "#     # Durchführung des Random Search\n",
    "#     tuner.search(X_train_scaled, y_train_scaled, epochs=50, verbose =0, batch_size=20, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "#     \n",
    "#     # Abrufen und Speichern des besten Modells\n",
    "#     best_model = tuner.get_best_models(num_models=1)[0]\n",
    "#     model_path = os.path.join(directory, project_name, 'best_model.h5') \n",
    "#     best_model.save(model_path)\n",
    "#     \n",
    "# \n",
    "#     # Optional: Abrufen und Ausgeben der besten Hyperparameter\n",
    "#     best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "#     \n",
    "#     # Konvertieren der Hyperparameter in ein DataFrame\n",
    "#     df_hyperparameters = pd.DataFrame([best_hyperparameters.values])\n",
    "#     # Speichern des DataFrame als CSV\n",
    "#     df_hyperparameters.to_csv(f'random_search_D4_{run}.csv', index=False)\n",
    "#     \n",
    "#     print(f\"Beste Hyperparameter für Lauf {run}: {best_hyperparameters.values}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:02:04.001524500Z",
     "start_time": "2024-03-14T16:02:03.994879700Z"
    }
   },
   "id": "d0f02feb42b652f5"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-14T16:02:04.001524500Z",
     "start_time": "2024-03-14T16:02:03.998823Z"
    }
   },
   "id": "3e35d5ebef369658"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

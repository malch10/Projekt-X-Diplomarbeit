{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b0518e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T22:16:29.681602600Z",
     "start_time": "2024-04-02T22:16:24.932233700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense , Dropout\n",
    "from scikeras.wrappers import KerasRegressor \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras_tuner import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datenvorverarbeitung"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b39f256a208ce1fb"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ff61b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T22:16:29.696635200Z",
     "start_time": "2024-04-02T22:16:29.681602600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n0             0.0000      -0.00200        100   6000   5000      449.80\n1             0.0000      -0.00192        100   6000   5000      479.76\n2             0.0000      -0.00184        100   6000   5000      506.60\n3             0.0000      -0.00176        100   6000   5000      530.80\n4             0.0000      -0.00168        100   6000   5000      552.15\n...              ...           ...        ...    ...    ...         ...\n351283        0.0024       0.00168        500   9000   5000     1365.50\n351284        0.0024       0.00176        500   9000   5000     1247.20\n351285        0.0024       0.00184        500   9000   5000     1114.10\n351286        0.0024       0.00192        500   9000   5000      983.97\n351287        0.0024       0.00200        500   9000   5000      942.84\n\n[179928 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n      <th>Temperatur</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0000</td>\n      <td>-0.00200</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>449.80</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0000</td>\n      <td>-0.00192</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>479.76</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0000</td>\n      <td>-0.00184</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>506.60</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0000</td>\n      <td>-0.00176</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>530.80</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0000</td>\n      <td>-0.00168</td>\n      <td>100</td>\n      <td>6000</td>\n      <td>5000</td>\n      <td>552.15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>351283</th>\n      <td>0.0024</td>\n      <td>0.00168</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>1365.50</td>\n    </tr>\n    <tr>\n      <th>351284</th>\n      <td>0.0024</td>\n      <td>0.00176</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>1247.20</td>\n    </tr>\n    <tr>\n      <th>351285</th>\n      <td>0.0024</td>\n      <td>0.00184</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>1114.10</td>\n    </tr>\n    <tr>\n      <th>351286</th>\n      <td>0.0024</td>\n      <td>0.00192</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>983.97</td>\n    </tr>\n    <tr>\n      <th>351287</th>\n      <td>0.0024</td>\n      <td>0.00200</td>\n      <td>500</td>\n      <td>9000</td>\n      <td>5000</td>\n      <td>942.84</td>\n    </tr>\n  </tbody>\n</table>\n<p>179928 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('C:/Users/erikm/Desktop/Diplomarbeit Erik Marr/Daten/Finish/Finish_ALL_D4_t_21_I_F_PKL.pkl')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft  Temperatur\n",
      "131733        0.0000      -0.00200        100   7000   6000      478.13\n",
      "131734        0.0000      -0.00192        100   7000   6000      511.16\n",
      "131735        0.0000      -0.00184        100   7000   6000      541.43\n",
      "131736        0.0000      -0.00176        100   7000   6000      569.14\n",
      "131737        0.0000      -0.00168        100   7000   6000      594.14\n",
      "...              ...           ...        ...    ...    ...         ...\n",
      "175639        0.0024       0.00168        500   7000   6000      942.96\n",
      "175640        0.0024       0.00176        500   7000   6000      865.10\n",
      "175641        0.0024       0.00184        500   7000   6000      786.99\n",
      "175642        0.0024       0.00192        500   7000   6000      708.11\n",
      "175643        0.0024       0.00200        500   7000   6000      690.48\n",
      "\n",
      "[22491 rows x 6 columns]\n",
      "Empty DataFrame\n",
      "Columns: [X-Koordinate, Y-Koordinate, Zeitpunkt, Strom, Kraft, Temperatur]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [X-Koordinate, Y-Koordinate, Zeitpunkt, Strom, Kraft, Temperatur]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/plain": "        X-Koordinate  Y-Koordinate  Zeitpunkt  Strom  Kraft\n0            0.00084       0.00040        260   7000   9000\n1            0.00084       0.00152        460   8000   7000\n2            0.00012      -0.00104        320   8000   7000\n3            0.00132       0.00120        400   8000   7000\n4            0.00132       0.00064        280   7000   9000\n...              ...           ...        ...    ...    ...\n157432       0.00228       0.00032        220   8000   7000\n157433       0.00204      -0.00112        340   8000   6000\n157434       0.00036       0.00168        460   8000   7000\n157435       0.00024       0.00104        320   9000   5000\n157436       0.00216      -0.00064        260   8000   7000\n\n[157437 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X-Koordinate</th>\n      <th>Y-Koordinate</th>\n      <th>Zeitpunkt</th>\n      <th>Strom</th>\n      <th>Kraft</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00084</td>\n      <td>0.00040</td>\n      <td>260</td>\n      <td>7000</td>\n      <td>9000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00084</td>\n      <td>0.00152</td>\n      <td>460</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00012</td>\n      <td>-0.00104</td>\n      <td>320</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.00132</td>\n      <td>0.00120</td>\n      <td>400</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.00132</td>\n      <td>0.00064</td>\n      <td>280</td>\n      <td>7000</td>\n      <td>9000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>157432</th>\n      <td>0.00228</td>\n      <td>0.00032</td>\n      <td>220</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>157433</th>\n      <td>0.00204</td>\n      <td>-0.00112</td>\n      <td>340</td>\n      <td>8000</td>\n      <td>6000</td>\n    </tr>\n    <tr>\n      <th>157434</th>\n      <td>0.00036</td>\n      <td>0.00168</td>\n      <td>460</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n    <tr>\n      <th>157435</th>\n      <td>0.00024</td>\n      <td>0.00104</td>\n      <td>320</td>\n      <td>9000</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>157436</th>\n      <td>0.00216</td>\n      <td>-0.00064</td>\n      <td>260</td>\n      <td>8000</td>\n      <td>7000</td>\n    </tr>\n  </tbody>\n</table>\n<p>157437 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mit der Bedingung wird die Simulationsdatei I7000 F6000 ausgewählt.\n",
    "bedingung = (data['Kraft'] == 6000) & (data['Strom'] == 7000)\n",
    "#df_test setzt sich aus der Simulation I7000 F6000 mit 21 Zeitschritten zwischen 100ms und 500 ms zusammen\n",
    "df_test = data[bedingung].copy()\n",
    "print(df_test)\n",
    "data_all = data.drop(df_test.index) # Testdaten aus den Trainingsdaten droppen\n",
    "print(data_all[(data_all['Kraft'] == 6000) & (data_all['Strom'] == 7000)]) # Kontrolle\n",
    "#print(data_all[(data_all['Kraft'] == 6000) & (data_all['Strom'] == 7000)])\n",
    "\n",
    "\n",
    "# df_test_500 setzt sich alleine aus dem Zeitpunkt t = 500ms für Simulation I7000 F6000 zusammen\n",
    "df_test_500 = df_test[(df_test['Zeitpunkt'] == 500)] #\n",
    "\n",
    "# Data shufflen\n",
    "data_all = data_all.sample(frac=1, random_state=42)  # Hier wird 42 als Random State verwendet, um die Ergebnisse reproduzierbar zu machen\n",
    "df_reset = data_all.reset_index(drop=True)\n",
    "df_reset\n",
    "\n",
    "#Zuweisung der Trainingsdaten\n",
    "y = df_reset[\"Temperatur\"]\n",
    "X = df_reset.drop(\"Temperatur\", axis=1)\n",
    "\n",
    "# Zuweisung der Testdaten\n",
    "y_2 = df_test[\"Temperatur\"]\n",
    "X_2 = df_test.drop(\"Temperatur\", axis=1)\n",
    "X_2 = X_2.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:49:43.011138100Z",
     "start_time": "2024-04-03T08:49:42.921908400Z"
    }
   },
   "id": "f3128fa3a4407365"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c705edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:49:44.108154500Z",
     "start_time": "2024-04-03T08:49:44.062867700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialisiere einen MinMaxScaler für die Features\n",
    "scaler_features = MinMaxScaler()\n",
    "# Skaliere X_train und X_test\n",
    "X_train_scaled = scaler_features.fit_transform(X)\n",
    "X_test_scaled = scaler_features.transform(X_2)\n",
    "# Initialisiere einen SEPARATEN MinMaxScaler für das Ziel, wenn nötig\n",
    "scaler_target = MinMaxScaler()\n",
    "# Skaliere y_train und y_test. Beachte, dass y_train.reshape(-1, 1) verwendet wird, da MinMaxScaler \n",
    "# erwartet, dass die Eingaben als 2D-Arrays kommen, und Ziele normalerweise als 1D-Arrays vorliegen.\n",
    "y_train_scaled = scaler_target.fit_transform(y.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler_target.transform(y_2.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbefe631e495b483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-03T08:50:18.804177400Z",
     "start_time": "2024-04-03T08:50:18.716232600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        , 0.33333333, 0.25      ],\n       [0.        , 0.02      , 0.        , 0.33333333, 0.25      ],\n       [0.        , 0.04      , 0.        , 0.33333333, 0.25      ],\n       ...,\n       [1.        , 0.96      , 1.        , 0.33333333, 0.25      ],\n       [1.        , 0.98      , 1.        , 0.33333333, 0.25      ],\n       [1.        , 1.        , 1.        , 0.33333333, 0.25      ]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.20797414],\n       [0.41215193],\n       [0.59782138],\n       ...,\n       [0.32599012],\n       [0.76202929],\n       [0.49529289]])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_scaled"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:49:45.225957800Z",
     "start_time": "2024-04-03T08:49:45.193575300Z"
    }
   },
   "id": "ce04ce43aac2242f"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "Epoch 1/2000\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "630/630 [==============================] - 3s 3ms/step - loss: 0.0257 - mae: 0.0471 - val_loss: 0.0206 - val_mae: 0.0291\n",
      "Epoch 2/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0191 - mae: 0.0201 - val_loss: 0.0180 - val_mae: 0.0159\n",
      "Epoch 3/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0173 - mae: 0.0144 - val_loss: 0.0166 - val_mae: 0.0118\n",
      "Epoch 4/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0161 - mae: 0.0116 - val_loss: 0.0155 - val_mae: 0.0103\n",
      "Epoch 5/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0150 - mae: 0.0104 - val_loss: 0.0145 - val_mae: 0.0097\n",
      "Epoch 6/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0140 - mae: 0.0096 - val_loss: 0.0136 - val_mae: 0.0107\n",
      "Epoch 7/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0132 - mae: 0.0095 - val_loss: 0.0127 - val_mae: 0.0081\n",
      "Epoch 8/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0124 - mae: 0.0086 - val_loss: 0.0120 - val_mae: 0.0082\n",
      "Epoch 9/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0117 - mae: 0.0087 - val_loss: 0.0113 - val_mae: 0.0074\n",
      "Epoch 10/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0110 - mae: 0.0081 - val_loss: 0.0108 - val_mae: 0.0088\n",
      "Epoch 11/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0105 - mae: 0.0079 - val_loss: 0.0102 - val_mae: 0.0073\n",
      "Epoch 12/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0100 - mae: 0.0081 - val_loss: 0.0097 - val_mae: 0.0075\n",
      "Epoch 13/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0095 - mae: 0.0074 - val_loss: 0.0093 - val_mae: 0.0071\n",
      "Epoch 14/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0091 - mae: 0.0073 - val_loss: 0.0089 - val_mae: 0.0066\n",
      "Epoch 15/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0087 - mae: 0.0072 - val_loss: 0.0086 - val_mae: 0.0092\n",
      "Epoch 16/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0084 - mae: 0.0071 - val_loss: 0.0082 - val_mae: 0.0065\n",
      "Epoch 17/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0080 - mae: 0.0072 - val_loss: 0.0079 - val_mae: 0.0068\n",
      "Epoch 18/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0078 - mae: 0.0070 - val_loss: 0.0076 - val_mae: 0.0059\n",
      "Epoch 19/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0075 - mae: 0.0070 - val_loss: 0.0073 - val_mae: 0.0061\n",
      "Epoch 20/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0072 - mae: 0.0067 - val_loss: 0.0072 - val_mae: 0.0084\n",
      "Epoch 21/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0070 - mae: 0.0067 - val_loss: 0.0069 - val_mae: 0.0055\n",
      "Epoch 22/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0068 - mae: 0.0068 - val_loss: 0.0067 - val_mae: 0.0075\n",
      "Epoch 23/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0066 - mae: 0.0065 - val_loss: 0.0065 - val_mae: 0.0059\n",
      "Epoch 24/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0064 - mae: 0.0064 - val_loss: 0.0063 - val_mae: 0.0063\n",
      "Epoch 25/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0062 - mae: 0.0063 - val_loss: 0.0061 - val_mae: 0.0065\n",
      "Epoch 26/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0060 - mae: 0.0064 - val_loss: 0.0059 - val_mae: 0.0068\n",
      "Epoch 27/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0058 - mae: 0.0059 - val_loss: 0.0057 - val_mae: 0.0052\n",
      "Epoch 28/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0057 - mae: 0.0063 - val_loss: 0.0056 - val_mae: 0.0058\n",
      "Epoch 29/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0055 - mae: 0.0061 - val_loss: 0.0054 - val_mae: 0.0057\n",
      "Epoch 30/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0053 - mae: 0.0059 - val_loss: 0.0053 - val_mae: 0.0052\n",
      "Epoch 31/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0052 - mae: 0.0061 - val_loss: 0.0051 - val_mae: 0.0051\n",
      "Epoch 32/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0050 - mae: 0.0057 - val_loss: 0.0051 - val_mae: 0.0107\n",
      "Epoch 33/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0049 - mae: 0.0059 - val_loss: 0.0048 - val_mae: 0.0061\n",
      "Epoch 34/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0048 - mae: 0.0059 - val_loss: 0.0047 - val_mae: 0.0050\n",
      "Epoch 35/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0047 - mae: 0.0058 - val_loss: 0.0046 - val_mae: 0.0059\n",
      "Epoch 36/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0045 - mae: 0.0055 - val_loss: 0.0044 - val_mae: 0.0048\n",
      "Epoch 37/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0044 - mae: 0.0054 - val_loss: 0.0043 - val_mae: 0.0053\n",
      "Epoch 38/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0043 - mae: 0.0058 - val_loss: 0.0042 - val_mae: 0.0047\n",
      "Epoch 39/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0042 - mae: 0.0054 - val_loss: 0.0041 - val_mae: 0.0073\n",
      "Epoch 40/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0041 - mae: 0.0058 - val_loss: 0.0040 - val_mae: 0.0055\n",
      "Epoch 41/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0039 - mae: 0.0054 - val_loss: 0.0039 - val_mae: 0.0057\n",
      "Epoch 42/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0038 - mae: 0.0053 - val_loss: 0.0038 - val_mae: 0.0051\n",
      "Epoch 43/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0037 - mae: 0.0056 - val_loss: 0.0037 - val_mae: 0.0060\n",
      "Epoch 44/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0036 - mae: 0.0053 - val_loss: 0.0036 - val_mae: 0.0080\n",
      "Epoch 45/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0036 - mae: 0.0054 - val_loss: 0.0035 - val_mae: 0.0057\n",
      "Epoch 46/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0035 - mae: 0.0053 - val_loss: 0.0035 - val_mae: 0.0092\n",
      "Epoch 47/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0034 - mae: 0.0052 - val_loss: 0.0033 - val_mae: 0.0045\n",
      "Epoch 48/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0033 - mae: 0.0052 - val_loss: 0.0033 - val_mae: 0.0055\n",
      "Epoch 49/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0032 - mae: 0.0054 - val_loss: 0.0032 - val_mae: 0.0044\n",
      "Epoch 50/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0031 - mae: 0.0050 - val_loss: 0.0031 - val_mae: 0.0050\n",
      "Epoch 51/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0031 - mae: 0.0052 - val_loss: 0.0030 - val_mae: 0.0056\n",
      "Epoch 52/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0030 - mae: 0.0051 - val_loss: 0.0029 - val_mae: 0.0047\n",
      "Epoch 53/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0029 - mae: 0.0050 - val_loss: 0.0029 - val_mae: 0.0051\n",
      "Epoch 54/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0028 - mae: 0.0050 - val_loss: 0.0028 - val_mae: 0.0057\n",
      "Epoch 55/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0028 - mae: 0.0051 - val_loss: 0.0027 - val_mae: 0.0053\n",
      "Epoch 56/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0027 - mae: 0.0050 - val_loss: 0.0027 - val_mae: 0.0057\n",
      "Epoch 57/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0026 - mae: 0.0049 - val_loss: 0.0026 - val_mae: 0.0048\n",
      "Epoch 58/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0026 - mae: 0.0048 - val_loss: 0.0025 - val_mae: 0.0046\n",
      "Epoch 59/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0025 - mae: 0.0050 - val_loss: 0.0025 - val_mae: 0.0041\n",
      "Epoch 60/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0024 - mae: 0.0046 - val_loss: 0.0024 - val_mae: 0.0049\n",
      "Epoch 61/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0024 - mae: 0.0051 - val_loss: 0.0023 - val_mae: 0.0040\n",
      "Epoch 62/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0023 - mae: 0.0046 - val_loss: 0.0023 - val_mae: 0.0047\n",
      "Epoch 63/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0023 - mae: 0.0047 - val_loss: 0.0022 - val_mae: 0.0042\n",
      "Epoch 64/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0022 - mae: 0.0050 - val_loss: 0.0022 - val_mae: 0.0050\n",
      "Epoch 65/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0022 - mae: 0.0047 - val_loss: 0.0021 - val_mae: 0.0046\n",
      "Epoch 66/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0021 - val_mae: 0.0043\n",
      "Epoch 67/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0020 - mae: 0.0046 - val_loss: 0.0020 - val_mae: 0.0048\n",
      "Epoch 68/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0020 - val_mae: 0.0054\n",
      "Epoch 69/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0019 - mae: 0.0046 - val_loss: 0.0019 - val_mae: 0.0040\n",
      "Epoch 70/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0039\n",
      "Epoch 71/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0019 - mae: 0.0046 - val_loss: 0.0018 - val_mae: 0.0046\n",
      "Epoch 72/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0053\n",
      "Epoch 73/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0018 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0043\n",
      "Epoch 74/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0041\n",
      "Epoch 75/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0017 - mae: 0.0045 - val_loss: 0.0017 - val_mae: 0.0040\n",
      "Epoch 76/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0017 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0041\n",
      "Epoch 77/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0016 - mae: 0.0044 - val_loss: 0.0016 - val_mae: 0.0046\n",
      "Epoch 78/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0041\n",
      "Epoch 79/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0015 - mae: 0.0044 - val_loss: 0.0015 - val_mae: 0.0041\n",
      "Epoch 80/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0015 - mae: 0.0044 - val_loss: 0.0015 - val_mae: 0.0044\n",
      "Epoch 81/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0054\n",
      "Epoch 82/2000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0039\n",
      "Epoch 83/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0014 - mae: 0.0043 - val_loss: 0.0014 - val_mae: 0.0045\n",
      "Epoch 84/2000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0045\n",
      "Epoch 85/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0039\n",
      "Epoch 86/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0013 - mae: 0.0043 - val_loss: 0.0013 - val_mae: 0.0049\n",
      "Epoch 87/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0037\n",
      "Epoch 88/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0045\n",
      "Epoch 89/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0042\n",
      "Epoch 90/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0012 - mae: 0.0043 - val_loss: 0.0012 - val_mae: 0.0047\n",
      "Epoch 91/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0046\n",
      "Epoch 92/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0012 - mae: 0.0042 - val_loss: 0.0012 - val_mae: 0.0048\n",
      "Epoch 93/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0038\n",
      "Epoch 94/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0043\n",
      "Epoch 95/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0040\n",
      "Epoch 96/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0048\n",
      "Epoch 97/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0010 - val_mae: 0.0037\n",
      "Epoch 98/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0010 - mae: 0.0042 - val_loss: 0.0010 - val_mae: 0.0038\n",
      "Epoch 99/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0010 - mae: 0.0042 - val_loss: 0.0010 - val_mae: 0.0055\n",
      "Epoch 100/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.8825e-04 - mae: 0.0041 - val_loss: 9.8577e-04 - val_mae: 0.0045\n",
      "Epoch 101/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.6915e-04 - mae: 0.0041 - val_loss: 9.5427e-04 - val_mae: 0.0036\n",
      "Epoch 102/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.5034e-04 - mae: 0.0041 - val_loss: 9.4123e-04 - val_mae: 0.0042\n",
      "Epoch 103/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.3409e-04 - mae: 0.0043 - val_loss: 9.2439e-04 - val_mae: 0.0043\n",
      "Epoch 104/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.1450e-04 - mae: 0.0041 - val_loss: 9.3146e-04 - val_mae: 0.0059\n",
      "Epoch 105/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.9806e-04 - mae: 0.0042 - val_loss: 8.8601e-04 - val_mae: 0.0039\n",
      "Epoch 106/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.8015e-04 - mae: 0.0041 - val_loss: 8.8532e-04 - val_mae: 0.0051\n",
      "Epoch 107/2000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.6583e-04 - mae: 0.0042 - val_loss: 8.5585e-04 - val_mae: 0.0039\n",
      "Epoch 108/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.4936e-04 - mae: 0.0041 - val_loss: 8.4503e-04 - val_mae: 0.0042\n",
      "Epoch 109/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.3269e-04 - mae: 0.0040 - val_loss: 8.2007e-04 - val_mae: 0.0035\n",
      "Epoch 110/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.2204e-04 - mae: 0.0043 - val_loss: 8.1044e-04 - val_mae: 0.0039\n",
      "Epoch 111/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.0654e-04 - mae: 0.0042 - val_loss: 8.0197e-04 - val_mae: 0.0045\n",
      "Epoch 112/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.9152e-04 - mae: 0.0041 - val_loss: 7.8297e-04 - val_mae: 0.0039\n",
      "Epoch 113/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.7872e-04 - mae: 0.0041 - val_loss: 7.7017e-04 - val_mae: 0.0040\n",
      "Epoch 114/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.6683e-04 - mae: 0.0042 - val_loss: 7.6111e-04 - val_mae: 0.0041\n",
      "Epoch 115/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.5186e-04 - mae: 0.0040 - val_loss: 7.4055e-04 - val_mae: 0.0035\n",
      "Epoch 116/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.4041e-04 - mae: 0.0041 - val_loss: 7.3906e-04 - val_mae: 0.0043\n",
      "Epoch 117/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.2605e-04 - mae: 0.0039 - val_loss: 7.1902e-04 - val_mae: 0.0038\n",
      "Epoch 118/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.1551e-04 - mae: 0.0040 - val_loss: 7.1103e-04 - val_mae: 0.0041\n",
      "Epoch 119/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.0288e-04 - mae: 0.0040 - val_loss: 6.9867e-04 - val_mae: 0.0042\n",
      "Epoch 120/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.9442e-04 - mae: 0.0042 - val_loss: 6.8318e-04 - val_mae: 0.0037\n",
      "Epoch 121/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.8238e-04 - mae: 0.0041 - val_loss: 6.7152e-04 - val_mae: 0.0036\n",
      "Epoch 122/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.7039e-04 - mae: 0.0040 - val_loss: 6.6743e-04 - val_mae: 0.0043\n",
      "Epoch 123/2000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.6140e-04 - mae: 0.0041 - val_loss: 6.5713e-04 - val_mae: 0.0040\n",
      "Epoch 124/2000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.4979e-04 - mae: 0.0039 - val_loss: 6.4031e-04 - val_mae: 0.0035\n",
      "Epoch 125/2000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.4118e-04 - mae: 0.0040 - val_loss: 6.4400e-04 - val_mae: 0.0047\n",
      "Epoch 126/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 6.3181e-04 - mae: 0.0040 - val_loss: 6.2047e-04 - val_mae: 0.0034\n",
      "Epoch 127/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 6.2171e-04 - mae: 0.0040 - val_loss: 6.1332e-04 - val_mae: 0.0035\n",
      "Epoch 128/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 6.1313e-04 - mae: 0.0040 - val_loss: 6.1211e-04 - val_mae: 0.0043\n",
      "Epoch 129/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 6.0435e-04 - mae: 0.0040 - val_loss: 6.1883e-04 - val_mae: 0.0050\n",
      "Epoch 130/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.9428e-04 - mae: 0.0039 - val_loss: 6.0162e-04 - val_mae: 0.0048\n",
      "Epoch 131/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.8742e-04 - mae: 0.0040 - val_loss: 5.7760e-04 - val_mae: 0.0035\n",
      "Epoch 132/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.7756e-04 - mae: 0.0039 - val_loss: 5.7635e-04 - val_mae: 0.0040\n",
      "Epoch 133/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.7094e-04 - mae: 0.0040 - val_loss: 5.6138e-04 - val_mae: 0.0035\n",
      "Epoch 134/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.6243e-04 - mae: 0.0039 - val_loss: 5.5353e-04 - val_mae: 0.0035\n",
      "Epoch 135/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.5448e-04 - mae: 0.0039 - val_loss: 5.4660e-04 - val_mae: 0.0035\n",
      "Epoch 136/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.4742e-04 - mae: 0.0040 - val_loss: 5.4005e-04 - val_mae: 0.0036\n",
      "Epoch 137/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.4006e-04 - mae: 0.0040 - val_loss: 5.4269e-04 - val_mae: 0.0045\n",
      "Epoch 138/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.3349e-04 - mae: 0.0040 - val_loss: 5.2661e-04 - val_mae: 0.0037\n",
      "Epoch 139/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.2562e-04 - mae: 0.0039 - val_loss: 5.2924e-04 - val_mae: 0.0045\n",
      "Epoch 140/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.1843e-04 - mae: 0.0039 - val_loss: 5.2530e-04 - val_mae: 0.0048\n",
      "Epoch 141/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.1317e-04 - mae: 0.0040 - val_loss: 5.2339e-04 - val_mae: 0.0051\n",
      "Epoch 142/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 5.0606e-04 - mae: 0.0039 - val_loss: 5.0462e-04 - val_mae: 0.0041\n",
      "Epoch 143/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.0005e-04 - mae: 0.0039 - val_loss: 5.0493e-04 - val_mae: 0.0047\n",
      "Epoch 144/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.9220e-04 - mae: 0.0038 - val_loss: 4.8467e-04 - val_mae: 0.0033\n",
      "Epoch 145/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.8836e-04 - mae: 0.0040 - val_loss: 4.8898e-04 - val_mae: 0.0043\n",
      "Epoch 146/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.8110e-04 - mae: 0.0038 - val_loss: 4.7344e-04 - val_mae: 0.0034\n",
      "Epoch 147/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.7473e-04 - mae: 0.0038 - val_loss: 4.7438e-04 - val_mae: 0.0041\n",
      "Epoch 148/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.6991e-04 - mae: 0.0039 - val_loss: 4.6519e-04 - val_mae: 0.0036\n",
      "Epoch 149/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.6482e-04 - mae: 0.0039 - val_loss: 4.5729e-04 - val_mae: 0.0033\n",
      "Epoch 150/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.5985e-04 - mae: 0.0039 - val_loss: 4.6841e-04 - val_mae: 0.0047\n",
      "Epoch 151/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.5355e-04 - mae: 0.0038 - val_loss: 4.4752e-04 - val_mae: 0.0034\n",
      "Epoch 152/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 4.4896e-04 - mae: 0.0039 - val_loss: 4.4253e-04 - val_mae: 0.0034\n",
      "Epoch 153/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.4412e-04 - mae: 0.0039 - val_loss: 4.8405e-04 - val_mae: 0.0065\n",
      "Epoch 154/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.4097e-04 - mae: 0.0040 - val_loss: 4.4352e-04 - val_mae: 0.0045\n",
      "Epoch 155/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.3381e-04 - mae: 0.0038 - val_loss: 4.3334e-04 - val_mae: 0.0040\n",
      "Epoch 156/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.3049e-04 - mae: 0.0038 - val_loss: 4.2320e-04 - val_mae: 0.0033\n",
      "Epoch 157/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.2373e-04 - mae: 0.0037 - val_loss: 4.2256e-04 - val_mae: 0.0037\n",
      "Epoch 158/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.2146e-04 - mae: 0.0039 - val_loss: 4.1302e-04 - val_mae: 0.0032\n",
      "Epoch 159/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.1546e-04 - mae: 0.0037 - val_loss: 4.1342e-04 - val_mae: 0.0037\n",
      "Epoch 160/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.1287e-04 - mae: 0.0039 - val_loss: 4.0449e-04 - val_mae: 0.0032\n",
      "Epoch 161/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.0835e-04 - mae: 0.0038 - val_loss: 4.0613e-04 - val_mae: 0.0039\n",
      "Epoch 162/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 4.0318e-04 - mae: 0.0037 - val_loss: 4.0320e-04 - val_mae: 0.0040\n",
      "Epoch 163/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9958e-04 - mae: 0.0037 - val_loss: 4.0135e-04 - val_mae: 0.0040\n",
      "Epoch 164/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9646e-04 - mae: 0.0038 - val_loss: 3.9183e-04 - val_mae: 0.0036\n",
      "Epoch 165/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.9144e-04 - mae: 0.0037 - val_loss: 3.8955e-04 - val_mae: 0.0037\n",
      "Epoch 166/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.8871e-04 - mae: 0.0038 - val_loss: 3.8680e-04 - val_mae: 0.0038\n",
      "Epoch 167/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.8485e-04 - mae: 0.0038 - val_loss: 3.8863e-04 - val_mae: 0.0043\n",
      "Epoch 168/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.8171e-04 - mae: 0.0038 - val_loss: 3.7799e-04 - val_mae: 0.0036\n",
      "Epoch 169/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.7670e-04 - mae: 0.0037 - val_loss: 3.9366e-04 - val_mae: 0.0053\n",
      "Epoch 170/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.7443e-04 - mae: 0.0038 - val_loss: 3.7052e-04 - val_mae: 0.0035\n",
      "Epoch 171/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.7168e-04 - mae: 0.0038 - val_loss: 3.7055e-04 - val_mae: 0.0037\n",
      "Epoch 172/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.6744e-04 - mae: 0.0037 - val_loss: 3.6907e-04 - val_mae: 0.0040\n",
      "Epoch 173/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.6547e-04 - mae: 0.0038 - val_loss: 3.6016e-04 - val_mae: 0.0034\n",
      "Epoch 174/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.6102e-04 - mae: 0.0037 - val_loss: 3.8304e-04 - val_mae: 0.0058\n",
      "Epoch 175/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.6133e-04 - mae: 0.0040 - val_loss: 3.6477e-04 - val_mae: 0.0044\n",
      "Epoch 176/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.5433e-04 - mae: 0.0036 - val_loss: 3.6423e-04 - val_mae: 0.0048\n",
      "Epoch 177/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.5307e-04 - mae: 0.0038 - val_loss: 3.4962e-04 - val_mae: 0.0036\n",
      "Epoch 178/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.4938e-04 - mae: 0.0037 - val_loss: 3.5925e-04 - val_mae: 0.0046\n",
      "Epoch 179/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.4756e-04 - mae: 0.0038 - val_loss: 3.4495e-04 - val_mae: 0.0035\n",
      "Epoch 180/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.4481e-04 - mae: 0.0038 - val_loss: 3.3961e-04 - val_mae: 0.0034\n",
      "Epoch 181/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.4112e-04 - mae: 0.0037 - val_loss: 3.3771e-04 - val_mae: 0.0033\n",
      "Epoch 182/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.3776e-04 - mae: 0.0036 - val_loss: 3.3918e-04 - val_mae: 0.0038\n",
      "Epoch 183/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.3674e-04 - mae: 0.0038 - val_loss: 3.3784e-04 - val_mae: 0.0040\n",
      "Epoch 184/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.3334e-04 - mae: 0.0037 - val_loss: 3.3200e-04 - val_mae: 0.0036\n",
      "Epoch 185/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.3073e-04 - mae: 0.0037 - val_loss: 3.3388e-04 - val_mae: 0.0041\n",
      "Epoch 186/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2711e-04 - mae: 0.0036 - val_loss: 3.4375e-04 - val_mae: 0.0050\n",
      "Epoch 187/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2693e-04 - mae: 0.0038 - val_loss: 3.4160e-04 - val_mae: 0.0051\n",
      "Epoch 188/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.2290e-04 - mae: 0.0036 - val_loss: 3.1795e-04 - val_mae: 0.0031\n",
      "Epoch 189/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.2200e-04 - mae: 0.0037 - val_loss: 3.2411e-04 - val_mae: 0.0039\n",
      "Epoch 190/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.1905e-04 - mae: 0.0037 - val_loss: 3.2025e-04 - val_mae: 0.0038\n",
      "Epoch 191/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 3.1622e-04 - mae: 0.0036 - val_loss: 3.1440e-04 - val_mae: 0.0035\n",
      "Epoch 192/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.1411e-04 - mae: 0.0036 - val_loss: 3.1914e-04 - val_mae: 0.0044\n",
      "Epoch 193/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.1159e-04 - mae: 0.0036 - val_loss: 3.1002e-04 - val_mae: 0.0035\n",
      "Epoch 194/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.1042e-04 - mae: 0.0037 - val_loss: 3.1428e-04 - val_mae: 0.0042\n",
      "Epoch 195/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.0706e-04 - mae: 0.0036 - val_loss: 3.0906e-04 - val_mae: 0.0038\n",
      "Epoch 196/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.0688e-04 - mae: 0.0037 - val_loss: 3.0258e-04 - val_mae: 0.0033\n",
      "Epoch 197/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 3.0403e-04 - mae: 0.0037 - val_loss: 3.0719e-04 - val_mae: 0.0040\n",
      "Epoch 198/2000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0150e-04 - mae: 0.0036 - val_loss: 3.0756e-04 - val_mae: 0.0042\n",
      "Epoch 199/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.9982e-04 - mae: 0.0037 - val_loss: 3.0193e-04 - val_mae: 0.0039\n",
      "Epoch 200/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.9784e-04 - mae: 0.0037 - val_loss: 2.9456e-04 - val_mae: 0.0033\n",
      "Epoch 201/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.9745e-04 - mae: 0.0038 - val_loss: 2.9416e-04 - val_mae: 0.0035\n",
      "Epoch 202/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.9455e-04 - mae: 0.0037 - val_loss: 3.0594e-04 - val_mae: 0.0047\n",
      "Epoch 203/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.9259e-04 - mae: 0.0037 - val_loss: 2.9638e-04 - val_mae: 0.0041\n",
      "Epoch 204/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.8957e-04 - mae: 0.0035 - val_loss: 2.8720e-04 - val_mae: 0.0034\n",
      "Epoch 205/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.8928e-04 - mae: 0.0037 - val_loss: 2.8577e-04 - val_mae: 0.0034\n",
      "Epoch 206/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.8638e-04 - mae: 0.0036 - val_loss: 2.8310e-04 - val_mae: 0.0033\n",
      "Epoch 207/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.8693e-04 - mae: 0.0038 - val_loss: 2.9041e-04 - val_mae: 0.0041\n",
      "Epoch 208/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.8310e-04 - mae: 0.0036 - val_loss: 2.7809e-04 - val_mae: 0.0031\n",
      "Epoch 209/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.8201e-04 - mae: 0.0036 - val_loss: 2.7779e-04 - val_mae: 0.0032\n",
      "Epoch 210/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.8023e-04 - mae: 0.0036 - val_loss: 2.7774e-04 - val_mae: 0.0033\n",
      "Epoch 211/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.7807e-04 - mae: 0.0035 - val_loss: 2.7424e-04 - val_mae: 0.0032\n",
      "Epoch 212/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.7950e-04 - mae: 0.0038 - val_loss: 2.8372e-04 - val_mae: 0.0042\n",
      "Epoch 213/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.7658e-04 - mae: 0.0037 - val_loss: 2.7416e-04 - val_mae: 0.0035\n",
      "Epoch 214/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.7388e-04 - mae: 0.0036 - val_loss: 2.8927e-04 - val_mae: 0.0051\n",
      "Epoch 215/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.7416e-04 - mae: 0.0037 - val_loss: 2.7636e-04 - val_mae: 0.0038\n",
      "Epoch 216/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.7246e-04 - mae: 0.0037 - val_loss: 2.6671e-04 - val_mae: 0.0031\n",
      "Epoch 217/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.6924e-04 - mae: 0.0035 - val_loss: 2.6783e-04 - val_mae: 0.0034\n",
      "Epoch 218/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.6850e-04 - mae: 0.0036 - val_loss: 2.6449e-04 - val_mae: 0.0032\n",
      "Epoch 219/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.6742e-04 - mae: 0.0036 - val_loss: 2.6461e-04 - val_mae: 0.0033\n",
      "Epoch 220/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.6583e-04 - mae: 0.0036 - val_loss: 2.6221e-04 - val_mae: 0.0032\n",
      "Epoch 221/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.6345e-04 - mae: 0.0035 - val_loss: 2.6362e-04 - val_mae: 0.0034\n",
      "Epoch 222/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.6489e-04 - mae: 0.0037 - val_loss: 2.6094e-04 - val_mae: 0.0035\n",
      "Epoch 223/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.6159e-04 - mae: 0.0036 - val_loss: 2.5777e-04 - val_mae: 0.0032\n",
      "Epoch 224/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.6121e-04 - mae: 0.0036 - val_loss: 2.5731e-04 - val_mae: 0.0031\n",
      "Epoch 225/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.5871e-04 - mae: 0.0035 - val_loss: 2.5802e-04 - val_mae: 0.0034\n",
      "Epoch 226/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.5847e-04 - mae: 0.0036 - val_loss: 2.6333e-04 - val_mae: 0.0041\n",
      "Epoch 227/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.5672e-04 - mae: 0.0036 - val_loss: 2.5561e-04 - val_mae: 0.0035\n",
      "Epoch 228/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.5667e-04 - mae: 0.0036 - val_loss: 2.5208e-04 - val_mae: 0.0033\n",
      "Epoch 229/2000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5440e-04 - mae: 0.0036 - val_loss: 2.5070e-04 - val_mae: 0.0031\n",
      "Epoch 230/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.5374e-04 - mae: 0.0036 - val_loss: 2.5230e-04 - val_mae: 0.0035\n",
      "Epoch 231/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.5191e-04 - mae: 0.0035 - val_loss: 2.5057e-04 - val_mae: 0.0033\n",
      "Epoch 232/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.5082e-04 - mae: 0.0036 - val_loss: 2.5104e-04 - val_mae: 0.0036\n",
      "Epoch 233/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.5165e-04 - mae: 0.0037 - val_loss: 2.5240e-04 - val_mae: 0.0036\n",
      "Epoch 234/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.4884e-04 - mae: 0.0036 - val_loss: 2.5711e-04 - val_mae: 0.0046\n",
      "Epoch 235/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.4912e-04 - mae: 0.0037 - val_loss: 2.4371e-04 - val_mae: 0.0031\n",
      "Epoch 236/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.4732e-04 - mae: 0.0036 - val_loss: 2.5417e-04 - val_mae: 0.0041\n",
      "Epoch 237/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.4573e-04 - mae: 0.0036 - val_loss: 2.4947e-04 - val_mae: 0.0038\n",
      "Epoch 238/2000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 2.4466e-04 - mae: 0.0035 - val_loss: 2.4444e-04 - val_mae: 0.0035\n",
      "Epoch 239/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.4351e-04 - mae: 0.0035 - val_loss: 2.4591e-04 - val_mae: 0.0036\n",
      "Epoch 240/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.4212e-04 - mae: 0.0035 - val_loss: 2.3765e-04 - val_mae: 0.0030\n",
      "Epoch 241/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.4308e-04 - mae: 0.0037 - val_loss: 2.3927e-04 - val_mae: 0.0033\n",
      "Epoch 242/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.4056e-04 - mae: 0.0035 - val_loss: 2.3796e-04 - val_mae: 0.0032\n",
      "Epoch 243/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.3942e-04 - mae: 0.0035 - val_loss: 2.4126e-04 - val_mae: 0.0036\n",
      "Epoch 244/2000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.3824e-04 - mae: 0.0035 - val_loss: 2.4788e-04 - val_mae: 0.0045\n",
      "Epoch 245/2000\n",
      "616/630 [============================>.] - ETA: 0s - loss: 2.3914e-04 - mae: 0.0037Restoring model weights from the end of the best epoch: 240.\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 2.3919e-04 - mae: 0.0037 - val_loss: 2.4072e-04 - val_mae: 0.0037\n",
      "Epoch 245: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Netzwerkarchitektur\n",
    "model = Sequential([\n",
    "\n",
    "    Dense(280, activation='relu', input_shape=(5,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(232, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(152, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(40, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(88, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(8, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(280, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "    \n",
    "    Dense(1 , activation = 'linear')\n",
    "])\n",
    "\n",
    "# Optimierer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# Modell kompilieren (Verwendung von mean_squared_error als Verlustfunktion für Regression)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])  # Metriken für Regression: Mean Absolute Error und Mean Squared Error\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# Trainingsparameter\n",
    "batch_size = 200 \n",
    "epochs = 2000\n",
    "\n",
    "# Modell trainieren (Annahme: X_train, y_train, X_val, y_val sind vordefiniert)\n",
    "history = model.fit(X_train_scaled, y_train_scaled,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping])\n",
    "\n",
    "model.save('D4_t_21_I_F_1.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T22:27:02.943868200Z",
     "start_time": "2024-04-02T22:17:36.126475200Z"
    }
   },
   "id": "8b52e1a9a6ff3aeb"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training für Fold 1...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 5s 5ms/step - loss: 0.0350 - mae: 0.0691 - val_loss: 0.0230 - val_mae: 0.0373\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0213 - mae: 0.0297 - val_loss: 0.0201 - val_mae: 0.0247\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0193 - mae: 0.0207 - val_loss: 0.0186 - val_mae: 0.0177\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0182 - mae: 0.0159 - val_loss: 0.0177 - val_mae: 0.0141\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0174 - mae: 0.0135 - val_loss: 0.0170 - val_mae: 0.0120\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0167 - mae: 0.0119 - val_loss: 0.0163 - val_mae: 0.0114\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0160 - mae: 0.0107 - val_loss: 0.0157 - val_mae: 0.0099\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0154 - mae: 0.0105 - val_loss: 0.0152 - val_mae: 0.0123\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0148 - mae: 0.0101 - val_loss: 0.0145 - val_mae: 0.0096\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0142 - mae: 0.0093 - val_loss: 0.0139 - val_mae: 0.0090\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0136 - mae: 0.0093 - val_loss: 0.0133 - val_mae: 0.0085\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0131 - mae: 0.0087 - val_loss: 0.0129 - val_mae: 0.0121\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0126 - mae: 0.0087 - val_loss: 0.0123 - val_mae: 0.0075\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0121 - mae: 0.0085 - val_loss: 0.0118 - val_mae: 0.0070\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0116 - mae: 0.0083 - val_loss: 0.0113 - val_mae: 0.0085\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0111 - mae: 0.0079 - val_loss: 0.0109 - val_mae: 0.0071\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0107 - mae: 0.0080 - val_loss: 0.0106 - val_mae: 0.0117\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0103 - mae: 0.0076 - val_loss: 0.0101 - val_mae: 0.0077\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 3s 5ms/step - loss: 0.0099 - mae: 0.0076 - val_loss: 0.0097 - val_mae: 0.0068\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0095 - mae: 0.0075 - val_loss: 0.0094 - val_mae: 0.0084\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0092 - mae: 0.0073 - val_loss: 0.0090 - val_mae: 0.0090\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0088 - mae: 0.0071 - val_loss: 0.0087 - val_mae: 0.0086\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0085 - mae: 0.0071 - val_loss: 0.0084 - val_mae: 0.0071\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0083 - mae: 0.0073 - val_loss: 0.0081 - val_mae: 0.0061\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0080 - mae: 0.0071 - val_loss: 0.0079 - val_mae: 0.0069\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0077 - mae: 0.0065 - val_loss: 0.0076 - val_mae: 0.0063\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0075 - mae: 0.0067 - val_loss: 0.0074 - val_mae: 0.0073\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0073 - mae: 0.0066 - val_loss: 0.0071 - val_mae: 0.0063\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0070 - mae: 0.0067 - val_loss: 0.0069 - val_mae: 0.0068\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0068 - mae: 0.0063 - val_loss: 0.0067 - val_mae: 0.0074\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0066 - mae: 0.0066 - val_loss: 0.0066 - val_mae: 0.0089\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0064 - mae: 0.0064 - val_loss: 0.0063 - val_mae: 0.0062\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0062 - mae: 0.0063 - val_loss: 0.0061 - val_mae: 0.0058\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0060 - mae: 0.0062 - val_loss: 0.0060 - val_mae: 0.0066\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0059 - mae: 0.0064 - val_loss: 0.0058 - val_mae: 0.0062\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0057 - mae: 0.0060 - val_loss: 0.0056 - val_mae: 0.0054\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0055 - mae: 0.0060 - val_loss: 0.0055 - val_mae: 0.0064\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0054 - mae: 0.0062 - val_loss: 0.0053 - val_mae: 0.0054\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0052 - mae: 0.0058 - val_loss: 0.0051 - val_mae: 0.0062\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0051 - mae: 0.0059 - val_loss: 0.0050 - val_mae: 0.0053\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0049 - mae: 0.0056 - val_loss: 0.0049 - val_mae: 0.0072\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0048 - mae: 0.0055 - val_loss: 0.0047 - val_mae: 0.0057\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0046 - mae: 0.0058 - val_loss: 0.0046 - val_mae: 0.0055\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0045 - mae: 0.0056 - val_loss: 0.0044 - val_mae: 0.0047\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0044 - mae: 0.0057 - val_loss: 0.0043 - val_mae: 0.0062\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0042 - mae: 0.0057 - val_loss: 0.0042 - val_mae: 0.0068\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0041 - mae: 0.0054 - val_loss: 0.0041 - val_mae: 0.0048\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0040 - mae: 0.0055 - val_loss: 0.0040 - val_mae: 0.0052\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0039 - mae: 0.0055 - val_loss: 0.0039 - val_mae: 0.0067\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0038 - mae: 0.0053 - val_loss: 0.0037 - val_mae: 0.0045\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0037 - mae: 0.0054 - val_loss: 0.0036 - val_mae: 0.0056\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0036 - mae: 0.0052 - val_loss: 0.0035 - val_mae: 0.0048\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0035 - mae: 0.0051 - val_loss: 0.0034 - val_mae: 0.0056\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0034 - mae: 0.0052 - val_loss: 0.0034 - val_mae: 0.0059\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0033 - mae: 0.0051 - val_loss: 0.0032 - val_mae: 0.0049\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0032 - mae: 0.0051 - val_loss: 0.0032 - val_mae: 0.0050\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0031 - mae: 0.0051 - val_loss: 0.0031 - val_mae: 0.0050\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0030 - mae: 0.0050 - val_loss: 0.0030 - val_mae: 0.0055\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0029 - mae: 0.0052 - val_loss: 0.0029 - val_mae: 0.0055\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0029 - mae: 0.0050 - val_loss: 0.0028 - val_mae: 0.0050\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0028 - mae: 0.0050 - val_loss: 0.0027 - val_mae: 0.0047\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0027 - mae: 0.0050 - val_loss: 0.0027 - val_mae: 0.0041\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0026 - mae: 0.0050 - val_loss: 0.0026 - val_mae: 0.0050\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0026 - mae: 0.0049 - val_loss: 0.0025 - val_mae: 0.0044\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0025 - mae: 0.0049 - val_loss: 0.0025 - val_mae: 0.0048\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0024 - mae: 0.0050 - val_loss: 0.0024 - val_mae: 0.0069\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0024 - mae: 0.0047 - val_loss: 0.0024 - val_mae: 0.0062\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0023 - mae: 0.0047 - val_loss: 0.0023 - val_mae: 0.0086\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0022 - mae: 0.0050 - val_loss: 0.0023 - val_mae: 0.0087\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0022 - mae: 0.0046 - val_loss: 0.0021 - val_mae: 0.0041\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0021 - mae: 0.0048 - val_loss: 0.0021 - val_mae: 0.0043\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0021 - val_mae: 0.0049\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0020 - val_mae: 0.0045\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0047 - val_loss: 0.0020 - val_mae: 0.0050\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0044\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0058\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0046 - val_loss: 0.0018 - val_mae: 0.0044\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0056\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0050\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0045 - val_loss: 0.0017 - val_mae: 0.0052\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0045 - val_loss: 0.0017 - val_mae: 0.0060\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0044\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0016 - mae: 0.0044 - val_loss: 0.0016 - val_mae: 0.0040\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0045\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0040\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0044 - val_loss: 0.0015 - val_mae: 0.0041\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0041\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0048\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0053\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0048\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0042 - val_loss: 0.0013 - val_mae: 0.0037\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0040\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0042\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0042 - val_loss: 0.0012 - val_mae: 0.0044\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0043 - val_loss: 0.0012 - val_mae: 0.0038\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0041 - val_loss: 0.0012 - val_mae: 0.0058\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0040\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0037\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0041 - val_loss: 0.0011 - val_mae: 0.0038\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0052\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0045\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0010 - val_mae: 0.0036\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0010 - mae: 0.0041 - val_loss: 0.0010 - val_mae: 0.0044\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0010 - mae: 0.0041 - val_loss: 0.0010 - val_mae: 0.0045\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.8931e-04 - mae: 0.0041 - val_loss: 9.7958e-04 - val_mae: 0.0040\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.6876e-04 - mae: 0.0040 - val_loss: 9.8904e-04 - val_mae: 0.0064\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.5131e-04 - mae: 0.0041 - val_loss: 9.3936e-04 - val_mae: 0.0038\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.3364e-04 - mae: 0.0041 - val_loss: 9.2029e-04 - val_mae: 0.0036\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.1718e-04 - mae: 0.0042 - val_loss: 9.0518e-04 - val_mae: 0.0038\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.9938e-04 - mae: 0.0041 - val_loss: 8.8445e-04 - val_mae: 0.0034\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.8485e-04 - mae: 0.0042 - val_loss: 8.7246e-04 - val_mae: 0.0038\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.6880e-04 - mae: 0.0041 - val_loss: 8.5444e-04 - val_mae: 0.0035\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 8.5373e-04 - mae: 0.0041 - val_loss: 8.4191e-04 - val_mae: 0.0036\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 8.3875e-04 - mae: 0.0041 - val_loss: 8.3028e-04 - val_mae: 0.0039\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 8.2350e-04 - mae: 0.0040 - val_loss: 8.1537e-04 - val_mae: 0.0039\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.1184e-04 - mae: 0.0042 - val_loss: 7.9878e-04 - val_mae: 0.0035\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 7.9619e-04 - mae: 0.0040 - val_loss: 7.8514e-04 - val_mae: 0.0035\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.8347e-04 - mae: 0.0040 - val_loss: 7.7090e-04 - val_mae: 0.0035\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.7004e-04 - mae: 0.0040 - val_loss: 7.6037e-04 - val_mae: 0.0035\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.6080e-04 - mae: 0.0042 - val_loss: 7.6086e-04 - val_mae: 0.0049\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.4610e-04 - mae: 0.0040 - val_loss: 7.3772e-04 - val_mae: 0.0038\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.3428e-04 - mae: 0.0040 - val_loss: 7.3087e-04 - val_mae: 0.0040\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.2330e-04 - mae: 0.0040 - val_loss: 7.3224e-04 - val_mae: 0.0054\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.1199e-04 - mae: 0.0040 - val_loss: 7.0707e-04 - val_mae: 0.0040\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.0046e-04 - mae: 0.0040 - val_loss: 6.9398e-04 - val_mae: 0.0039\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.8938e-04 - mae: 0.0039 - val_loss: 6.7925e-04 - val_mae: 0.0035\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.8122e-04 - mae: 0.0041 - val_loss: 6.7159e-04 - val_mae: 0.0037\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.6939e-04 - mae: 0.0040 - val_loss: 6.7406e-04 - val_mae: 0.0048\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.6058e-04 - mae: 0.0040 - val_loss: 6.6834e-04 - val_mae: 0.0052\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.5131e-04 - mae: 0.0041 - val_loss: 6.4267e-04 - val_mae: 0.0036\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.4092e-04 - mae: 0.0039 - val_loss: 6.4523e-04 - val_mae: 0.0049\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.3456e-04 - mae: 0.0041 - val_loss: 6.2820e-04 - val_mae: 0.0039\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.2396e-04 - mae: 0.0039 - val_loss: 6.1483e-04 - val_mae: 0.0034\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.1761e-04 - mae: 0.0041 - val_loss: 6.0634e-04 - val_mae: 0.0034\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.0834e-04 - mae: 0.0040 - val_loss: 6.0253e-04 - val_mae: 0.0038\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.9860e-04 - mae: 0.0038 - val_loss: 5.9129e-04 - val_mae: 0.0035\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.9076e-04 - mae: 0.0039 - val_loss: 5.8670e-04 - val_mae: 0.0038\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.8316e-04 - mae: 0.0039 - val_loss: 5.7971e-04 - val_mae: 0.0041\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.7642e-04 - mae: 0.0040 - val_loss: 5.7711e-04 - val_mae: 0.0043\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.6938e-04 - mae: 0.0040 - val_loss: 5.7271e-04 - val_mae: 0.0047\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.6212e-04 - mae: 0.0040 - val_loss: 5.6640e-04 - val_mae: 0.0047\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.5447e-04 - mae: 0.0039 - val_loss: 5.4675e-04 - val_mae: 0.0036\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.4792e-04 - mae: 0.0039 - val_loss: 5.4236e-04 - val_mae: 0.0038\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.4164e-04 - mae: 0.0040 - val_loss: 5.3519e-04 - val_mae: 0.0037\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.3393e-04 - mae: 0.0038 - val_loss: 5.3224e-04 - val_mae: 0.0039\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.2644e-04 - mae: 0.0038 - val_loss: 5.2302e-04 - val_mae: 0.0037\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.2207e-04 - mae: 0.0040 - val_loss: 5.3214e-04 - val_mae: 0.0050\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.1497e-04 - mae: 0.0039 - val_loss: 5.0817e-04 - val_mae: 0.0035\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.0889e-04 - mae: 0.0039 - val_loss: 5.0145e-04 - val_mae: 0.0034\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.0391e-04 - mae: 0.0039 - val_loss: 4.9625e-04 - val_mae: 0.0035\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.9672e-04 - mae: 0.0038 - val_loss: 4.8905e-04 - val_mae: 0.0033\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.9226e-04 - mae: 0.0039 - val_loss: 4.9192e-04 - val_mae: 0.0039\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.8549e-04 - mae: 0.0038 - val_loss: 4.8423e-04 - val_mae: 0.0039\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.8138e-04 - mae: 0.0039 - val_loss: 5.0264e-04 - val_mae: 0.0059\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.7609e-04 - mae: 0.0039 - val_loss: 4.6853e-04 - val_mae: 0.0034\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.7026e-04 - mae: 0.0038 - val_loss: 4.9131e-04 - val_mae: 0.0058\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.6607e-04 - mae: 0.0039 - val_loss: 4.7589e-04 - val_mae: 0.0048\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.6041e-04 - mae: 0.0038 - val_loss: 4.6110e-04 - val_mae: 0.0041\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.5745e-04 - mae: 0.0040 - val_loss: 4.4752e-04 - val_mae: 0.0031\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.5024e-04 - mae: 0.0037 - val_loss: 4.4643e-04 - val_mae: 0.0036\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.4713e-04 - mae: 0.0039 - val_loss: 4.5322e-04 - val_mae: 0.0047\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.4211e-04 - mae: 0.0038 - val_loss: 4.3948e-04 - val_mae: 0.0038\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.3733e-04 - mae: 0.0038 - val_loss: 4.4195e-04 - val_mae: 0.0043\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.3351e-04 - mae: 0.0038 - val_loss: 4.2733e-04 - val_mae: 0.0035\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.3014e-04 - mae: 0.0039 - val_loss: 4.2265e-04 - val_mae: 0.0034\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.2578e-04 - mae: 0.0039 - val_loss: 4.4102e-04 - val_mae: 0.0050\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.2101e-04 - mae: 0.0038 - val_loss: 4.1429e-04 - val_mae: 0.0033\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.1622e-04 - mae: 0.0037 - val_loss: 4.1085e-04 - val_mae: 0.0032\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.1411e-04 - mae: 0.0038 - val_loss: 4.0585e-04 - val_mae: 0.0032\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0972e-04 - mae: 0.0038 - val_loss: 4.0421e-04 - val_mae: 0.0035\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0469e-04 - mae: 0.0037 - val_loss: 4.0189e-04 - val_mae: 0.0035\n",
      "Epoch 172/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0224e-04 - mae: 0.0038 - val_loss: 4.3551e-04 - val_mae: 0.0060\n",
      "Epoch 173/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.9890e-04 - mae: 0.0038 - val_loss: 3.9755e-04 - val_mae: 0.0039\n",
      "Epoch 174/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.9434e-04 - mae: 0.0037 - val_loss: 3.9641e-04 - val_mae: 0.0040\n",
      "Epoch 175/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.9194e-04 - mae: 0.0038 - val_loss: 3.9440e-04 - val_mae: 0.0041\n",
      "Epoch 176/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8790e-04 - mae: 0.0037 - val_loss: 3.9540e-04 - val_mae: 0.0047\n",
      "Epoch 177/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8410e-04 - mae: 0.0037 - val_loss: 3.9723e-04 - val_mae: 0.0052\n",
      "Epoch 178/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8086e-04 - mae: 0.0037 - val_loss: 3.7836e-04 - val_mae: 0.0036\n",
      "Epoch 179/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7845e-04 - mae: 0.0038 - val_loss: 3.7109e-04 - val_mae: 0.0031\n",
      "Epoch 180/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7454e-04 - mae: 0.0037 - val_loss: 3.7321e-04 - val_mae: 0.0036\n",
      "Epoch 181/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7271e-04 - mae: 0.0038 - val_loss: 3.6771e-04 - val_mae: 0.0034\n",
      "Epoch 182/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6785e-04 - mae: 0.0036 - val_loss: 3.6197e-04 - val_mae: 0.0031\n",
      "Epoch 183/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6532e-04 - mae: 0.0036 - val_loss: 4.2110e-04 - val_mae: 0.0078\n",
      "Epoch 184/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6346e-04 - mae: 0.0037 - val_loss: 3.5533e-04 - val_mae: 0.0030\n",
      "Epoch 185/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6157e-04 - mae: 0.0038 - val_loss: 3.5996e-04 - val_mae: 0.0038\n",
      "Epoch 186/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5638e-04 - mae: 0.0036 - val_loss: 3.7056e-04 - val_mae: 0.0050\n",
      "Epoch 187/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5441e-04 - mae: 0.0037 - val_loss: 3.5623e-04 - val_mae: 0.0038\n",
      "Epoch 188/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5192e-04 - mae: 0.0037 - val_loss: 3.4460e-04 - val_mae: 0.0030\n",
      "Epoch 189/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5078e-04 - mae: 0.0038 - val_loss: 3.4767e-04 - val_mae: 0.0036\n",
      "Epoch 190/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4614e-04 - mae: 0.0036 - val_loss: 3.5008e-04 - val_mae: 0.0040\n",
      "Epoch 191/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4540e-04 - mae: 0.0037 - val_loss: 3.3753e-04 - val_mae: 0.0030\n",
      "Epoch 192/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4023e-04 - mae: 0.0035 - val_loss: 3.3533e-04 - val_mae: 0.0031\n",
      "Epoch 193/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3951e-04 - mae: 0.0037 - val_loss: 3.4174e-04 - val_mae: 0.0042\n",
      "Epoch 194/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3798e-04 - mae: 0.0038 - val_loss: 3.3707e-04 - val_mae: 0.0038\n",
      "Epoch 195/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3502e-04 - mae: 0.0037 - val_loss: 3.2897e-04 - val_mae: 0.0031\n",
      "Epoch 196/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3354e-04 - mae: 0.0037 - val_loss: 3.3351e-04 - val_mae: 0.0038\n",
      "Epoch 197/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2854e-04 - mae: 0.0035 - val_loss: 3.2338e-04 - val_mae: 0.0030\n",
      "Epoch 198/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2657e-04 - mae: 0.0035 - val_loss: 3.2207e-04 - val_mae: 0.0032\n",
      "Epoch 199/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2629e-04 - mae: 0.0037 - val_loss: 3.2963e-04 - val_mae: 0.0042\n",
      "Epoch 200/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2327e-04 - mae: 0.0036 - val_loss: 3.2249e-04 - val_mae: 0.0037\n",
      "Epoch 201/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2151e-04 - mae: 0.0037 - val_loss: 3.1654e-04 - val_mae: 0.0033\n",
      "Epoch 202/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1840e-04 - mae: 0.0036 - val_loss: 3.3502e-04 - val_mae: 0.0051\n",
      "Epoch 203/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1813e-04 - mae: 0.0037 - val_loss: 3.5118e-04 - val_mae: 0.0064\n",
      "Epoch 204/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1553e-04 - mae: 0.0037 - val_loss: 3.2799e-04 - val_mae: 0.0049\n",
      "Epoch 205/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1347e-04 - mae: 0.0037 - val_loss: 3.1123e-04 - val_mae: 0.0035\n",
      "Epoch 206/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1095e-04 - mae: 0.0036 - val_loss: 3.0594e-04 - val_mae: 0.0032\n",
      "Epoch 207/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0931e-04 - mae: 0.0036 - val_loss: 3.1045e-04 - val_mae: 0.0039\n",
      "Epoch 208/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0680e-04 - mae: 0.0036 - val_loss: 3.0229e-04 - val_mae: 0.0032\n",
      "Epoch 209/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0568e-04 - mae: 0.0036 - val_loss: 2.9995e-04 - val_mae: 0.0032\n",
      "Epoch 210/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0439e-04 - mae: 0.0037 - val_loss: 2.9918e-04 - val_mae: 0.0032\n",
      "Epoch 211/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0314e-04 - mae: 0.0037 - val_loss: 2.9574e-04 - val_mae: 0.0029\n",
      "Epoch 212/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9878e-04 - mae: 0.0035 - val_loss: 2.9562e-04 - val_mae: 0.0032\n",
      "Epoch 213/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9897e-04 - mae: 0.0037 - val_loss: 3.0381e-04 - val_mae: 0.0041\n",
      "Epoch 214/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9662e-04 - mae: 0.0036 - val_loss: 2.9299e-04 - val_mae: 0.0033\n",
      "Epoch 215/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9589e-04 - mae: 0.0037 - val_loss: 2.9929e-04 - val_mae: 0.0043\n",
      "Epoch 216/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9374e-04 - mae: 0.0036 - val_loss: 2.9412e-04 - val_mae: 0.0038\n",
      "Epoch 217/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9148e-04 - mae: 0.0036 - val_loss: 2.9388e-04 - val_mae: 0.0039\n",
      "Epoch 218/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9000e-04 - mae: 0.0036 - val_loss: 2.8504e-04 - val_mae: 0.0031\n",
      "Epoch 219/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8913e-04 - mae: 0.0037 - val_loss: 2.9334e-04 - val_mae: 0.0042\n",
      "Epoch 220/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8674e-04 - mae: 0.0036 - val_loss: 2.8583e-04 - val_mae: 0.0035\n",
      "Epoch 221/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8568e-04 - mae: 0.0036 - val_loss: 2.8313e-04 - val_mae: 0.0034\n",
      "Epoch 222/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8407e-04 - mae: 0.0036 - val_loss: 2.8887e-04 - val_mae: 0.0043\n",
      "Epoch 223/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8305e-04 - mae: 0.0037 - val_loss: 2.7902e-04 - val_mae: 0.0033\n",
      "Epoch 224/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7943e-04 - mae: 0.0035 - val_loss: 2.7516e-04 - val_mae: 0.0030\n",
      "Epoch 225/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8127e-04 - mae: 0.0038 - val_loss: 2.9692e-04 - val_mae: 0.0050\n",
      "Epoch 226/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7934e-04 - mae: 0.0037 - val_loss: 2.7373e-04 - val_mae: 0.0031\n",
      "Epoch 227/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7600e-04 - mae: 0.0035 - val_loss: 2.7292e-04 - val_mae: 0.0032\n",
      "Epoch 228/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7537e-04 - mae: 0.0036 - val_loss: 2.7176e-04 - val_mae: 0.0033\n",
      "Epoch 229/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7410e-04 - mae: 0.0036 - val_loss: 2.6835e-04 - val_mae: 0.0030\n",
      "Epoch 230/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7199e-04 - mae: 0.0036 - val_loss: 2.6811e-04 - val_mae: 0.0032\n",
      "Epoch 231/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.7243e-04 - mae: 0.0037 - val_loss: 2.7179e-04 - val_mae: 0.0037\n",
      "Epoch 232/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7034e-04 - mae: 0.0036 - val_loss: 2.7365e-04 - val_mae: 0.0041\n",
      "Epoch 233/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6834e-04 - mae: 0.0036 - val_loss: 2.7084e-04 - val_mae: 0.0039\n",
      "Epoch 234/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6877e-04 - mae: 0.0037 - val_loss: 2.6425e-04 - val_mae: 0.0032\n",
      "Epoch 235/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6699e-04 - mae: 0.0036 - val_loss: 2.6486e-04 - val_mae: 0.0035\n",
      "Epoch 236/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6502e-04 - mae: 0.0036 - val_loss: 2.6213e-04 - val_mae: 0.0032\n",
      "Epoch 237/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6455e-04 - mae: 0.0037 - val_loss: 2.6080e-04 - val_mae: 0.0032\n",
      "Epoch 238/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6396e-04 - mae: 0.0037 - val_loss: 2.6514e-04 - val_mae: 0.0038\n",
      "Epoch 239/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6144e-04 - mae: 0.0036 - val_loss: 2.7274e-04 - val_mae: 0.0047\n",
      "Epoch 240/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5946e-04 - mae: 0.0035 - val_loss: 2.5617e-04 - val_mae: 0.0032\n",
      "Epoch 241/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6146e-04 - mae: 0.0038 - val_loss: 2.6012e-04 - val_mae: 0.0037\n",
      "Epoch 242/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5824e-04 - mae: 0.0036 - val_loss: 2.5494e-04 - val_mae: 0.0033\n",
      "Epoch 243/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5933e-04 - mae: 0.0038 - val_loss: 2.5622e-04 - val_mae: 0.0034\n",
      "Epoch 244/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5584e-04 - mae: 0.0036 - val_loss: 2.5095e-04 - val_mae: 0.0030\n",
      "Epoch 245/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5588e-04 - mae: 0.0037 - val_loss: 2.5595e-04 - val_mae: 0.0036\n",
      "Epoch 246/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5459e-04 - mae: 0.0036 - val_loss: 2.4923e-04 - val_mae: 0.0031\n",
      "Epoch 247/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5267e-04 - mae: 0.0035 - val_loss: 2.4884e-04 - val_mae: 0.0032\n",
      "Epoch 248/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5246e-04 - mae: 0.0036 - val_loss: 2.6238e-04 - val_mae: 0.0043\n",
      "Epoch 249/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5176e-04 - mae: 0.0036 - val_loss: 2.6190e-04 - val_mae: 0.0047\n",
      "Epoch 250/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5159e-04 - mae: 0.0037 - val_loss: 2.5503e-04 - val_mae: 0.0042\n",
      "Epoch 251/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4865e-04 - mae: 0.0035 - val_loss: 2.4723e-04 - val_mae: 0.0034\n",
      "Epoch 252/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4851e-04 - mae: 0.0036 - val_loss: 2.4373e-04 - val_mae: 0.0031\n",
      "Epoch 253/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4820e-04 - mae: 0.0037 - val_loss: 2.5204e-04 - val_mae: 0.0042\n",
      "Epoch 254/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4599e-04 - mae: 0.0036 - val_loss: 2.4818e-04 - val_mae: 0.0039\n",
      "Epoch 255/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4657e-04 - mae: 0.0037 - val_loss: 2.4596e-04 - val_mae: 0.0037\n",
      "Epoch 256/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4514e-04 - mae: 0.0036 - val_loss: 2.4410e-04 - val_mae: 0.0035\n",
      "Epoch 257/1000\n",
      "622/630 [============================>.] - ETA: 0s - loss: 2.4386e-04 - mae: 0.0036Restoring model weights from the end of the best epoch: 252.\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4391e-04 - mae: 0.0036 - val_loss: 2.4909e-04 - val_mae: 0.0041\n",
      "Epoch 257: early stopping\n",
      "Training für Fold 2...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 4s 4ms/step - loss: 0.0331 - mae: 0.0658 - val_loss: 0.0218 - val_mae: 0.0340\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0203 - mae: 0.0281 - val_loss: 0.0190 - val_mae: 0.0217\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0182 - mae: 0.0191 - val_loss: 0.0175 - val_mae: 0.0164\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0170 - mae: 0.0152 - val_loss: 0.0164 - val_mae: 0.0133\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0159 - mae: 0.0124 - val_loss: 0.0155 - val_mae: 0.0113\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0151 - mae: 0.0111 - val_loss: 0.0147 - val_mae: 0.0119\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0143 - mae: 0.0103 - val_loss: 0.0139 - val_mae: 0.0107\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0135 - mae: 0.0096 - val_loss: 0.0131 - val_mae: 0.0096\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0128 - mae: 0.0092 - val_loss: 0.0125 - val_mae: 0.0097\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0121 - mae: 0.0089 - val_loss: 0.0118 - val_mae: 0.0085\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0115 - mae: 0.0087 - val_loss: 0.0112 - val_mae: 0.0092\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0109 - mae: 0.0084 - val_loss: 0.0107 - val_mae: 0.0092\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0104 - mae: 0.0084 - val_loss: 0.0101 - val_mae: 0.0073\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0099 - mae: 0.0076 - val_loss: 0.0097 - val_mae: 0.0084\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0094 - mae: 0.0076 - val_loss: 0.0092 - val_mae: 0.0080\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0090 - mae: 0.0076 - val_loss: 0.0088 - val_mae: 0.0075\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0086 - mae: 0.0077 - val_loss: 0.0084 - val_mae: 0.0066\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0083 - mae: 0.0071 - val_loss: 0.0082 - val_mae: 0.0091\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0080 - mae: 0.0072 - val_loss: 0.0078 - val_mae: 0.0069\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0077 - mae: 0.0073 - val_loss: 0.0075 - val_mae: 0.0067\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0074 - mae: 0.0069 - val_loss: 0.0073 - val_mae: 0.0069\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0071 - mae: 0.0071 - val_loss: 0.0070 - val_mae: 0.0058\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0069 - mae: 0.0068 - val_loss: 0.0068 - val_mae: 0.0073\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0067 - mae: 0.0070 - val_loss: 0.0066 - val_mae: 0.0062\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0065 - mae: 0.0065 - val_loss: 0.0064 - val_mae: 0.0057\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0063 - mae: 0.0067 - val_loss: 0.0062 - val_mae: 0.0056\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0061 - mae: 0.0066 - val_loss: 0.0060 - val_mae: 0.0056\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0059 - mae: 0.0066 - val_loss: 0.0058 - val_mae: 0.0060\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0058 - mae: 0.0062 - val_loss: 0.0057 - val_mae: 0.0058\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0056 - mae: 0.0060 - val_loss: 0.0055 - val_mae: 0.0054\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0055 - mae: 0.0062 - val_loss: 0.0054 - val_mae: 0.0057\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0053 - mae: 0.0062 - val_loss: 0.0052 - val_mae: 0.0052\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0052 - mae: 0.0060 - val_loss: 0.0051 - val_mae: 0.0068\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0050 - mae: 0.0059 - val_loss: 0.0050 - val_mae: 0.0075\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0049 - mae: 0.0059 - val_loss: 0.0048 - val_mae: 0.0072\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0047 - mae: 0.0058 - val_loss: 0.0047 - val_mae: 0.0062\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0046 - mae: 0.0059 - val_loss: 0.0046 - val_mae: 0.0066\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0045 - mae: 0.0056 - val_loss: 0.0044 - val_mae: 0.0056\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0044 - mae: 0.0058 - val_loss: 0.0043 - val_mae: 0.0059\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0043 - mae: 0.0055 - val_loss: 0.0042 - val_mae: 0.0048\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0041 - mae: 0.0054 - val_loss: 0.0041 - val_mae: 0.0056\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0040 - mae: 0.0056 - val_loss: 0.0040 - val_mae: 0.0055\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0039 - mae: 0.0056 - val_loss: 0.0039 - val_mae: 0.0056\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0038 - mae: 0.0055 - val_loss: 0.0038 - val_mae: 0.0045\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0037 - mae: 0.0055 - val_loss: 0.0037 - val_mae: 0.0064\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0036 - mae: 0.0052 - val_loss: 0.0036 - val_mae: 0.0050\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0035 - mae: 0.0054 - val_loss: 0.0035 - val_mae: 0.0046\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0035 - mae: 0.0054 - val_loss: 0.0034 - val_mae: 0.0044\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0034 - mae: 0.0054 - val_loss: 0.0033 - val_mae: 0.0047\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0033 - mae: 0.0051 - val_loss: 0.0032 - val_mae: 0.0054\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0032 - mae: 0.0052 - val_loss: 0.0032 - val_mae: 0.0059\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0031 - mae: 0.0051 - val_loss: 0.0031 - val_mae: 0.0051\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0030 - mae: 0.0051 - val_loss: 0.0030 - val_mae: 0.0044\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0030 - mae: 0.0051 - val_loss: 0.0029 - val_mae: 0.0055\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0029 - mae: 0.0050 - val_loss: 0.0028 - val_mae: 0.0046\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0028 - mae: 0.0050 - val_loss: 0.0028 - val_mae: 0.0048\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0027 - mae: 0.0050 - val_loss: 0.0027 - val_mae: 0.0054\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0027 - mae: 0.0050 - val_loss: 0.0026 - val_mae: 0.0048\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0026 - mae: 0.0050 - val_loss: 0.0026 - val_mae: 0.0048\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0026 - mae: 0.0051 - val_loss: 0.0025 - val_mae: 0.0059\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0025 - mae: 0.0048 - val_loss: 0.0025 - val_mae: 0.0051\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0024 - mae: 0.0049 - val_loss: 0.0024 - val_mae: 0.0042\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0024 - mae: 0.0050 - val_loss: 0.0024 - val_mae: 0.0056\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0023 - mae: 0.0049 - val_loss: 0.0023 - val_mae: 0.0049\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0023 - mae: 0.0051 - val_loss: 0.0022 - val_mae: 0.0047\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0022 - mae: 0.0046 - val_loss: 0.0022 - val_mae: 0.0047\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0022 - mae: 0.0049 - val_loss: 0.0021 - val_mae: 0.0053\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0021 - mae: 0.0048 - val_loss: 0.0021 - val_mae: 0.0039\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0021 - mae: 0.0048 - val_loss: 0.0021 - val_mae: 0.0063\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0046 - val_loss: 0.0020 - val_mae: 0.0040\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0049 - val_loss: 0.0019 - val_mae: 0.0052\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0068\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0019 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0049\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0046 - val_loss: 0.0018 - val_mae: 0.0038\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0043\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0045\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0039\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0044 - val_loss: 0.0016 - val_mae: 0.0042\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0055\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0049\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0042\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0044 - val_loss: 0.0015 - val_mae: 0.0039\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0044 - val_loss: 0.0015 - val_mae: 0.0055\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0041\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0045\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0038\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0058\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0038\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0039\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0053\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0055\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0043 - val_loss: 0.0012 - val_mae: 0.0045\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0041\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0041\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0042\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0054\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0039\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0040\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0010 - val_mae: 0.0047\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0010 - mae: 0.0044 - val_loss: 0.0010 - val_mae: 0.0050\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0010 - mae: 0.0044 - val_loss: 0.0010 - val_mae: 0.0053\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.9366e-04 - mae: 0.0043 - val_loss: 9.9149e-04 - val_mae: 0.0050\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.7488e-04 - mae: 0.0043 - val_loss: 9.6173e-04 - val_mae: 0.0040\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.5760e-04 - mae: 0.0044 - val_loss: 9.5758e-04 - val_mae: 0.0051\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.3730e-04 - mae: 0.0042 - val_loss: 9.4298e-04 - val_mae: 0.0053\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.2028e-04 - mae: 0.0043 - val_loss: 9.6058e-04 - val_mae: 0.0078\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.0122e-04 - mae: 0.0042 - val_loss: 8.9319e-04 - val_mae: 0.0042\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.8531e-04 - mae: 0.0043 - val_loss: 8.7188e-04 - val_mae: 0.0039\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.6947e-04 - mae: 0.0043 - val_loss: 8.6155e-04 - val_mae: 0.0044\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.5063e-04 - mae: 0.0041 - val_loss: 8.5007e-04 - val_mae: 0.0048\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.3653e-04 - mae: 0.0043 - val_loss: 8.2051e-04 - val_mae: 0.0036\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.1993e-04 - mae: 0.0042 - val_loss: 8.0833e-04 - val_mae: 0.0038\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.0843e-04 - mae: 0.0044 - val_loss: 7.9980e-04 - val_mae: 0.0043\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.9196e-04 - mae: 0.0042 - val_loss: 7.9643e-04 - val_mae: 0.0050\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.8110e-04 - mae: 0.0044 - val_loss: 7.6621e-04 - val_mae: 0.0037\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.6651e-04 - mae: 0.0043 - val_loss: 7.8006e-04 - val_mae: 0.0057\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.5351e-04 - mae: 0.0042 - val_loss: 7.4427e-04 - val_mae: 0.0040\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.4079e-04 - mae: 0.0042 - val_loss: 7.4215e-04 - val_mae: 0.0047\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.2792e-04 - mae: 0.0041 - val_loss: 7.1508e-04 - val_mae: 0.0035\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.1675e-04 - mae: 0.0042 - val_loss: 7.1320e-04 - val_mae: 0.0046\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.0545e-04 - mae: 0.0042 - val_loss: 7.1145e-04 - val_mae: 0.0053\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.9283e-04 - mae: 0.0041 - val_loss: 6.9512e-04 - val_mae: 0.0048\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.8413e-04 - mae: 0.0043 - val_loss: 6.8044e-04 - val_mae: 0.0047\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.7033e-04 - mae: 0.0041 - val_loss: 6.7588e-04 - val_mae: 0.0048\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.6162e-04 - mae: 0.0042 - val_loss: 6.5109e-04 - val_mae: 0.0038\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.4978e-04 - mae: 0.0041 - val_loss: 6.4380e-04 - val_mae: 0.0040\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.4107e-04 - mae: 0.0042 - val_loss: 6.3144e-04 - val_mae: 0.0037\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.3184e-04 - mae: 0.0042 - val_loss: 6.2149e-04 - val_mae: 0.0038\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.2092e-04 - mae: 0.0041 - val_loss: 6.1292e-04 - val_mae: 0.0037\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.1167e-04 - mae: 0.0041 - val_loss: 6.0555e-04 - val_mae: 0.0039\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.0313e-04 - mae: 0.0041 - val_loss: 6.1552e-04 - val_mae: 0.0053\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.9429e-04 - mae: 0.0041 - val_loss: 5.9968e-04 - val_mae: 0.0047\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.8573e-04 - mae: 0.0041 - val_loss: 5.8953e-04 - val_mae: 0.0047\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.7788e-04 - mae: 0.0041 - val_loss: 5.7441e-04 - val_mae: 0.0042\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.7031e-04 - mae: 0.0041 - val_loss: 5.6934e-04 - val_mae: 0.0043\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.6191e-04 - mae: 0.0041 - val_loss: 5.6993e-04 - val_mae: 0.0051\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.5447e-04 - mae: 0.0041 - val_loss: 5.4932e-04 - val_mae: 0.0041\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 5.4725e-04 - mae: 0.0041 - val_loss: 5.4120e-04 - val_mae: 0.0040\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.3968e-04 - mae: 0.0040 - val_loss: 5.3165e-04 - val_mae: 0.0036\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.3182e-04 - mae: 0.0040 - val_loss: 5.2615e-04 - val_mae: 0.0039\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.2652e-04 - mae: 0.0041 - val_loss: 5.3014e-04 - val_mae: 0.0046\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.1884e-04 - mae: 0.0040 - val_loss: 5.1795e-04 - val_mae: 0.0041\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.1446e-04 - mae: 0.0042 - val_loss: 5.0761e-04 - val_mae: 0.0039\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.0466e-04 - mae: 0.0039 - val_loss: 4.9773e-04 - val_mae: 0.0036\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.9958e-04 - mae: 0.0040 - val_loss: 4.9399e-04 - val_mae: 0.0038\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.9437e-04 - mae: 0.0041 - val_loss: 5.1247e-04 - val_mae: 0.0059\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.8757e-04 - mae: 0.0040 - val_loss: 4.9502e-04 - val_mae: 0.0050\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.8145e-04 - mae: 0.0040 - val_loss: 4.8990e-04 - val_mae: 0.0050\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.7629e-04 - mae: 0.0040 - val_loss: 4.7644e-04 - val_mae: 0.0043\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.7050e-04 - mae: 0.0039 - val_loss: 4.8273e-04 - val_mae: 0.0052\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.6445e-04 - mae: 0.0039 - val_loss: 4.5902e-04 - val_mae: 0.0038\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.5992e-04 - mae: 0.0040 - val_loss: 4.5083e-04 - val_mae: 0.0034\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.5313e-04 - mae: 0.0039 - val_loss: 4.5939e-04 - val_mae: 0.0045\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.4805e-04 - mae: 0.0039 - val_loss: 4.4434e-04 - val_mae: 0.0038\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.4252e-04 - mae: 0.0038 - val_loss: 4.3837e-04 - val_mae: 0.0036\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.3775e-04 - mae: 0.0038 - val_loss: 4.3785e-04 - val_mae: 0.0041\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.3373e-04 - mae: 0.0039 - val_loss: 4.2546e-04 - val_mae: 0.0033\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.2834e-04 - mae: 0.0039 - val_loss: 4.2193e-04 - val_mae: 0.0036\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.2508e-04 - mae: 0.0040 - val_loss: 4.1807e-04 - val_mae: 0.0036\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.1976e-04 - mae: 0.0039 - val_loss: 4.1112e-04 - val_mae: 0.0033\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.1385e-04 - mae: 0.0037 - val_loss: 4.0994e-04 - val_mae: 0.0036\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.1273e-04 - mae: 0.0040 - val_loss: 4.0511e-04 - val_mae: 0.0034\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0605e-04 - mae: 0.0038 - val_loss: 4.1496e-04 - val_mae: 0.0048\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0338e-04 - mae: 0.0039 - val_loss: 3.9563e-04 - val_mae: 0.0033\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.9815e-04 - mae: 0.0038 - val_loss: 3.9357e-04 - val_mae: 0.0036\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.9531e-04 - mae: 0.0039 - val_loss: 4.1461e-04 - val_mae: 0.0058\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.9113e-04 - mae: 0.0038 - val_loss: 3.8568e-04 - val_mae: 0.0035\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8782e-04 - mae: 0.0039 - val_loss: 3.8038e-04 - val_mae: 0.0034\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8328e-04 - mae: 0.0038 - val_loss: 3.9130e-04 - val_mae: 0.0048\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8004e-04 - mae: 0.0038 - val_loss: 3.7707e-04 - val_mae: 0.0038\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7636e-04 - mae: 0.0038 - val_loss: 3.7137e-04 - val_mae: 0.0035\n",
      "Epoch 172/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7275e-04 - mae: 0.0038 - val_loss: 3.6731e-04 - val_mae: 0.0035\n",
      "Epoch 173/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7088e-04 - mae: 0.0039 - val_loss: 3.6195e-04 - val_mae: 0.0033\n",
      "Epoch 174/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6637e-04 - mae: 0.0038 - val_loss: 3.6287e-04 - val_mae: 0.0037\n",
      "Epoch 175/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6296e-04 - mae: 0.0038 - val_loss: 3.6487e-04 - val_mae: 0.0041\n",
      "Epoch 176/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5984e-04 - mae: 0.0038 - val_loss: 3.5292e-04 - val_mae: 0.0034\n",
      "Epoch 177/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5774e-04 - mae: 0.0039 - val_loss: 3.6403e-04 - val_mae: 0.0044\n",
      "Epoch 178/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5217e-04 - mae: 0.0037 - val_loss: 3.5323e-04 - val_mae: 0.0040\n",
      "Epoch 179/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5015e-04 - mae: 0.0037 - val_loss: 3.5483e-04 - val_mae: 0.0042\n",
      "Epoch 180/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4741e-04 - mae: 0.0038 - val_loss: 3.4026e-04 - val_mae: 0.0032\n",
      "Epoch 181/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4357e-04 - mae: 0.0037 - val_loss: 3.4152e-04 - val_mae: 0.0036\n",
      "Epoch 182/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4249e-04 - mae: 0.0038 - val_loss: 3.6146e-04 - val_mae: 0.0055\n",
      "Epoch 183/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4060e-04 - mae: 0.0039 - val_loss: 3.3384e-04 - val_mae: 0.0034\n",
      "Epoch 184/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3431e-04 - mae: 0.0036 - val_loss: 3.3466e-04 - val_mae: 0.0037\n",
      "Epoch 185/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3475e-04 - mae: 0.0038 - val_loss: 3.2923e-04 - val_mae: 0.0035\n",
      "Epoch 186/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3198e-04 - mae: 0.0038 - val_loss: 3.2879e-04 - val_mae: 0.0037\n",
      "Epoch 187/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2873e-04 - mae: 0.0038 - val_loss: 3.2927e-04 - val_mae: 0.0040\n",
      "Epoch 188/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2586e-04 - mae: 0.0037 - val_loss: 3.3917e-04 - val_mae: 0.0052\n",
      "Epoch 189/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2379e-04 - mae: 0.0038 - val_loss: 3.2467e-04 - val_mae: 0.0040\n",
      "Epoch 190/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2017e-04 - mae: 0.0037 - val_loss: 3.1387e-04 - val_mae: 0.0031\n",
      "Epoch 191/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2066e-04 - mae: 0.0039 - val_loss: 3.9154e-04 - val_mae: 0.0082\n",
      "Epoch 192/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1736e-04 - mae: 0.0038 - val_loss: 3.3166e-04 - val_mae: 0.0048\n",
      "Epoch 193/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1355e-04 - mae: 0.0037 - val_loss: 3.1042e-04 - val_mae: 0.0033\n",
      "Epoch 194/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1236e-04 - mae: 0.0038 - val_loss: 3.2258e-04 - val_mae: 0.0044\n",
      "Epoch 195/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0884e-04 - mae: 0.0036 - val_loss: 3.1033e-04 - val_mae: 0.0038\n",
      "Epoch 196/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0739e-04 - mae: 0.0037 - val_loss: 3.1022e-04 - val_mae: 0.0041\n",
      "Epoch 197/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0639e-04 - mae: 0.0038 - val_loss: 3.0177e-04 - val_mae: 0.0035\n",
      "Epoch 198/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0466e-04 - mae: 0.0038 - val_loss: 3.0907e-04 - val_mae: 0.0041\n",
      "Epoch 199/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0076e-04 - mae: 0.0036 - val_loss: 3.1210e-04 - val_mae: 0.0047\n",
      "Epoch 200/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9980e-04 - mae: 0.0037 - val_loss: 3.3825e-04 - val_mae: 0.0066\n",
      "Epoch 201/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9917e-04 - mae: 0.0038 - val_loss: 2.9285e-04 - val_mae: 0.0033\n",
      "Epoch 202/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9529e-04 - mae: 0.0037 - val_loss: 2.8958e-04 - val_mae: 0.0032\n",
      "Epoch 203/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9345e-04 - mae: 0.0037 - val_loss: 2.8929e-04 - val_mae: 0.0034\n",
      "Epoch 204/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9173e-04 - mae: 0.0037 - val_loss: 3.0270e-04 - val_mae: 0.0047\n",
      "Epoch 205/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9023e-04 - mae: 0.0037 - val_loss: 3.1093e-04 - val_mae: 0.0057\n",
      "Epoch 206/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8773e-04 - mae: 0.0036 - val_loss: 2.8506e-04 - val_mae: 0.0034\n",
      "Epoch 207/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8709e-04 - mae: 0.0037 - val_loss: 2.8577e-04 - val_mae: 0.0037\n",
      "Epoch 208/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8423e-04 - mae: 0.0036 - val_loss: 2.8276e-04 - val_mae: 0.0035\n",
      "Epoch 209/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8231e-04 - mae: 0.0036 - val_loss: 2.7849e-04 - val_mae: 0.0034\n",
      "Epoch 210/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8150e-04 - mae: 0.0037 - val_loss: 3.0471e-04 - val_mae: 0.0057\n",
      "Epoch 211/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8042e-04 - mae: 0.0037 - val_loss: 2.7534e-04 - val_mae: 0.0034\n",
      "Epoch 212/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7733e-04 - mae: 0.0036 - val_loss: 2.8419e-04 - val_mae: 0.0045\n",
      "Epoch 213/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7700e-04 - mae: 0.0037 - val_loss: 2.7354e-04 - val_mae: 0.0035\n",
      "Epoch 214/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7507e-04 - mae: 0.0037 - val_loss: 2.7748e-04 - val_mae: 0.0041\n",
      "Epoch 215/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7379e-04 - mae: 0.0037 - val_loss: 2.7105e-04 - val_mae: 0.0036\n",
      "Epoch 216/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7247e-04 - mae: 0.0037 - val_loss: 2.7199e-04 - val_mae: 0.0037\n",
      "Epoch 217/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7115e-04 - mae: 0.0037 - val_loss: 2.7896e-04 - val_mae: 0.0044\n",
      "Epoch 218/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6925e-04 - mae: 0.0037 - val_loss: 2.6974e-04 - val_mae: 0.0038\n",
      "Epoch 219/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6725e-04 - mae: 0.0036 - val_loss: 2.6243e-04 - val_mae: 0.0032\n",
      "Epoch 220/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6565e-04 - mae: 0.0036 - val_loss: 2.6275e-04 - val_mae: 0.0034\n",
      "Epoch 221/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6413e-04 - mae: 0.0036 - val_loss: 2.5825e-04 - val_mae: 0.0031\n",
      "Epoch 222/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6362e-04 - mae: 0.0037 - val_loss: 2.6364e-04 - val_mae: 0.0038\n",
      "Epoch 223/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6210e-04 - mae: 0.0037 - val_loss: 2.6835e-04 - val_mae: 0.0044\n",
      "Epoch 224/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6150e-04 - mae: 0.0037 - val_loss: 2.5594e-04 - val_mae: 0.0032\n",
      "Epoch 225/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5819e-04 - mae: 0.0035 - val_loss: 2.5428e-04 - val_mae: 0.0032\n",
      "Epoch 226/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5880e-04 - mae: 0.0037 - val_loss: 2.5553e-04 - val_mae: 0.0034\n",
      "Epoch 227/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5628e-04 - mae: 0.0036 - val_loss: 2.6828e-04 - val_mae: 0.0049\n",
      "Epoch 228/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5606e-04 - mae: 0.0037 - val_loss: 2.5913e-04 - val_mae: 0.0041\n",
      "Epoch 229/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5388e-04 - mae: 0.0036 - val_loss: 2.6514e-04 - val_mae: 0.0048\n",
      "Epoch 230/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5486e-04 - mae: 0.0038 - val_loss: 2.5171e-04 - val_mae: 0.0035\n",
      "Epoch 231/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5061e-04 - mae: 0.0035 - val_loss: 2.4819e-04 - val_mae: 0.0034\n",
      "Epoch 232/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5040e-04 - mae: 0.0036 - val_loss: 2.4728e-04 - val_mae: 0.0034\n",
      "Epoch 233/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4926e-04 - mae: 0.0036 - val_loss: 2.5270e-04 - val_mae: 0.0040\n",
      "Epoch 234/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4834e-04 - mae: 0.0036 - val_loss: 2.4402e-04 - val_mae: 0.0032\n",
      "Epoch 235/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4669e-04 - mae: 0.0036 - val_loss: 2.5649e-04 - val_mae: 0.0046\n",
      "Epoch 236/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4615e-04 - mae: 0.0036 - val_loss: 2.4311e-04 - val_mae: 0.0033\n",
      "Epoch 237/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4581e-04 - mae: 0.0037 - val_loss: 2.4055e-04 - val_mae: 0.0033\n",
      "Epoch 238/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4436e-04 - mae: 0.0037 - val_loss: 2.4004e-04 - val_mae: 0.0032\n",
      "Epoch 239/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4263e-04 - mae: 0.0036 - val_loss: 2.4257e-04 - val_mae: 0.0036\n",
      "Epoch 240/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4171e-04 - mae: 0.0036 - val_loss: 2.3760e-04 - val_mae: 0.0032\n",
      "Epoch 241/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4045e-04 - mae: 0.0036 - val_loss: 2.4028e-04 - val_mae: 0.0037\n",
      "Epoch 242/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3913e-04 - mae: 0.0036 - val_loss: 2.3844e-04 - val_mae: 0.0034\n",
      "Epoch 243/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3994e-04 - mae: 0.0037 - val_loss: 2.5257e-04 - val_mae: 0.0050\n",
      "Epoch 244/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3686e-04 - mae: 0.0035 - val_loss: 2.5127e-04 - val_mae: 0.0046\n",
      "Epoch 245/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3671e-04 - mae: 0.0036 - val_loss: 2.3687e-04 - val_mae: 0.0036\n",
      "Epoch 246/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3528e-04 - mae: 0.0035 - val_loss: 2.3140e-04 - val_mae: 0.0032\n",
      "Epoch 247/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3634e-04 - mae: 0.0037 - val_loss: 2.3007e-04 - val_mae: 0.0031\n",
      "Epoch 248/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3276e-04 - mae: 0.0035 - val_loss: 2.4829e-04 - val_mae: 0.0052\n",
      "Epoch 249/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3227e-04 - mae: 0.0035 - val_loss: 2.4491e-04 - val_mae: 0.0047\n",
      "Epoch 250/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3210e-04 - mae: 0.0036 - val_loss: 2.3588e-04 - val_mae: 0.0041\n",
      "Epoch 251/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3039e-04 - mae: 0.0035 - val_loss: 2.2685e-04 - val_mae: 0.0032\n",
      "Epoch 252/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2963e-04 - mae: 0.0035 - val_loss: 2.2587e-04 - val_mae: 0.0032\n",
      "Epoch 253/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2972e-04 - mae: 0.0036 - val_loss: 2.3072e-04 - val_mae: 0.0037\n",
      "Epoch 254/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2868e-04 - mae: 0.0036 - val_loss: 2.3154e-04 - val_mae: 0.0039\n",
      "Epoch 255/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2869e-04 - mae: 0.0037 - val_loss: 2.2935e-04 - val_mae: 0.0038\n",
      "Epoch 256/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2728e-04 - mae: 0.0036 - val_loss: 2.2609e-04 - val_mae: 0.0035\n",
      "Epoch 257/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2505e-04 - mae: 0.0035 - val_loss: 2.2471e-04 - val_mae: 0.0035\n",
      "Epoch 258/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2612e-04 - mae: 0.0036 - val_loss: 2.2147e-04 - val_mae: 0.0032\n",
      "Epoch 259/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2520e-04 - mae: 0.0036 - val_loss: 2.2567e-04 - val_mae: 0.0037\n",
      "Epoch 260/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2276e-04 - mae: 0.0035 - val_loss: 2.2138e-04 - val_mae: 0.0033\n",
      "Epoch 261/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2283e-04 - mae: 0.0035 - val_loss: 2.1942e-04 - val_mae: 0.0032\n",
      "Epoch 262/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2182e-04 - mae: 0.0035 - val_loss: 2.2011e-04 - val_mae: 0.0034\n",
      "Epoch 263/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2076e-04 - mae: 0.0035 - val_loss: 2.2521e-04 - val_mae: 0.0039\n",
      "Epoch 264/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1964e-04 - mae: 0.0035 - val_loss: 2.1962e-04 - val_mae: 0.0034\n",
      "Epoch 265/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2094e-04 - mae: 0.0037 - val_loss: 2.2072e-04 - val_mae: 0.0038\n",
      "Epoch 266/1000\n",
      "626/630 [============================>.] - ETA: 0s - loss: 2.2021e-04 - mae: 0.0036Restoring model weights from the end of the best epoch: 261.\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2022e-04 - mae: 0.0036 - val_loss: 2.2425e-04 - val_mae: 0.0041\n",
      "Epoch 266: early stopping\n",
      "Training für Fold 3...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 4s 4ms/step - loss: 0.0256 - mae: 0.0450 - val_loss: 0.0209 - val_mae: 0.0263\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0195 - mae: 0.0216 - val_loss: 0.0184 - val_mae: 0.0170\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0176 - mae: 0.0157 - val_loss: 0.0168 - val_mae: 0.0126\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0162 - mae: 0.0123 - val_loss: 0.0157 - val_mae: 0.0131\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0151 - mae: 0.0110 - val_loss: 0.0146 - val_mae: 0.0103\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0142 - mae: 0.0102 - val_loss: 0.0138 - val_mae: 0.0105\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0134 - mae: 0.0097 - val_loss: 0.0130 - val_mae: 0.0102\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0126 - mae: 0.0091 - val_loss: 0.0122 - val_mae: 0.0086\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0119 - mae: 0.0088 - val_loss: 0.0116 - val_mae: 0.0079\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0113 - mae: 0.0085 - val_loss: 0.0110 - val_mae: 0.0078\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0108 - mae: 0.0085 - val_loss: 0.0106 - val_mae: 0.0086\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0103 - mae: 0.0082 - val_loss: 0.0101 - val_mae: 0.0093\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0099 - mae: 0.0080 - val_loss: 0.0097 - val_mae: 0.0079\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0095 - mae: 0.0079 - val_loss: 0.0093 - val_mae: 0.0067\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0091 - mae: 0.0075 - val_loss: 0.0090 - val_mae: 0.0071\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0088 - mae: 0.0075 - val_loss: 0.0086 - val_mae: 0.0066\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0085 - mae: 0.0077 - val_loss: 0.0084 - val_mae: 0.0082\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0083 - mae: 0.0072 - val_loss: 0.0081 - val_mae: 0.0077\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0080 - mae: 0.0068 - val_loss: 0.0079 - val_mae: 0.0064\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0078 - mae: 0.0072 - val_loss: 0.0076 - val_mae: 0.0061\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0075 - mae: 0.0067 - val_loss: 0.0074 - val_mae: 0.0063\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0073 - mae: 0.0068 - val_loss: 0.0072 - val_mae: 0.0080\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0071 - mae: 0.0067 - val_loss: 0.0070 - val_mae: 0.0061\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0069 - mae: 0.0066 - val_loss: 0.0068 - val_mae: 0.0067\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0067 - mae: 0.0065 - val_loss: 0.0067 - val_mae: 0.0094\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0065 - mae: 0.0067 - val_loss: 0.0064 - val_mae: 0.0057\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0063 - mae: 0.0063 - val_loss: 0.0062 - val_mae: 0.0059\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0061 - mae: 0.0061 - val_loss: 0.0060 - val_mae: 0.0054\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0059 - mae: 0.0061 - val_loss: 0.0059 - val_mae: 0.0075\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0058 - mae: 0.0062 - val_loss: 0.0057 - val_mae: 0.0056\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0056 - mae: 0.0060 - val_loss: 0.0055 - val_mae: 0.0067\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0054 - mae: 0.0059 - val_loss: 0.0054 - val_mae: 0.0050\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0053 - mae: 0.0058 - val_loss: 0.0052 - val_mae: 0.0059\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0051 - mae: 0.0061 - val_loss: 0.0051 - val_mae: 0.0060\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0050 - mae: 0.0057 - val_loss: 0.0049 - val_mae: 0.0059\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0049 - mae: 0.0058 - val_loss: 0.0048 - val_mae: 0.0053\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0047 - mae: 0.0055 - val_loss: 0.0047 - val_mae: 0.0067\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0046 - mae: 0.0057 - val_loss: 0.0045 - val_mae: 0.0055\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0044 - mae: 0.0055 - val_loss: 0.0044 - val_mae: 0.0066\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0043 - mae: 0.0056 - val_loss: 0.0042 - val_mae: 0.0053\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0042 - mae: 0.0055 - val_loss: 0.0042 - val_mae: 0.0068\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0041 - mae: 0.0054 - val_loss: 0.0040 - val_mae: 0.0050\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0039 - mae: 0.0054 - val_loss: 0.0039 - val_mae: 0.0047\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0038 - mae: 0.0054 - val_loss: 0.0038 - val_mae: 0.0056\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0037 - mae: 0.0052 - val_loss: 0.0037 - val_mae: 0.0054\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0036 - mae: 0.0053 - val_loss: 0.0036 - val_mae: 0.0055\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0035 - mae: 0.0053 - val_loss: 0.0035 - val_mae: 0.0055\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0034 - mae: 0.0051 - val_loss: 0.0034 - val_mae: 0.0054\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0033 - mae: 0.0052 - val_loss: 0.0032 - val_mae: 0.0045\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0032 - mae: 0.0050 - val_loss: 0.0032 - val_mae: 0.0051\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0031 - mae: 0.0051 - val_loss: 0.0031 - val_mae: 0.0044\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0030 - mae: 0.0050 - val_loss: 0.0030 - val_mae: 0.0044\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0029 - mae: 0.0051 - val_loss: 0.0029 - val_mae: 0.0048\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0028 - mae: 0.0050 - val_loss: 0.0028 - val_mae: 0.0070\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0028 - mae: 0.0051 - val_loss: 0.0028 - val_mae: 0.0069\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0027 - mae: 0.0049 - val_loss: 0.0026 - val_mae: 0.0044\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0026 - mae: 0.0051 - val_loss: 0.0026 - val_mae: 0.0059\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0025 - mae: 0.0049 - val_loss: 0.0025 - val_mae: 0.0048\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0025 - mae: 0.0049 - val_loss: 0.0024 - val_mae: 0.0061\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0024 - mae: 0.0047 - val_loss: 0.0024 - val_mae: 0.0055\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0023 - mae: 0.0049 - val_loss: 0.0023 - val_mae: 0.0049\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0023 - mae: 0.0047 - val_loss: 0.0022 - val_mae: 0.0048\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0022 - mae: 0.0050 - val_loss: 0.0022 - val_mae: 0.0047\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0021 - mae: 0.0048 - val_loss: 0.0021 - val_mae: 0.0041\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0021 - mae: 0.0047 - val_loss: 0.0021 - val_mae: 0.0052\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0047 - val_loss: 0.0020 - val_mae: 0.0045\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0047\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0019 - mae: 0.0046 - val_loss: 0.0019 - val_mae: 0.0043\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0041\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0045 - val_loss: 0.0018 - val_mae: 0.0046\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0017 - val_mae: 0.0045\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0045 - val_loss: 0.0017 - val_mae: 0.0046\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0048\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0016 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0042\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0016 - mae: 0.0043 - val_loss: 0.0016 - val_mae: 0.0045\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0048\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0040\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0039\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 0.0014 - val_mae: 0.0056\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0041\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0038\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0038\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0040\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0043 - val_loss: 0.0012 - val_mae: 0.0046\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0063\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0038\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0041\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0057\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0036\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0058\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0047\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0010 - mae: 0.0042 - val_loss: 0.0010 - val_mae: 0.0037\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0010 - mae: 0.0042 - val_loss: 0.0010 - val_mae: 0.0061\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.9612e-04 - mae: 0.0042 - val_loss: 9.8320e-04 - val_mae: 0.0041\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.7704e-04 - mae: 0.0043 - val_loss: 9.6713e-04 - val_mae: 0.0044\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.5489e-04 - mae: 0.0042 - val_loss: 9.3774e-04 - val_mae: 0.0035\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.3338e-04 - mae: 0.0040 - val_loss: 9.2107e-04 - val_mae: 0.0039\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.1684e-04 - mae: 0.0043 - val_loss: 9.0846e-04 - val_mae: 0.0044\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.9623e-04 - mae: 0.0041 - val_loss: 8.8070e-04 - val_mae: 0.0035\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.7975e-04 - mae: 0.0042 - val_loss: 8.6535e-04 - val_mae: 0.0037\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.6161e-04 - mae: 0.0041 - val_loss: 8.5892e-04 - val_mae: 0.0046\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.4464e-04 - mae: 0.0041 - val_loss: 8.5544e-04 - val_mae: 0.0053\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.3188e-04 - mae: 0.0043 - val_loss: 8.1979e-04 - val_mae: 0.0039\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.1447e-04 - mae: 0.0041 - val_loss: 8.1218e-04 - val_mae: 0.0046\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.9769e-04 - mae: 0.0039 - val_loss: 7.9620e-04 - val_mae: 0.0046\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.8526e-04 - mae: 0.0041 - val_loss: 7.7171e-04 - val_mae: 0.0035\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.7056e-04 - mae: 0.0041 - val_loss: 7.5861e-04 - val_mae: 0.0037\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.5623e-04 - mae: 0.0040 - val_loss: 7.4299e-04 - val_mae: 0.0035\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.4258e-04 - mae: 0.0040 - val_loss: 7.3006e-04 - val_mae: 0.0036\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.3221e-04 - mae: 0.0042 - val_loss: 7.2339e-04 - val_mae: 0.0040\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.1783e-04 - mae: 0.0040 - val_loss: 7.0815e-04 - val_mae: 0.0036\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.0519e-04 - mae: 0.0040 - val_loss: 6.9715e-04 - val_mae: 0.0038\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.9399e-04 - mae: 0.0040 - val_loss: 6.9482e-04 - val_mae: 0.0044\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.8219e-04 - mae: 0.0040 - val_loss: 6.7903e-04 - val_mae: 0.0040\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.6997e-04 - mae: 0.0039 - val_loss: 6.7525e-04 - val_mae: 0.0049\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.5988e-04 - mae: 0.0040 - val_loss: 6.5089e-04 - val_mae: 0.0038\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.4802e-04 - mae: 0.0039 - val_loss: 6.7751e-04 - val_mae: 0.0065\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.3989e-04 - mae: 0.0041 - val_loss: 6.4607e-04 - val_mae: 0.0049\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.2945e-04 - mae: 0.0040 - val_loss: 6.2353e-04 - val_mae: 0.0039\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.1926e-04 - mae: 0.0040 - val_loss: 6.0847e-04 - val_mae: 0.0034\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.0944e-04 - mae: 0.0039 - val_loss: 6.0240e-04 - val_mae: 0.0036\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.0019e-04 - mae: 0.0039 - val_loss: 5.9443e-04 - val_mae: 0.0037\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.9305e-04 - mae: 0.0040 - val_loss: 5.8121e-04 - val_mae: 0.0034\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.8270e-04 - mae: 0.0039 - val_loss: 5.7436e-04 - val_mae: 0.0035\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.7414e-04 - mae: 0.0039 - val_loss: 5.8286e-04 - val_mae: 0.0048\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.6692e-04 - mae: 0.0039 - val_loss: 5.5794e-04 - val_mae: 0.0035\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.5592e-04 - mae: 0.0037 - val_loss: 5.5075e-04 - val_mae: 0.0036\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.4992e-04 - mae: 0.0039 - val_loss: 5.5092e-04 - val_mae: 0.0044\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.4384e-04 - mae: 0.0040 - val_loss: 5.3320e-04 - val_mae: 0.0034\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.3436e-04 - mae: 0.0038 - val_loss: 5.2757e-04 - val_mae: 0.0035\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.2667e-04 - mae: 0.0038 - val_loss: 5.3334e-04 - val_mae: 0.0048\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.2200e-04 - mae: 0.0040 - val_loss: 5.2487e-04 - val_mae: 0.0047\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.1328e-04 - mae: 0.0038 - val_loss: 5.0405e-04 - val_mae: 0.0033\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.0637e-04 - mae: 0.0038 - val_loss: 5.0142e-04 - val_mae: 0.0037\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.9955e-04 - mae: 0.0038 - val_loss: 4.9107e-04 - val_mae: 0.0033\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.9409e-04 - mae: 0.0038 - val_loss: 4.8503e-04 - val_mae: 0.0033\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.8729e-04 - mae: 0.0038 - val_loss: 4.9737e-04 - val_mae: 0.0047\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.8192e-04 - mae: 0.0039 - val_loss: 4.7244e-04 - val_mae: 0.0033\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.7481e-04 - mae: 0.0038 - val_loss: 4.6690e-04 - val_mae: 0.0033\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.6898e-04 - mae: 0.0038 - val_loss: 4.6384e-04 - val_mae: 0.0034\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.6443e-04 - mae: 0.0039 - val_loss: 4.5605e-04 - val_mae: 0.0033\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.5667e-04 - mae: 0.0037 - val_loss: 4.5628e-04 - val_mae: 0.0039\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.5337e-04 - mae: 0.0039 - val_loss: 4.5019e-04 - val_mae: 0.0039\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.4654e-04 - mae: 0.0037 - val_loss: 4.4042e-04 - val_mae: 0.0035\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.4173e-04 - mae: 0.0038 - val_loss: 4.3852e-04 - val_mae: 0.0036\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.3600e-04 - mae: 0.0037 - val_loss: 4.3518e-04 - val_mae: 0.0040\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.3357e-04 - mae: 0.0039 - val_loss: 4.3198e-04 - val_mae: 0.0039\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.2618e-04 - mae: 0.0037 - val_loss: 4.2678e-04 - val_mae: 0.0039\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.2105e-04 - mae: 0.0036 - val_loss: 4.1784e-04 - val_mae: 0.0036\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.1649e-04 - mae: 0.0036 - val_loss: 4.1504e-04 - val_mae: 0.0038\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.1425e-04 - mae: 0.0039 - val_loss: 4.0434e-04 - val_mae: 0.0032\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0884e-04 - mae: 0.0038 - val_loss: 4.1293e-04 - val_mae: 0.0046\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0426e-04 - mae: 0.0037 - val_loss: 4.0035e-04 - val_mae: 0.0036\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0082e-04 - mae: 0.0038 - val_loss: 3.9360e-04 - val_mae: 0.0033\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.9460e-04 - mae: 0.0036 - val_loss: 4.1205e-04 - val_mae: 0.0049\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.9263e-04 - mae: 0.0038 - val_loss: 3.8504e-04 - val_mae: 0.0032\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8760e-04 - mae: 0.0037 - val_loss: 3.8626e-04 - val_mae: 0.0037\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8442e-04 - mae: 0.0037 - val_loss: 3.7748e-04 - val_mae: 0.0033\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8074e-04 - mae: 0.0037 - val_loss: 3.8113e-04 - val_mae: 0.0038\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7656e-04 - mae: 0.0037 - val_loss: 3.7157e-04 - val_mae: 0.0034\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7355e-04 - mae: 0.0037 - val_loss: 3.6530e-04 - val_mae: 0.0031\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6901e-04 - mae: 0.0036 - val_loss: 3.6273e-04 - val_mae: 0.0032\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6594e-04 - mae: 0.0037 - val_loss: 3.6432e-04 - val_mae: 0.0037\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6405e-04 - mae: 0.0038 - val_loss: 3.5651e-04 - val_mae: 0.0032\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5975e-04 - mae: 0.0037 - val_loss: 3.6507e-04 - val_mae: 0.0044\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5614e-04 - mae: 0.0036 - val_loss: 3.6761e-04 - val_mae: 0.0048\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5321e-04 - mae: 0.0037 - val_loss: 3.6422e-04 - val_mae: 0.0049\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5172e-04 - mae: 0.0038 - val_loss: 3.4295e-04 - val_mae: 0.0031\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4673e-04 - mae: 0.0036 - val_loss: 3.4113e-04 - val_mae: 0.0032\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4430e-04 - mae: 0.0037 - val_loss: 3.6460e-04 - val_mae: 0.0054\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4010e-04 - mae: 0.0035 - val_loss: 3.3367e-04 - val_mae: 0.0030\n",
      "Epoch 172/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3778e-04 - mae: 0.0036 - val_loss: 3.3644e-04 - val_mae: 0.0037\n",
      "Epoch 173/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3649e-04 - mae: 0.0037 - val_loss: 3.3926e-04 - val_mae: 0.0043\n",
      "Epoch 174/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3243e-04 - mae: 0.0036 - val_loss: 3.4001e-04 - val_mae: 0.0045\n",
      "Epoch 175/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2914e-04 - mae: 0.0035 - val_loss: 3.2315e-04 - val_mae: 0.0031\n",
      "Epoch 176/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2698e-04 - mae: 0.0036 - val_loss: 3.3054e-04 - val_mae: 0.0042\n",
      "Epoch 177/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2614e-04 - mae: 0.0037 - val_loss: 3.2847e-04 - val_mae: 0.0041\n",
      "Epoch 178/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2231e-04 - mae: 0.0036 - val_loss: 3.3063e-04 - val_mae: 0.0045\n",
      "Epoch 179/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1917e-04 - mae: 0.0036 - val_loss: 3.1997e-04 - val_mae: 0.0038\n",
      "Epoch 180/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1725e-04 - mae: 0.0036 - val_loss: 3.1603e-04 - val_mae: 0.0037\n",
      "Epoch 181/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1577e-04 - mae: 0.0037 - val_loss: 3.0885e-04 - val_mae: 0.0032\n",
      "Epoch 182/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1433e-04 - mae: 0.0037 - val_loss: 3.0710e-04 - val_mae: 0.0032\n",
      "Epoch 183/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0980e-04 - mae: 0.0036 - val_loss: 3.0436e-04 - val_mae: 0.0031\n",
      "Epoch 184/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0829e-04 - mae: 0.0036 - val_loss: 3.0830e-04 - val_mae: 0.0037\n",
      "Epoch 185/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0665e-04 - mae: 0.0037 - val_loss: 3.0856e-04 - val_mae: 0.0038\n",
      "Epoch 186/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0290e-04 - mae: 0.0035 - val_loss: 3.0176e-04 - val_mae: 0.0037\n",
      "Epoch 187/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0127e-04 - mae: 0.0036 - val_loss: 3.1068e-04 - val_mae: 0.0044\n",
      "Epoch 188/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9951e-04 - mae: 0.0036 - val_loss: 2.9632e-04 - val_mae: 0.0034\n",
      "Epoch 189/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9819e-04 - mae: 0.0037 - val_loss: 2.9761e-04 - val_mae: 0.0037\n",
      "Epoch 190/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9603e-04 - mae: 0.0036 - val_loss: 2.9027e-04 - val_mae: 0.0032\n",
      "Epoch 191/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9374e-04 - mae: 0.0036 - val_loss: 2.9080e-04 - val_mae: 0.0035\n",
      "Epoch 192/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9296e-04 - mae: 0.0037 - val_loss: 2.8915e-04 - val_mae: 0.0035\n",
      "Epoch 193/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8978e-04 - mae: 0.0036 - val_loss: 2.9028e-04 - val_mae: 0.0038\n",
      "Epoch 194/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8641e-04 - mae: 0.0035 - val_loss: 2.9523e-04 - val_mae: 0.0045\n",
      "Epoch 195/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8544e-04 - mae: 0.0035 - val_loss: 2.8220e-04 - val_mae: 0.0033\n",
      "Epoch 196/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8418e-04 - mae: 0.0036 - val_loss: 2.8003e-04 - val_mae: 0.0034\n",
      "Epoch 197/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8269e-04 - mae: 0.0036 - val_loss: 2.8280e-04 - val_mae: 0.0038\n",
      "Epoch 198/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8062e-04 - mae: 0.0036 - val_loss: 2.8064e-04 - val_mae: 0.0038\n",
      "Epoch 199/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7890e-04 - mae: 0.0036 - val_loss: 2.7326e-04 - val_mae: 0.0032\n",
      "Epoch 200/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7776e-04 - mae: 0.0036 - val_loss: 2.7502e-04 - val_mae: 0.0035\n",
      "Epoch 201/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7456e-04 - mae: 0.0035 - val_loss: 2.7883e-04 - val_mae: 0.0040\n",
      "Epoch 202/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7289e-04 - mae: 0.0035 - val_loss: 2.7057e-04 - val_mae: 0.0035\n",
      "Epoch 203/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7091e-04 - mae: 0.0035 - val_loss: 2.6708e-04 - val_mae: 0.0032\n",
      "Epoch 204/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7106e-04 - mae: 0.0036 - val_loss: 2.6781e-04 - val_mae: 0.0034\n",
      "Epoch 205/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6772e-04 - mae: 0.0035 - val_loss: 2.6611e-04 - val_mae: 0.0034\n",
      "Epoch 206/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6705e-04 - mae: 0.0036 - val_loss: 2.7255e-04 - val_mae: 0.0041\n",
      "Epoch 207/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6541e-04 - mae: 0.0036 - val_loss: 2.6120e-04 - val_mae: 0.0032\n",
      "Epoch 208/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6359e-04 - mae: 0.0035 - val_loss: 2.5884e-04 - val_mae: 0.0031\n",
      "Epoch 209/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6150e-04 - mae: 0.0035 - val_loss: 2.6689e-04 - val_mae: 0.0041\n",
      "Epoch 210/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5955e-04 - mae: 0.0034 - val_loss: 2.5471e-04 - val_mae: 0.0030\n",
      "Epoch 211/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5791e-04 - mae: 0.0034 - val_loss: 2.5193e-04 - val_mae: 0.0029\n",
      "Epoch 212/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5793e-04 - mae: 0.0035 - val_loss: 2.6174e-04 - val_mae: 0.0041\n",
      "Epoch 213/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5724e-04 - mae: 0.0036 - val_loss: 2.5488e-04 - val_mae: 0.0035\n",
      "Epoch 214/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5427e-04 - mae: 0.0035 - val_loss: 2.4999e-04 - val_mae: 0.0031\n",
      "Epoch 215/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5280e-04 - mae: 0.0035 - val_loss: 2.4934e-04 - val_mae: 0.0032\n",
      "Epoch 216/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5133e-04 - mae: 0.0034 - val_loss: 2.4962e-04 - val_mae: 0.0033\n",
      "Epoch 217/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5137e-04 - mae: 0.0036 - val_loss: 2.7731e-04 - val_mae: 0.0059\n",
      "Epoch 218/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4855e-04 - mae: 0.0034 - val_loss: 2.4460e-04 - val_mae: 0.0030\n",
      "Epoch 219/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4874e-04 - mae: 0.0036 - val_loss: 2.4472e-04 - val_mae: 0.0032\n",
      "Epoch 220/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4791e-04 - mae: 0.0036 - val_loss: 2.4056e-04 - val_mae: 0.0029\n",
      "Epoch 221/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4393e-04 - mae: 0.0033 - val_loss: 2.5120e-04 - val_mae: 0.0042\n",
      "Epoch 222/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4449e-04 - mae: 0.0035 - val_loss: 2.5989e-04 - val_mae: 0.0052\n",
      "Epoch 223/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4283e-04 - mae: 0.0034 - val_loss: 2.5024e-04 - val_mae: 0.0040\n",
      "Epoch 224/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4237e-04 - mae: 0.0035 - val_loss: 2.4282e-04 - val_mae: 0.0037\n",
      "Epoch 225/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4055e-04 - mae: 0.0034 - val_loss: 2.3770e-04 - val_mae: 0.0032\n",
      "Epoch 226/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3986e-04 - mae: 0.0035 - val_loss: 2.3608e-04 - val_mae: 0.0032\n",
      "Epoch 227/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3859e-04 - mae: 0.0035 - val_loss: 2.3370e-04 - val_mae: 0.0031\n",
      "Epoch 228/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3745e-04 - mae: 0.0035 - val_loss: 2.3236e-04 - val_mae: 0.0030\n",
      "Epoch 229/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3578e-04 - mae: 0.0034 - val_loss: 2.3426e-04 - val_mae: 0.0032\n",
      "Epoch 230/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3434e-04 - mae: 0.0034 - val_loss: 2.4192e-04 - val_mae: 0.0041\n",
      "Epoch 231/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3454e-04 - mae: 0.0035 - val_loss: 2.3641e-04 - val_mae: 0.0036\n",
      "Epoch 232/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3286e-04 - mae: 0.0034 - val_loss: 2.3287e-04 - val_mae: 0.0035\n",
      "Epoch 233/1000\n",
      "616/630 [============================>.] - ETA: 0s - loss: 2.3223e-04 - mae: 0.0035Restoring model weights from the end of the best epoch: 228.\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3229e-04 - mae: 0.0035 - val_loss: 2.3688e-04 - val_mae: 0.0040\n",
      "Epoch 233: early stopping\n",
      "Training für Fold 4...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 4s 4ms/step - loss: 0.0260 - mae: 0.0497 - val_loss: 0.0206 - val_mae: 0.0290\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0193 - mae: 0.0230 - val_loss: 0.0182 - val_mae: 0.0181\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0175 - mae: 0.0165 - val_loss: 0.0169 - val_mae: 0.0138\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0164 - mae: 0.0137 - val_loss: 0.0159 - val_mae: 0.0123\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0155 - mae: 0.0119 - val_loss: 0.0151 - val_mae: 0.0116\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0147 - mae: 0.0111 - val_loss: 0.0144 - val_mae: 0.0125\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0140 - mae: 0.0107 - val_loss: 0.0137 - val_mae: 0.0106\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0133 - mae: 0.0101 - val_loss: 0.0130 - val_mae: 0.0101\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0127 - mae: 0.0094 - val_loss: 0.0125 - val_mae: 0.0094\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0122 - mae: 0.0093 - val_loss: 0.0120 - val_mae: 0.0124\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0117 - mae: 0.0091 - val_loss: 0.0115 - val_mae: 0.0081\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0113 - mae: 0.0086 - val_loss: 0.0111 - val_mae: 0.0105\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0109 - mae: 0.0086 - val_loss: 0.0107 - val_mae: 0.0083\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0105 - mae: 0.0084 - val_loss: 0.0104 - val_mae: 0.0099\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0101 - mae: 0.0079 - val_loss: 0.0099 - val_mae: 0.0072\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0098 - mae: 0.0082 - val_loss: 0.0096 - val_mae: 0.0071\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0095 - mae: 0.0076 - val_loss: 0.0093 - val_mae: 0.0066\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0092 - mae: 0.0078 - val_loss: 0.0090 - val_mae: 0.0066\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0089 - mae: 0.0074 - val_loss: 0.0088 - val_mae: 0.0067\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0087 - mae: 0.0078 - val_loss: 0.0086 - val_mae: 0.0089\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0084 - mae: 0.0073 - val_loss: 0.0083 - val_mae: 0.0079\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0082 - mae: 0.0072 - val_loss: 0.0080 - val_mae: 0.0060\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0079 - mae: 0.0072 - val_loss: 0.0078 - val_mae: 0.0067\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0077 - mae: 0.0069 - val_loss: 0.0076 - val_mae: 0.0082\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0075 - mae: 0.0068 - val_loss: 0.0074 - val_mae: 0.0086\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0073 - mae: 0.0068 - val_loss: 0.0072 - val_mae: 0.0061\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0071 - mae: 0.0068 - val_loss: 0.0070 - val_mae: 0.0060\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0069 - mae: 0.0067 - val_loss: 0.0068 - val_mae: 0.0085\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0067 - mae: 0.0068 - val_loss: 0.0066 - val_mae: 0.0080\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0065 - mae: 0.0064 - val_loss: 0.0064 - val_mae: 0.0065\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0063 - mae: 0.0066 - val_loss: 0.0062 - val_mae: 0.0057\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0061 - mae: 0.0063 - val_loss: 0.0060 - val_mae: 0.0057\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0060 - mae: 0.0061 - val_loss: 0.0061 - val_mae: 0.0141\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0058 - mae: 0.0061 - val_loss: 0.0057 - val_mae: 0.0057\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0056 - mae: 0.0062 - val_loss: 0.0055 - val_mae: 0.0053\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0055 - mae: 0.0061 - val_loss: 0.0054 - val_mae: 0.0072\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0053 - mae: 0.0061 - val_loss: 0.0053 - val_mae: 0.0070\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0052 - mae: 0.0060 - val_loss: 0.0051 - val_mae: 0.0056\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0051 - mae: 0.0062 - val_loss: 0.0050 - val_mae: 0.0074\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0049 - mae: 0.0057 - val_loss: 0.0048 - val_mae: 0.0052\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0048 - mae: 0.0058 - val_loss: 0.0047 - val_mae: 0.0052\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0046 - mae: 0.0059 - val_loss: 0.0046 - val_mae: 0.0054\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0045 - mae: 0.0059 - val_loss: 0.0045 - val_mae: 0.0057\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0044 - mae: 0.0058 - val_loss: 0.0043 - val_mae: 0.0050\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0043 - mae: 0.0057 - val_loss: 0.0042 - val_mae: 0.0058\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0042 - mae: 0.0058 - val_loss: 0.0041 - val_mae: 0.0054\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0041 - mae: 0.0057 - val_loss: 0.0040 - val_mae: 0.0068\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0040 - mae: 0.0057 - val_loss: 0.0039 - val_mae: 0.0047\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0039 - mae: 0.0057 - val_loss: 0.0038 - val_mae: 0.0056\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0037 - mae: 0.0053 - val_loss: 0.0037 - val_mae: 0.0070\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0036 - mae: 0.0057 - val_loss: 0.0036 - val_mae: 0.0061\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0035 - mae: 0.0054 - val_loss: 0.0035 - val_mae: 0.0052\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0034 - mae: 0.0054 - val_loss: 0.0034 - val_mae: 0.0045\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0034 - mae: 0.0053 - val_loss: 0.0033 - val_mae: 0.0048\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0033 - mae: 0.0053 - val_loss: 0.0032 - val_mae: 0.0062\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0032 - mae: 0.0054 - val_loss: 0.0031 - val_mae: 0.0050\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0031 - mae: 0.0053 - val_loss: 0.0030 - val_mae: 0.0050\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0030 - mae: 0.0053 - val_loss: 0.0030 - val_mae: 0.0053\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0029 - mae: 0.0052 - val_loss: 0.0029 - val_mae: 0.0050\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0028 - mae: 0.0051 - val_loss: 0.0028 - val_mae: 0.0052\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0028 - mae: 0.0051 - val_loss: 0.0027 - val_mae: 0.0045\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0027 - mae: 0.0051 - val_loss: 0.0026 - val_mae: 0.0047\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0026 - mae: 0.0050 - val_loss: 0.0026 - val_mae: 0.0048\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0025 - mae: 0.0051 - val_loss: 0.0025 - val_mae: 0.0047\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0025 - mae: 0.0050 - val_loss: 0.0025 - val_mae: 0.0075\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0024 - mae: 0.0052 - val_loss: 0.0024 - val_mae: 0.0074\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0023 - mae: 0.0051 - val_loss: 0.0023 - val_mae: 0.0054\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0023 - mae: 0.0049 - val_loss: 0.0024 - val_mae: 0.0107\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0022 - mae: 0.0049 - val_loss: 0.0022 - val_mae: 0.0047\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0022 - mae: 0.0050 - val_loss: 0.0021 - val_mae: 0.0044\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0021 - mae: 0.0049 - val_loss: 0.0021 - val_mae: 0.0076\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0049 - val_loss: 0.0020 - val_mae: 0.0050\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0048 - val_loss: 0.0020 - val_mae: 0.0055\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0019 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0041\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0019 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0048\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0049 - val_loss: 0.0019 - val_mae: 0.0069\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0048 - val_loss: 0.0018 - val_mae: 0.0041\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0048 - val_loss: 0.0017 - val_mae: 0.0044\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 3s 4ms/step - loss: 0.0017 - mae: 0.0048 - val_loss: 0.0017 - val_mae: 0.0041\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0049 - val_loss: 0.0017 - val_mae: 0.0056\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0047\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0016 - mae: 0.0048 - val_loss: 0.0016 - val_mae: 0.0046\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0040\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0042\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0015 - mae: 0.0047 - val_loss: 0.0015 - val_mae: 0.0043\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0014 - val_mae: 0.0043\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0042\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0014 - mae: 0.0046 - val_loss: 0.0013 - val_mae: 0.0041\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0013 - mae: 0.0047 - val_loss: 0.0013 - val_mae: 0.0051\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0040\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0044\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0053\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0012 - mae: 0.0046 - val_loss: 0.0012 - val_mae: 0.0053\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0048\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0012 - mae: 0.0045 - val_loss: 0.0012 - val_mae: 0.0044\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0045 - val_loss: 0.0011 - val_mae: 0.0045\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0041\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0036\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0043\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0010 - mae: 0.0044 - val_loss: 0.0010 - val_mae: 0.0037\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0010 - mae: 0.0044 - val_loss: 0.0010 - val_mae: 0.0039\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0010 - mae: 0.0042 - val_loss: 9.8696e-04 - val_mae: 0.0040\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.8277e-04 - mae: 0.0044 - val_loss: 9.6686e-04 - val_mae: 0.0040\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.6307e-04 - mae: 0.0044 - val_loss: 9.5415e-04 - val_mae: 0.0044\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.4342e-04 - mae: 0.0043 - val_loss: 9.4471e-04 - val_mae: 0.0052\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.2403e-04 - mae: 0.0042 - val_loss: 9.2301e-04 - val_mae: 0.0051\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.0655e-04 - mae: 0.0043 - val_loss: 9.1483e-04 - val_mae: 0.0055\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.8857e-04 - mae: 0.0043 - val_loss: 8.7628e-04 - val_mae: 0.0041\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.7509e-04 - mae: 0.0045 - val_loss: 8.5647e-04 - val_mae: 0.0037\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.5598e-04 - mae: 0.0042 - val_loss: 8.5153e-04 - val_mae: 0.0047\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.4242e-04 - mae: 0.0044 - val_loss: 8.2821e-04 - val_mae: 0.0039\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.2550e-04 - mae: 0.0042 - val_loss: 8.1069e-04 - val_mae: 0.0036\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.1200e-04 - mae: 0.0043 - val_loss: 7.9914e-04 - val_mae: 0.0039\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.9674e-04 - mae: 0.0042 - val_loss: 7.8483e-04 - val_mae: 0.0039\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.8390e-04 - mae: 0.0043 - val_loss: 7.7045e-04 - val_mae: 0.0038\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.7001e-04 - mae: 0.0042 - val_loss: 7.5542e-04 - val_mae: 0.0036\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.5714e-04 - mae: 0.0042 - val_loss: 7.5284e-04 - val_mae: 0.0045\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.4452e-04 - mae: 0.0042 - val_loss: 7.3184e-04 - val_mae: 0.0037\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.3121e-04 - mae: 0.0041 - val_loss: 7.1992e-04 - val_mae: 0.0038\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.1917e-04 - mae: 0.0041 - val_loss: 7.0901e-04 - val_mae: 0.0038\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 7.0717e-04 - mae: 0.0041 - val_loss: 7.0872e-04 - val_mae: 0.0047\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.9573e-04 - mae: 0.0041 - val_loss: 6.8543e-04 - val_mae: 0.0038\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.8450e-04 - mae: 0.0041 - val_loss: 6.7525e-04 - val_mae: 0.0039\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.7388e-04 - mae: 0.0041 - val_loss: 6.7758e-04 - val_mae: 0.0050\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.6331e-04 - mae: 0.0041 - val_loss: 6.5668e-04 - val_mae: 0.0041\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.5152e-04 - mae: 0.0040 - val_loss: 6.4596e-04 - val_mae: 0.0040\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.4235e-04 - mae: 0.0041 - val_loss: 6.5389e-04 - val_mae: 0.0054\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.3439e-04 - mae: 0.0042 - val_loss: 6.4254e-04 - val_mae: 0.0055\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.2344e-04 - mae: 0.0041 - val_loss: 6.1576e-04 - val_mae: 0.0039\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.1603e-04 - mae: 0.0042 - val_loss: 6.0330e-04 - val_mae: 0.0035\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.0612e-04 - mae: 0.0041 - val_loss: 5.9622e-04 - val_mae: 0.0037\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.9760e-04 - mae: 0.0041 - val_loss: 5.8526e-04 - val_mae: 0.0034\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.8838e-04 - mae: 0.0040 - val_loss: 5.7805e-04 - val_mae: 0.0035\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.8042e-04 - mae: 0.0040 - val_loss: 5.8777e-04 - val_mae: 0.0050\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.7376e-04 - mae: 0.0041 - val_loss: 5.6255e-04 - val_mae: 0.0036\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.6548e-04 - mae: 0.0040 - val_loss: 5.5833e-04 - val_mae: 0.0039\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.6015e-04 - mae: 0.0042 - val_loss: 5.6320e-04 - val_mae: 0.0046\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.4965e-04 - mae: 0.0039 - val_loss: 5.4565e-04 - val_mae: 0.0040\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.4323e-04 - mae: 0.0040 - val_loss: 5.4064e-04 - val_mae: 0.0040\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.3714e-04 - mae: 0.0040 - val_loss: 5.6432e-04 - val_mae: 0.0064\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.3025e-04 - mae: 0.0040 - val_loss: 5.4897e-04 - val_mae: 0.0060\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.2172e-04 - mae: 0.0039 - val_loss: 5.1311e-04 - val_mae: 0.0034\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.1617e-04 - mae: 0.0040 - val_loss: 5.0737e-04 - val_mae: 0.0035\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.1237e-04 - mae: 0.0041 - val_loss: 5.1129e-04 - val_mae: 0.0044\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.0354e-04 - mae: 0.0039 - val_loss: 5.3336e-04 - val_mae: 0.0063\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.9814e-04 - mae: 0.0039 - val_loss: 4.8880e-04 - val_mae: 0.0035\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.9166e-04 - mae: 0.0039 - val_loss: 4.8413e-04 - val_mae: 0.0035\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.8606e-04 - mae: 0.0039 - val_loss: 5.0187e-04 - val_mae: 0.0055\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.8032e-04 - mae: 0.0039 - val_loss: 4.7504e-04 - val_mae: 0.0038\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.7701e-04 - mae: 0.0041 - val_loss: 4.8397e-04 - val_mae: 0.0052\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.7033e-04 - mae: 0.0039 - val_loss: 4.8105e-04 - val_mae: 0.0054\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.6546e-04 - mae: 0.0039 - val_loss: 4.6760e-04 - val_mae: 0.0044\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.6103e-04 - mae: 0.0040 - val_loss: 4.5311e-04 - val_mae: 0.0036\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.5431e-04 - mae: 0.0038 - val_loss: 4.4588e-04 - val_mae: 0.0034\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.4956e-04 - mae: 0.0038 - val_loss: 4.4109e-04 - val_mae: 0.0033\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.4468e-04 - mae: 0.0038 - val_loss: 4.5016e-04 - val_mae: 0.0046\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.4172e-04 - mae: 0.0040 - val_loss: 4.3203e-04 - val_mae: 0.0034\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.3557e-04 - mae: 0.0038 - val_loss: 4.3129e-04 - val_mae: 0.0038\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.3291e-04 - mae: 0.0040 - val_loss: 4.2596e-04 - val_mae: 0.0036\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.2740e-04 - mae: 0.0039 - val_loss: 4.2243e-04 - val_mae: 0.0037\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.2324e-04 - mae: 0.0039 - val_loss: 4.1583e-04 - val_mae: 0.0035\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.2022e-04 - mae: 0.0040 - val_loss: 4.1325e-04 - val_mae: 0.0037\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.1281e-04 - mae: 0.0037 - val_loss: 4.7101e-04 - val_mae: 0.0081\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.1074e-04 - mae: 0.0039 - val_loss: 4.0579e-04 - val_mae: 0.0036\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.0627e-04 - mae: 0.0038 - val_loss: 3.9940e-04 - val_mae: 0.0035\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.0395e-04 - mae: 0.0039 - val_loss: 4.1135e-04 - val_mae: 0.0047\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.9875e-04 - mae: 0.0038 - val_loss: 3.9861e-04 - val_mae: 0.0042\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.9491e-04 - mae: 0.0038 - val_loss: 3.9086e-04 - val_mae: 0.0037\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.9307e-04 - mae: 0.0039 - val_loss: 3.8898e-04 - val_mae: 0.0038\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.8794e-04 - mae: 0.0038 - val_loss: 3.8125e-04 - val_mae: 0.0035\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.8433e-04 - mae: 0.0038 - val_loss: 4.0730e-04 - val_mae: 0.0058\n",
      "Epoch 172/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.8211e-04 - mae: 0.0039 - val_loss: 3.8546e-04 - val_mae: 0.0042\n",
      "Epoch 173/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.7896e-04 - mae: 0.0039 - val_loss: 3.7793e-04 - val_mae: 0.0041\n",
      "Epoch 174/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.7528e-04 - mae: 0.0038 - val_loss: 3.6633e-04 - val_mae: 0.0033\n",
      "Epoch 175/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.7149e-04 - mae: 0.0038 - val_loss: 3.7204e-04 - val_mae: 0.0042\n",
      "Epoch 176/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.6844e-04 - mae: 0.0038 - val_loss: 3.6367e-04 - val_mae: 0.0036\n",
      "Epoch 177/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.6579e-04 - mae: 0.0038 - val_loss: 3.5837e-04 - val_mae: 0.0033\n",
      "Epoch 178/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.6292e-04 - mae: 0.0038 - val_loss: 3.5755e-04 - val_mae: 0.0035\n",
      "Epoch 179/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.6081e-04 - mae: 0.0039 - val_loss: 3.5268e-04 - val_mae: 0.0034\n",
      "Epoch 180/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.5593e-04 - mae: 0.0037 - val_loss: 3.5916e-04 - val_mae: 0.0042\n",
      "Epoch 181/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.5518e-04 - mae: 0.0039 - val_loss: 3.5058e-04 - val_mae: 0.0037\n",
      "Epoch 182/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.5005e-04 - mae: 0.0037 - val_loss: 3.4444e-04 - val_mae: 0.0033\n",
      "Epoch 183/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.4764e-04 - mae: 0.0037 - val_loss: 3.4893e-04 - val_mae: 0.0038\n",
      "Epoch 184/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.4477e-04 - mae: 0.0037 - val_loss: 3.4588e-04 - val_mae: 0.0040\n",
      "Epoch 185/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.4249e-04 - mae: 0.0037 - val_loss: 3.3826e-04 - val_mae: 0.0034\n",
      "Epoch 186/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.4133e-04 - mae: 0.0039 - val_loss: 3.4817e-04 - val_mae: 0.0048\n",
      "Epoch 187/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.3740e-04 - mae: 0.0037 - val_loss: 3.3268e-04 - val_mae: 0.0035\n",
      "Epoch 188/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.3535e-04 - mae: 0.0038 - val_loss: 3.3333e-04 - val_mae: 0.0036\n",
      "Epoch 189/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.3315e-04 - mae: 0.0038 - val_loss: 3.4103e-04 - val_mae: 0.0047\n",
      "Epoch 190/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.3160e-04 - mae: 0.0039 - val_loss: 3.2390e-04 - val_mae: 0.0034\n",
      "Epoch 191/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.2849e-04 - mae: 0.0038 - val_loss: 3.4472e-04 - val_mae: 0.0055\n",
      "Epoch 192/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.2547e-04 - mae: 0.0037 - val_loss: 3.1732e-04 - val_mae: 0.0031\n",
      "Epoch 193/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.2399e-04 - mae: 0.0038 - val_loss: 3.2328e-04 - val_mae: 0.0041\n",
      "Epoch 194/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.2211e-04 - mae: 0.0038 - val_loss: 3.4757e-04 - val_mae: 0.0061\n",
      "Epoch 195/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.1968e-04 - mae: 0.0038 - val_loss: 3.2412e-04 - val_mae: 0.0045\n",
      "Epoch 196/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.1682e-04 - mae: 0.0037 - val_loss: 3.1484e-04 - val_mae: 0.0036\n",
      "Epoch 197/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.1537e-04 - mae: 0.0038 - val_loss: 3.1016e-04 - val_mae: 0.0035\n",
      "Epoch 198/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.1250e-04 - mae: 0.0037 - val_loss: 3.1412e-04 - val_mae: 0.0041\n",
      "Epoch 199/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.1275e-04 - mae: 0.0039 - val_loss: 3.1148e-04 - val_mae: 0.0040\n",
      "Epoch 200/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.0876e-04 - mae: 0.0037 - val_loss: 3.0363e-04 - val_mae: 0.0034\n",
      "Epoch 201/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.0705e-04 - mae: 0.0037 - val_loss: 3.2836e-04 - val_mae: 0.0059\n",
      "Epoch 202/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.0506e-04 - mae: 0.0037 - val_loss: 2.9803e-04 - val_mae: 0.0032\n",
      "Epoch 203/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.0382e-04 - mae: 0.0038 - val_loss: 3.0749e-04 - val_mae: 0.0042\n",
      "Epoch 204/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.0218e-04 - mae: 0.0038 - val_loss: 2.9849e-04 - val_mae: 0.0037\n",
      "Epoch 205/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.0014e-04 - mae: 0.0038 - val_loss: 3.1319e-04 - val_mae: 0.0051\n",
      "Epoch 206/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.9836e-04 - mae: 0.0038 - val_loss: 2.9039e-04 - val_mae: 0.0032\n",
      "Epoch 207/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.9545e-04 - mae: 0.0037 - val_loss: 2.9386e-04 - val_mae: 0.0036\n",
      "Epoch 208/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.9417e-04 - mae: 0.0037 - val_loss: 2.8932e-04 - val_mae: 0.0035\n",
      "Epoch 209/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.9331e-04 - mae: 0.0038 - val_loss: 3.0308e-04 - val_mae: 0.0049\n",
      "Epoch 210/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.9038e-04 - mae: 0.0037 - val_loss: 2.8577e-04 - val_mae: 0.0036\n",
      "Epoch 211/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8838e-04 - mae: 0.0037 - val_loss: 3.0308e-04 - val_mae: 0.0050\n",
      "Epoch 212/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8856e-04 - mae: 0.0038 - val_loss: 2.8685e-04 - val_mae: 0.0039\n",
      "Epoch 213/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8693e-04 - mae: 0.0038 - val_loss: 2.8650e-04 - val_mae: 0.0040\n",
      "Epoch 214/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8417e-04 - mae: 0.0037 - val_loss: 2.8302e-04 - val_mae: 0.0037\n",
      "Epoch 215/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8296e-04 - mae: 0.0037 - val_loss: 2.7568e-04 - val_mae: 0.0032\n",
      "Epoch 216/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8073e-04 - mae: 0.0037 - val_loss: 2.7370e-04 - val_mae: 0.0032\n",
      "Epoch 217/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8036e-04 - mae: 0.0038 - val_loss: 2.7313e-04 - val_mae: 0.0033\n",
      "Epoch 218/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.7886e-04 - mae: 0.0038 - val_loss: 2.7142e-04 - val_mae: 0.0033\n",
      "Epoch 219/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.7589e-04 - mae: 0.0037 - val_loss: 2.7629e-04 - val_mae: 0.0038\n",
      "Epoch 220/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.7729e-04 - mae: 0.0039 - val_loss: 2.6901e-04 - val_mae: 0.0033\n",
      "Epoch 221/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.7300e-04 - mae: 0.0036 - val_loss: 2.8504e-04 - val_mae: 0.0050\n",
      "Epoch 222/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.7219e-04 - mae: 0.0037 - val_loss: 2.6772e-04 - val_mae: 0.0034\n",
      "Epoch 223/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.7073e-04 - mae: 0.0037 - val_loss: 2.7091e-04 - val_mae: 0.0039\n",
      "Epoch 224/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.7132e-04 - mae: 0.0039 - val_loss: 2.6568e-04 - val_mae: 0.0035\n",
      "Epoch 225/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6869e-04 - mae: 0.0038 - val_loss: 2.6036e-04 - val_mae: 0.0031\n",
      "Epoch 226/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6693e-04 - mae: 0.0037 - val_loss: 2.6150e-04 - val_mae: 0.0034\n",
      "Epoch 227/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6608e-04 - mae: 0.0037 - val_loss: 2.7043e-04 - val_mae: 0.0042\n",
      "Epoch 228/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6546e-04 - mae: 0.0038 - val_loss: 2.5805e-04 - val_mae: 0.0032\n",
      "Epoch 229/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6178e-04 - mae: 0.0036 - val_loss: 2.5601e-04 - val_mae: 0.0031\n",
      "Epoch 230/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6195e-04 - mae: 0.0037 - val_loss: 2.6107e-04 - val_mae: 0.0039\n",
      "Epoch 231/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6120e-04 - mae: 0.0038 - val_loss: 2.5345e-04 - val_mae: 0.0032\n",
      "Epoch 232/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.5846e-04 - mae: 0.0036 - val_loss: 2.7106e-04 - val_mae: 0.0051\n",
      "Epoch 233/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.5919e-04 - mae: 0.0038 - val_loss: 2.6632e-04 - val_mae: 0.0046\n",
      "Epoch 234/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.5598e-04 - mae: 0.0036 - val_loss: 2.6282e-04 - val_mae: 0.0043\n",
      "Epoch 235/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.5657e-04 - mae: 0.0038 - val_loss: 2.9258e-04 - val_mae: 0.0065\n",
      "Epoch 236/1000\n",
      "628/630 [============================>.] - ETA: 0s - loss: 2.5454e-04 - mae: 0.0037Restoring model weights from the end of the best epoch: 231.\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5456e-04 - mae: 0.0037 - val_loss: 2.7927e-04 - val_mae: 0.0060\n",
      "Epoch 236: early stopping\n",
      "Training für Fold 5...\n",
      "Epoch 1/1000\n",
      "630/630 [==============================] - 4s 4ms/step - loss: 0.0288 - mae: 0.0560 - val_loss: 0.0209 - val_mae: 0.0267\n",
      "Epoch 2/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0196 - mae: 0.0219 - val_loss: 0.0187 - val_mae: 0.0178\n",
      "Epoch 3/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0180 - mae: 0.0157 - val_loss: 0.0174 - val_mae: 0.0133\n",
      "Epoch 4/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0169 - mae: 0.0131 - val_loss: 0.0165 - val_mae: 0.0121\n",
      "Epoch 5/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0161 - mae: 0.0114 - val_loss: 0.0157 - val_mae: 0.0112\n",
      "Epoch 6/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0153 - mae: 0.0107 - val_loss: 0.0149 - val_mae: 0.0093\n",
      "Epoch 7/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0145 - mae: 0.0100 - val_loss: 0.0142 - val_mae: 0.0092\n",
      "Epoch 8/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0138 - mae: 0.0092 - val_loss: 0.0135 - val_mae: 0.0089\n",
      "Epoch 9/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0132 - mae: 0.0092 - val_loss: 0.0128 - val_mae: 0.0087\n",
      "Epoch 10/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0125 - mae: 0.0088 - val_loss: 0.0122 - val_mae: 0.0082\n",
      "Epoch 11/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0119 - mae: 0.0087 - val_loss: 0.0116 - val_mae: 0.0074\n",
      "Epoch 12/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0114 - mae: 0.0084 - val_loss: 0.0111 - val_mae: 0.0077\n",
      "Epoch 13/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0109 - mae: 0.0083 - val_loss: 0.0106 - val_mae: 0.0093\n",
      "Epoch 14/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0104 - mae: 0.0078 - val_loss: 0.0101 - val_mae: 0.0081\n",
      "Epoch 15/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0099 - mae: 0.0080 - val_loss: 0.0097 - val_mae: 0.0081\n",
      "Epoch 16/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0095 - mae: 0.0076 - val_loss: 0.0093 - val_mae: 0.0070\n",
      "Epoch 17/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0091 - mae: 0.0076 - val_loss: 0.0089 - val_mae: 0.0091\n",
      "Epoch 18/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0087 - mae: 0.0074 - val_loss: 0.0086 - val_mae: 0.0074\n",
      "Epoch 19/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0084 - mae: 0.0075 - val_loss: 0.0083 - val_mae: 0.0090\n",
      "Epoch 20/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0081 - mae: 0.0073 - val_loss: 0.0080 - val_mae: 0.0068\n",
      "Epoch 21/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0078 - mae: 0.0072 - val_loss: 0.0077 - val_mae: 0.0067\n",
      "Epoch 22/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0076 - mae: 0.0070 - val_loss: 0.0075 - val_mae: 0.0074\n",
      "Epoch 23/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0073 - mae: 0.0070 - val_loss: 0.0072 - val_mae: 0.0077\n",
      "Epoch 24/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0071 - mae: 0.0068 - val_loss: 0.0070 - val_mae: 0.0065\n",
      "Epoch 25/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0069 - mae: 0.0068 - val_loss: 0.0068 - val_mae: 0.0056\n",
      "Epoch 26/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0067 - mae: 0.0067 - val_loss: 0.0066 - val_mae: 0.0058\n",
      "Epoch 27/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0065 - mae: 0.0065 - val_loss: 0.0064 - val_mae: 0.0073\n",
      "Epoch 28/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0063 - mae: 0.0064 - val_loss: 0.0062 - val_mae: 0.0059\n",
      "Epoch 29/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0061 - mae: 0.0063 - val_loss: 0.0060 - val_mae: 0.0055\n",
      "Epoch 30/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0060 - mae: 0.0064 - val_loss: 0.0059 - val_mae: 0.0054\n",
      "Epoch 31/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0058 - mae: 0.0061 - val_loss: 0.0057 - val_mae: 0.0065\n",
      "Epoch 32/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0056 - mae: 0.0061 - val_loss: 0.0056 - val_mae: 0.0081\n",
      "Epoch 33/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0055 - mae: 0.0061 - val_loss: 0.0054 - val_mae: 0.0067\n",
      "Epoch 34/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0053 - mae: 0.0060 - val_loss: 0.0052 - val_mae: 0.0059\n",
      "Epoch 35/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0052 - mae: 0.0061 - val_loss: 0.0051 - val_mae: 0.0053\n",
      "Epoch 36/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0050 - mae: 0.0058 - val_loss: 0.0050 - val_mae: 0.0058\n",
      "Epoch 37/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0049 - mae: 0.0058 - val_loss: 0.0048 - val_mae: 0.0057\n",
      "Epoch 38/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0047 - mae: 0.0057 - val_loss: 0.0047 - val_mae: 0.0047\n",
      "Epoch 39/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0046 - mae: 0.0056 - val_loss: 0.0046 - val_mae: 0.0063\n",
      "Epoch 40/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0045 - mae: 0.0055 - val_loss: 0.0044 - val_mae: 0.0057\n",
      "Epoch 41/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0044 - mae: 0.0055 - val_loss: 0.0043 - val_mae: 0.0070\n",
      "Epoch 42/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0042 - mae: 0.0058 - val_loss: 0.0042 - val_mae: 0.0050\n",
      "Epoch 43/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0041 - mae: 0.0053 - val_loss: 0.0040 - val_mae: 0.0045\n",
      "Epoch 44/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0040 - mae: 0.0055 - val_loss: 0.0040 - val_mae: 0.0055\n",
      "Epoch 45/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0039 - mae: 0.0053 - val_loss: 0.0038 - val_mae: 0.0050\n",
      "Epoch 46/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0038 - mae: 0.0053 - val_loss: 0.0038 - val_mae: 0.0063\n",
      "Epoch 47/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0037 - mae: 0.0053 - val_loss: 0.0036 - val_mae: 0.0065\n",
      "Epoch 48/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0036 - mae: 0.0053 - val_loss: 0.0035 - val_mae: 0.0047\n",
      "Epoch 49/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0035 - mae: 0.0053 - val_loss: 0.0034 - val_mae: 0.0053\n",
      "Epoch 50/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0034 - mae: 0.0053 - val_loss: 0.0033 - val_mae: 0.0046\n",
      "Epoch 51/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0033 - mae: 0.0056 - val_loss: 0.0033 - val_mae: 0.0053\n",
      "Epoch 52/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0032 - mae: 0.0051 - val_loss: 0.0032 - val_mae: 0.0056\n",
      "Epoch 53/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0031 - mae: 0.0052 - val_loss: 0.0031 - val_mae: 0.0043\n",
      "Epoch 54/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0030 - mae: 0.0051 - val_loss: 0.0030 - val_mae: 0.0044\n",
      "Epoch 55/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0030 - mae: 0.0052 - val_loss: 0.0029 - val_mae: 0.0043\n",
      "Epoch 56/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0029 - mae: 0.0051 - val_loss: 0.0028 - val_mae: 0.0047\n",
      "Epoch 57/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0028 - mae: 0.0052 - val_loss: 0.0028 - val_mae: 0.0054\n",
      "Epoch 58/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0027 - mae: 0.0049 - val_loss: 0.0027 - val_mae: 0.0046\n",
      "Epoch 59/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0027 - mae: 0.0049 - val_loss: 0.0026 - val_mae: 0.0047\n",
      "Epoch 60/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0026 - mae: 0.0050 - val_loss: 0.0026 - val_mae: 0.0045\n",
      "Epoch 61/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0025 - mae: 0.0050 - val_loss: 0.0025 - val_mae: 0.0052\n",
      "Epoch 62/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0025 - mae: 0.0050 - val_loss: 0.0024 - val_mae: 0.0045\n",
      "Epoch 63/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0024 - mae: 0.0048 - val_loss: 0.0024 - val_mae: 0.0044\n",
      "Epoch 64/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0023 - mae: 0.0049 - val_loss: 0.0023 - val_mae: 0.0046\n",
      "Epoch 65/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0023 - mae: 0.0049 - val_loss: 0.0022 - val_mae: 0.0050\n",
      "Epoch 66/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0022 - mae: 0.0049 - val_loss: 0.0022 - val_mae: 0.0049\n",
      "Epoch 67/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0022 - mae: 0.0050 - val_loss: 0.0021 - val_mae: 0.0045\n",
      "Epoch 68/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0021 - mae: 0.0046 - val_loss: 0.0021 - val_mae: 0.0043\n",
      "Epoch 69/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0047 - val_loss: 0.0020 - val_mae: 0.0044\n",
      "Epoch 70/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0020 - mae: 0.0050 - val_loss: 0.0020 - val_mae: 0.0056\n",
      "Epoch 71/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0019 - mae: 0.0048 - val_loss: 0.0019 - val_mae: 0.0063\n",
      "Epoch 72/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0019 - mae: 0.0047 - val_loss: 0.0019 - val_mae: 0.0040\n",
      "Epoch 73/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0048\n",
      "Epoch 74/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0018 - mae: 0.0047 - val_loss: 0.0018 - val_mae: 0.0045\n",
      "Epoch 75/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0017 - mae: 0.0048 - val_loss: 0.0017 - val_mae: 0.0041\n",
      "Epoch 76/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0017 - mae: 0.0046 - val_loss: 0.0017 - val_mae: 0.0043\n",
      "Epoch 77/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0017 - mae: 0.0047 - val_loss: 0.0016 - val_mae: 0.0042\n",
      "Epoch 78/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0016 - mae: 0.0046 - val_loss: 0.0016 - val_mae: 0.0052\n",
      "Epoch 79/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0016 - mae: 0.0045 - val_loss: 0.0016 - val_mae: 0.0040\n",
      "Epoch 80/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0015 - mae: 0.0046 - val_loss: 0.0015 - val_mae: 0.0049\n",
      "Epoch 81/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0040\n",
      "Epoch 82/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0045 - val_loss: 0.0015 - val_mae: 0.0058\n",
      "Epoch 83/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0014 - val_mae: 0.0043\n",
      "Epoch 84/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0014 - mae: 0.0043 - val_loss: 0.0014 - val_mae: 0.0046\n",
      "Epoch 85/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0041\n",
      "Epoch 86/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0013 - mae: 0.0045 - val_loss: 0.0013 - val_mae: 0.0048\n",
      "Epoch 87/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0044 - val_loss: 0.0013 - val_mae: 0.0043\n",
      "Epoch 88/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0043 - val_loss: 0.0013 - val_mae: 0.0040\n",
      "Epoch 89/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0043 - val_loss: 0.0012 - val_mae: 0.0040\n",
      "Epoch 90/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0044\n",
      "Epoch 91/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 0.0012 - val_mae: 0.0037\n",
      "Epoch 92/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0012 - mae: 0.0042 - val_loss: 0.0012 - val_mae: 0.0044\n",
      "Epoch 93/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0041\n",
      "Epoch 94/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0043 - val_loss: 0.0011 - val_mae: 0.0047\n",
      "Epoch 95/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 0.0011 - mae: 0.0042 - val_loss: 0.0011 - val_mae: 0.0039\n",
      "Epoch 96/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0011 - mae: 0.0044 - val_loss: 0.0011 - val_mae: 0.0085\n",
      "Epoch 97/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0010 - mae: 0.0043 - val_loss: 0.0010 - val_mae: 0.0037\n",
      "Epoch 98/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0010 - mae: 0.0042 - val_loss: 0.0010 - val_mae: 0.0042\n",
      "Epoch 99/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 0.0010 - mae: 0.0043 - val_loss: 9.8791e-04 - val_mae: 0.0040\n",
      "Epoch 100/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.8003e-04 - mae: 0.0042 - val_loss: 9.7491e-04 - val_mae: 0.0045\n",
      "Epoch 101/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.6178e-04 - mae: 0.0043 - val_loss: 9.6656e-04 - val_mae: 0.0055\n",
      "Epoch 102/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.4267e-04 - mae: 0.0042 - val_loss: 9.5605e-04 - val_mae: 0.0061\n",
      "Epoch 103/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 9.2392e-04 - mae: 0.0041 - val_loss: 9.2591e-04 - val_mae: 0.0046\n",
      "Epoch 104/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 9.0754e-04 - mae: 0.0042 - val_loss: 9.2394e-04 - val_mae: 0.0059\n",
      "Epoch 105/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.8997e-04 - mae: 0.0041 - val_loss: 8.8443e-04 - val_mae: 0.0042\n",
      "Epoch 106/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 8.7535e-04 - mae: 0.0043 - val_loss: 8.6043e-04 - val_mae: 0.0035\n",
      "Epoch 107/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.5797e-04 - mae: 0.0041 - val_loss: 8.5990e-04 - val_mae: 0.0048\n",
      "Epoch 108/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.4281e-04 - mae: 0.0041 - val_loss: 8.4260e-04 - val_mae: 0.0046\n",
      "Epoch 109/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.2683e-04 - mae: 0.0040 - val_loss: 8.1552e-04 - val_mae: 0.0036\n",
      "Epoch 110/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 8.1511e-04 - mae: 0.0043 - val_loss: 8.0849e-04 - val_mae: 0.0044\n",
      "Epoch 111/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.9854e-04 - mae: 0.0041 - val_loss: 7.9151e-04 - val_mae: 0.0038\n",
      "Epoch 112/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.8676e-04 - mae: 0.0042 - val_loss: 7.7396e-04 - val_mae: 0.0035\n",
      "Epoch 113/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.7169e-04 - mae: 0.0040 - val_loss: 7.6375e-04 - val_mae: 0.0037\n",
      "Epoch 114/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.6184e-04 - mae: 0.0042 - val_loss: 7.7319e-04 - val_mae: 0.0057\n",
      "Epoch 115/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.4789e-04 - mae: 0.0041 - val_loss: 7.3751e-04 - val_mae: 0.0036\n",
      "Epoch 116/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.3465e-04 - mae: 0.0040 - val_loss: 7.2685e-04 - val_mae: 0.0038\n",
      "Epoch 117/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.2357e-04 - mae: 0.0041 - val_loss: 7.1275e-04 - val_mae: 0.0035\n",
      "Epoch 118/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.1331e-04 - mae: 0.0041 - val_loss: 7.0530e-04 - val_mae: 0.0039\n",
      "Epoch 119/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 7.0012e-04 - mae: 0.0040 - val_loss: 6.9216e-04 - val_mae: 0.0037\n",
      "Epoch 120/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.9181e-04 - mae: 0.0042 - val_loss: 6.9771e-04 - val_mae: 0.0051\n",
      "Epoch 121/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.7906e-04 - mae: 0.0040 - val_loss: 6.8615e-04 - val_mae: 0.0050\n",
      "Epoch 122/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.6955e-04 - mae: 0.0040 - val_loss: 6.7096e-04 - val_mae: 0.0046\n",
      "Epoch 123/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.5856e-04 - mae: 0.0040 - val_loss: 6.6504e-04 - val_mae: 0.0045\n",
      "Epoch 124/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.5037e-04 - mae: 0.0041 - val_loss: 6.4833e-04 - val_mae: 0.0044\n",
      "Epoch 125/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.4109e-04 - mae: 0.0041 - val_loss: 6.4744e-04 - val_mae: 0.0050\n",
      "Epoch 126/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 6.3202e-04 - mae: 0.0041 - val_loss: 6.3569e-04 - val_mae: 0.0047\n",
      "Epoch 127/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.2307e-04 - mae: 0.0040 - val_loss: 6.1751e-04 - val_mae: 0.0039\n",
      "Epoch 128/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.1423e-04 - mae: 0.0040 - val_loss: 6.0637e-04 - val_mae: 0.0035\n",
      "Epoch 129/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 6.0592e-04 - mae: 0.0040 - val_loss: 5.9834e-04 - val_mae: 0.0036\n",
      "Epoch 130/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.9728e-04 - mae: 0.0039 - val_loss: 5.9335e-04 - val_mae: 0.0039\n",
      "Epoch 131/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.9020e-04 - mae: 0.0040 - val_loss: 5.8241e-04 - val_mae: 0.0035\n",
      "Epoch 132/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.8175e-04 - mae: 0.0039 - val_loss: 5.7686e-04 - val_mae: 0.0038\n",
      "Epoch 133/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.7546e-04 - mae: 0.0040 - val_loss: 5.8363e-04 - val_mae: 0.0047\n",
      "Epoch 134/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.6730e-04 - mae: 0.0039 - val_loss: 5.8466e-04 - val_mae: 0.0055\n",
      "Epoch 135/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.5894e-04 - mae: 0.0039 - val_loss: 5.8477e-04 - val_mae: 0.0061\n",
      "Epoch 136/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.5257e-04 - mae: 0.0039 - val_loss: 5.6716e-04 - val_mae: 0.0054\n",
      "Epoch 137/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.4691e-04 - mae: 0.0040 - val_loss: 5.4856e-04 - val_mae: 0.0045\n",
      "Epoch 138/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.4026e-04 - mae: 0.0040 - val_loss: 5.4663e-04 - val_mae: 0.0047\n",
      "Epoch 139/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.3176e-04 - mae: 0.0038 - val_loss: 5.3296e-04 - val_mae: 0.0041\n",
      "Epoch 140/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.2936e-04 - mae: 0.0041 - val_loss: 5.5345e-04 - val_mae: 0.0064\n",
      "Epoch 141/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.1990e-04 - mae: 0.0038 - val_loss: 5.1884e-04 - val_mae: 0.0040\n",
      "Epoch 142/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.1423e-04 - mae: 0.0038 - val_loss: 5.1404e-04 - val_mae: 0.0041\n",
      "Epoch 143/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 5.0845e-04 - mae: 0.0039 - val_loss: 5.0337e-04 - val_mae: 0.0035\n",
      "Epoch 144/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 5.0288e-04 - mae: 0.0039 - val_loss: 5.2593e-04 - val_mae: 0.0058\n",
      "Epoch 145/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.9750e-04 - mae: 0.0039 - val_loss: 4.9165e-04 - val_mae: 0.0036\n",
      "Epoch 146/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.8983e-04 - mae: 0.0037 - val_loss: 5.1156e-04 - val_mae: 0.0058\n",
      "Epoch 147/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.8636e-04 - mae: 0.0039 - val_loss: 4.8205e-04 - val_mae: 0.0037\n",
      "Epoch 148/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.8052e-04 - mae: 0.0039 - val_loss: 5.1512e-04 - val_mae: 0.0064\n",
      "Epoch 149/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.7875e-04 - mae: 0.0041 - val_loss: 4.7959e-04 - val_mae: 0.0044\n",
      "Epoch 150/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.6851e-04 - mae: 0.0037 - val_loss: 4.6886e-04 - val_mae: 0.0037\n",
      "Epoch 151/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.6753e-04 - mae: 0.0040 - val_loss: 4.6415e-04 - val_mae: 0.0037\n",
      "Epoch 152/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.5950e-04 - mae: 0.0037 - val_loss: 4.6089e-04 - val_mae: 0.0040\n",
      "Epoch 153/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.5584e-04 - mae: 0.0038 - val_loss: 4.5928e-04 - val_mae: 0.0042\n",
      "Epoch 154/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.5256e-04 - mae: 0.0039 - val_loss: 4.4448e-04 - val_mae: 0.0032\n",
      "Epoch 155/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.4635e-04 - mae: 0.0038 - val_loss: 4.3961e-04 - val_mae: 0.0033\n",
      "Epoch 156/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.4303e-04 - mae: 0.0039 - val_loss: 4.3943e-04 - val_mae: 0.0037\n",
      "Epoch 157/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.3752e-04 - mae: 0.0038 - val_loss: 4.3356e-04 - val_mae: 0.0034\n",
      "Epoch 158/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.3371e-04 - mae: 0.0038 - val_loss: 4.3566e-04 - val_mae: 0.0041\n",
      "Epoch 159/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.3045e-04 - mae: 0.0039 - val_loss: 4.3447e-04 - val_mae: 0.0042\n",
      "Epoch 160/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.2571e-04 - mae: 0.0038 - val_loss: 4.3381e-04 - val_mae: 0.0048\n",
      "Epoch 161/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.2094e-04 - mae: 0.0038 - val_loss: 4.1885e-04 - val_mae: 0.0037\n",
      "Epoch 162/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 4.1776e-04 - mae: 0.0038 - val_loss: 4.1233e-04 - val_mae: 0.0035\n",
      "Epoch 163/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.1326e-04 - mae: 0.0038 - val_loss: 4.3182e-04 - val_mae: 0.0055\n",
      "Epoch 164/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.1135e-04 - mae: 0.0039 - val_loss: 4.0541e-04 - val_mae: 0.0034\n",
      "Epoch 165/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0589e-04 - mae: 0.0038 - val_loss: 4.2290e-04 - val_mae: 0.0054\n",
      "Epoch 166/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 4.0189e-04 - mae: 0.0038 - val_loss: 3.9784e-04 - val_mae: 0.0034\n",
      "Epoch 167/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.9845e-04 - mae: 0.0038 - val_loss: 4.0025e-04 - val_mae: 0.0039\n",
      "Epoch 168/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.9590e-04 - mae: 0.0039 - val_loss: 3.8971e-04 - val_mae: 0.0033\n",
      "Epoch 169/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.9172e-04 - mae: 0.0038 - val_loss: 3.8572e-04 - val_mae: 0.0032\n",
      "Epoch 170/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8887e-04 - mae: 0.0038 - val_loss: 3.8731e-04 - val_mae: 0.0037\n",
      "Epoch 171/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.8542e-04 - mae: 0.0038 - val_loss: 3.8044e-04 - val_mae: 0.0034\n",
      "Epoch 172/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.8147e-04 - mae: 0.0038 - val_loss: 3.7913e-04 - val_mae: 0.0036\n",
      "Epoch 173/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7800e-04 - mae: 0.0037 - val_loss: 3.7242e-04 - val_mae: 0.0032\n",
      "Epoch 174/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7664e-04 - mae: 0.0039 - val_loss: 3.7997e-04 - val_mae: 0.0043\n",
      "Epoch 175/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.7298e-04 - mae: 0.0038 - val_loss: 3.6966e-04 - val_mae: 0.0036\n",
      "Epoch 176/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6799e-04 - mae: 0.0037 - val_loss: 3.6685e-04 - val_mae: 0.0036\n",
      "Epoch 177/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6724e-04 - mae: 0.0039 - val_loss: 3.6518e-04 - val_mae: 0.0038\n",
      "Epoch 178/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6303e-04 - mae: 0.0038 - val_loss: 3.6204e-04 - val_mae: 0.0037\n",
      "Epoch 179/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.6113e-04 - mae: 0.0038 - val_loss: 3.5592e-04 - val_mae: 0.0034\n",
      "Epoch 180/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.5754e-04 - mae: 0.0038 - val_loss: 3.6164e-04 - val_mae: 0.0043\n",
      "Epoch 181/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.5501e-04 - mae: 0.0038 - val_loss: 3.5003e-04 - val_mae: 0.0034\n",
      "Epoch 182/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5157e-04 - mae: 0.0038 - val_loss: 3.4928e-04 - val_mae: 0.0035\n",
      "Epoch 183/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.5111e-04 - mae: 0.0039 - val_loss: 3.5779e-04 - val_mae: 0.0046\n",
      "Epoch 184/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.4605e-04 - mae: 0.0037 - val_loss: 3.4461e-04 - val_mae: 0.0037\n",
      "Epoch 185/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.4311e-04 - mae: 0.0037 - val_loss: 3.5376e-04 - val_mae: 0.0045\n",
      "Epoch 186/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.4081e-04 - mae: 0.0037 - val_loss: 3.3759e-04 - val_mae: 0.0034\n",
      "Epoch 187/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3942e-04 - mae: 0.0038 - val_loss: 3.3322e-04 - val_mae: 0.0033\n",
      "Epoch 188/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3625e-04 - mae: 0.0038 - val_loss: 3.5079e-04 - val_mae: 0.0050\n",
      "Epoch 189/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.3374e-04 - mae: 0.0038 - val_loss: 3.2873e-04 - val_mae: 0.0033\n",
      "Epoch 190/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.3087e-04 - mae: 0.0037 - val_loss: 3.2632e-04 - val_mae: 0.0033\n",
      "Epoch 191/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2907e-04 - mae: 0.0038 - val_loss: 3.2499e-04 - val_mae: 0.0034\n",
      "Epoch 192/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 3.2659e-04 - mae: 0.0038 - val_loss: 3.2612e-04 - val_mae: 0.0038\n",
      "Epoch 193/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2481e-04 - mae: 0.0038 - val_loss: 3.2859e-04 - val_mae: 0.0042\n",
      "Epoch 194/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.2216e-04 - mae: 0.0038 - val_loss: 3.1806e-04 - val_mae: 0.0034\n",
      "Epoch 195/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1993e-04 - mae: 0.0038 - val_loss: 3.1707e-04 - val_mae: 0.0035\n",
      "Epoch 196/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1667e-04 - mae: 0.0037 - val_loss: 3.1268e-04 - val_mae: 0.0033\n",
      "Epoch 197/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1631e-04 - mae: 0.0038 - val_loss: 3.3784e-04 - val_mae: 0.0057\n",
      "Epoch 198/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1353e-04 - mae: 0.0038 - val_loss: 3.1443e-04 - val_mae: 0.0040\n",
      "Epoch 199/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.1119e-04 - mae: 0.0037 - val_loss: 3.1252e-04 - val_mae: 0.0040\n",
      "Epoch 200/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0850e-04 - mae: 0.0037 - val_loss: 3.1560e-04 - val_mae: 0.0045\n",
      "Epoch 201/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0753e-04 - mae: 0.0038 - val_loss: 3.0475e-04 - val_mae: 0.0036\n",
      "Epoch 202/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0488e-04 - mae: 0.0037 - val_loss: 3.0124e-04 - val_mae: 0.0033\n",
      "Epoch 203/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0364e-04 - mae: 0.0038 - val_loss: 3.0008e-04 - val_mae: 0.0034\n",
      "Epoch 204/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 3.0127e-04 - mae: 0.0037 - val_loss: 3.0685e-04 - val_mae: 0.0041\n",
      "Epoch 205/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9977e-04 - mae: 0.0038 - val_loss: 2.9522e-04 - val_mae: 0.0033\n",
      "Epoch 206/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9789e-04 - mae: 0.0038 - val_loss: 3.2992e-04 - val_mae: 0.0065\n",
      "Epoch 207/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9574e-04 - mae: 0.0037 - val_loss: 3.0789e-04 - val_mae: 0.0047\n",
      "Epoch 208/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.9277e-04 - mae: 0.0036 - val_loss: 2.8755e-04 - val_mae: 0.0031\n",
      "Epoch 209/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.9194e-04 - mae: 0.0037 - val_loss: 2.9440e-04 - val_mae: 0.0037\n",
      "Epoch 210/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.9141e-04 - mae: 0.0038 - val_loss: 3.1084e-04 - val_mae: 0.0057\n",
      "Epoch 211/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8823e-04 - mae: 0.0037 - val_loss: 2.9059e-04 - val_mae: 0.0040\n",
      "Epoch 212/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8685e-04 - mae: 0.0037 - val_loss: 2.9295e-04 - val_mae: 0.0043\n",
      "Epoch 213/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8389e-04 - mae: 0.0036 - val_loss: 2.8197e-04 - val_mae: 0.0034\n",
      "Epoch 214/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8556e-04 - mae: 0.0039 - val_loss: 2.9065e-04 - val_mae: 0.0041\n",
      "Epoch 215/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.8160e-04 - mae: 0.0037 - val_loss: 2.7930e-04 - val_mae: 0.0035\n",
      "Epoch 216/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.8089e-04 - mae: 0.0038 - val_loss: 2.9769e-04 - val_mae: 0.0048\n",
      "Epoch 217/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7959e-04 - mae: 0.0038 - val_loss: 2.8614e-04 - val_mae: 0.0043\n",
      "Epoch 218/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7734e-04 - mae: 0.0037 - val_loss: 2.7847e-04 - val_mae: 0.0038\n",
      "Epoch 219/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7541e-04 - mae: 0.0037 - val_loss: 2.7137e-04 - val_mae: 0.0033\n",
      "Epoch 220/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7449e-04 - mae: 0.0037 - val_loss: 2.7167e-04 - val_mae: 0.0034\n",
      "Epoch 221/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7287e-04 - mae: 0.0037 - val_loss: 2.7215e-04 - val_mae: 0.0036\n",
      "Epoch 222/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.7307e-04 - mae: 0.0039 - val_loss: 2.7952e-04 - val_mae: 0.0043\n",
      "Epoch 223/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6923e-04 - mae: 0.0036 - val_loss: 2.6440e-04 - val_mae: 0.0031\n",
      "Epoch 224/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6869e-04 - mae: 0.0037 - val_loss: 2.6972e-04 - val_mae: 0.0038\n",
      "Epoch 225/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6708e-04 - mae: 0.0037 - val_loss: 2.6836e-04 - val_mae: 0.0037\n",
      "Epoch 226/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6630e-04 - mae: 0.0038 - val_loss: 2.8078e-04 - val_mae: 0.0049\n",
      "Epoch 227/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6549e-04 - mae: 0.0038 - val_loss: 2.6247e-04 - val_mae: 0.0035\n",
      "Epoch 228/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6341e-04 - mae: 0.0037 - val_loss: 2.7392e-04 - val_mae: 0.0046\n",
      "Epoch 229/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.6105e-04 - mae: 0.0036 - val_loss: 2.6619e-04 - val_mae: 0.0042\n",
      "Epoch 230/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.6283e-04 - mae: 0.0039 - val_loss: 2.8976e-04 - val_mae: 0.0063\n",
      "Epoch 231/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.5853e-04 - mae: 0.0036 - val_loss: 2.6604e-04 - val_mae: 0.0043\n",
      "Epoch 232/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5685e-04 - mae: 0.0036 - val_loss: 2.5516e-04 - val_mae: 0.0033\n",
      "Epoch 233/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5706e-04 - mae: 0.0037 - val_loss: 2.6273e-04 - val_mae: 0.0040\n",
      "Epoch 234/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5581e-04 - mae: 0.0037 - val_loss: 2.5390e-04 - val_mae: 0.0035\n",
      "Epoch 235/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5509e-04 - mae: 0.0038 - val_loss: 2.6522e-04 - val_mae: 0.0049\n",
      "Epoch 236/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5328e-04 - mae: 0.0037 - val_loss: 2.5317e-04 - val_mae: 0.0037\n",
      "Epoch 237/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.5108e-04 - mae: 0.0036 - val_loss: 2.4763e-04 - val_mae: 0.0031\n",
      "Epoch 238/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4983e-04 - mae: 0.0036 - val_loss: 2.4814e-04 - val_mae: 0.0034\n",
      "Epoch 239/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.4926e-04 - mae: 0.0036 - val_loss: 2.4547e-04 - val_mae: 0.0032\n",
      "Epoch 240/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.4807e-04 - mae: 0.0037 - val_loss: 2.5136e-04 - val_mae: 0.0038\n",
      "Epoch 241/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4815e-04 - mae: 0.0037 - val_loss: 2.4244e-04 - val_mae: 0.0031\n",
      "Epoch 242/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.4591e-04 - mae: 0.0036 - val_loss: 2.7041e-04 - val_mae: 0.0057\n",
      "Epoch 243/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.4541e-04 - mae: 0.0037 - val_loss: 2.4766e-04 - val_mae: 0.0039\n",
      "Epoch 244/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4487e-04 - mae: 0.0037 - val_loss: 2.4417e-04 - val_mae: 0.0035\n",
      "Epoch 245/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.4291e-04 - mae: 0.0037 - val_loss: 2.4379e-04 - val_mae: 0.0036\n",
      "Epoch 246/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4137e-04 - mae: 0.0036 - val_loss: 2.4129e-04 - val_mae: 0.0036\n",
      "Epoch 247/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.4144e-04 - mae: 0.0037 - val_loss: 2.3862e-04 - val_mae: 0.0035\n",
      "Epoch 248/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3937e-04 - mae: 0.0036 - val_loss: 2.3974e-04 - val_mae: 0.0035\n",
      "Epoch 249/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3885e-04 - mae: 0.0036 - val_loss: 2.4580e-04 - val_mae: 0.0044\n",
      "Epoch 250/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3749e-04 - mae: 0.0036 - val_loss: 2.3923e-04 - val_mae: 0.0038\n",
      "Epoch 251/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.3850e-04 - mae: 0.0038 - val_loss: 2.3222e-04 - val_mae: 0.0031\n",
      "Epoch 252/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3600e-04 - mae: 0.0036 - val_loss: 2.3809e-04 - val_mae: 0.0038\n",
      "Epoch 253/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3534e-04 - mae: 0.0037 - val_loss: 2.3388e-04 - val_mae: 0.0035\n",
      "Epoch 254/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3357e-04 - mae: 0.0036 - val_loss: 2.3844e-04 - val_mae: 0.0040\n",
      "Epoch 255/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.3412e-04 - mae: 0.0037 - val_loss: 2.2795e-04 - val_mae: 0.0030\n",
      "Epoch 256/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3419e-04 - mae: 0.0037 - val_loss: 2.3726e-04 - val_mae: 0.0040\n",
      "Epoch 257/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.3050e-04 - mae: 0.0035 - val_loss: 2.3172e-04 - val_mae: 0.0036\n",
      "Epoch 258/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.3105e-04 - mae: 0.0036 - val_loss: 2.3164e-04 - val_mae: 0.0036\n",
      "Epoch 259/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.3117e-04 - mae: 0.0037 - val_loss: 2.2685e-04 - val_mae: 0.0032\n",
      "Epoch 260/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2921e-04 - mae: 0.0036 - val_loss: 2.2525e-04 - val_mae: 0.0031\n",
      "Epoch 261/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2875e-04 - mae: 0.0037 - val_loss: 2.2748e-04 - val_mae: 0.0033\n",
      "Epoch 262/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2835e-04 - mae: 0.0037 - val_loss: 2.3068e-04 - val_mae: 0.0038\n",
      "Epoch 263/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.2634e-04 - mae: 0.0036 - val_loss: 2.2514e-04 - val_mae: 0.0034\n",
      "Epoch 264/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2497e-04 - mae: 0.0035 - val_loss: 2.2106e-04 - val_mae: 0.0030\n",
      "Epoch 265/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2781e-04 - mae: 0.0038 - val_loss: 2.5392e-04 - val_mae: 0.0058\n",
      "Epoch 266/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.2353e-04 - mae: 0.0035 - val_loss: 2.2371e-04 - val_mae: 0.0034\n",
      "Epoch 267/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2378e-04 - mae: 0.0036 - val_loss: 2.3131e-04 - val_mae: 0.0043\n",
      "Epoch 268/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2376e-04 - mae: 0.0037 - val_loss: 2.2100e-04 - val_mae: 0.0033\n",
      "Epoch 269/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2197e-04 - mae: 0.0036 - val_loss: 2.1817e-04 - val_mae: 0.0031\n",
      "Epoch 270/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2123e-04 - mae: 0.0036 - val_loss: 2.2864e-04 - val_mae: 0.0041\n",
      "Epoch 271/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.2005e-04 - mae: 0.0035 - val_loss: 2.2118e-04 - val_mae: 0.0035\n",
      "Epoch 272/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.2083e-04 - mae: 0.0037 - val_loss: 2.1732e-04 - val_mae: 0.0032\n",
      "Epoch 273/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1942e-04 - mae: 0.0036 - val_loss: 2.1516e-04 - val_mae: 0.0031\n",
      "Epoch 274/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1940e-04 - mae: 0.0037 - val_loss: 2.1588e-04 - val_mae: 0.0032\n",
      "Epoch 275/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1928e-04 - mae: 0.0037 - val_loss: 2.1506e-04 - val_mae: 0.0032\n",
      "Epoch 276/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1634e-04 - mae: 0.0035 - val_loss: 2.2749e-04 - val_mae: 0.0044\n",
      "Epoch 277/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1801e-04 - mae: 0.0037 - val_loss: 2.1516e-04 - val_mae: 0.0033\n",
      "Epoch 278/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1657e-04 - mae: 0.0037 - val_loss: 2.1620e-04 - val_mae: 0.0035\n",
      "Epoch 279/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1557e-04 - mae: 0.0036 - val_loss: 2.1136e-04 - val_mae: 0.0031\n",
      "Epoch 280/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1486e-04 - mae: 0.0036 - val_loss: 2.1320e-04 - val_mae: 0.0033\n",
      "Epoch 281/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1277e-04 - mae: 0.0035 - val_loss: 2.2919e-04 - val_mae: 0.0050\n",
      "Epoch 282/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1271e-04 - mae: 0.0035 - val_loss: 2.2216e-04 - val_mae: 0.0043\n",
      "Epoch 283/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1352e-04 - mae: 0.0037 - val_loss: 2.3879e-04 - val_mae: 0.0059\n",
      "Epoch 284/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1301e-04 - mae: 0.0037 - val_loss: 2.0715e-04 - val_mae: 0.0029\n",
      "Epoch 285/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1207e-04 - mae: 0.0036 - val_loss: 2.1010e-04 - val_mae: 0.0034\n",
      "Epoch 286/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1005e-04 - mae: 0.0035 - val_loss: 2.0667e-04 - val_mae: 0.0031\n",
      "Epoch 287/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.1024e-04 - mae: 0.0036 - val_loss: 2.1201e-04 - val_mae: 0.0037\n",
      "Epoch 288/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0993e-04 - mae: 0.0036 - val_loss: 2.1829e-04 - val_mae: 0.0043\n",
      "Epoch 289/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.0853e-04 - mae: 0.0035 - val_loss: 2.0831e-04 - val_mae: 0.0035\n",
      "Epoch 290/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0830e-04 - mae: 0.0036 - val_loss: 2.0395e-04 - val_mae: 0.0030\n",
      "Epoch 291/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0880e-04 - mae: 0.0037 - val_loss: 2.0575e-04 - val_mae: 0.0032\n",
      "Epoch 292/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0739e-04 - mae: 0.0036 - val_loss: 2.1328e-04 - val_mae: 0.0039\n",
      "Epoch 293/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0654e-04 - mae: 0.0036 - val_loss: 2.0481e-04 - val_mae: 0.0033\n",
      "Epoch 294/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0528e-04 - mae: 0.0035 - val_loss: 2.0207e-04 - val_mae: 0.0030\n",
      "Epoch 295/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0423e-04 - mae: 0.0035 - val_loss: 2.1406e-04 - val_mae: 0.0045\n",
      "Epoch 296/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0677e-04 - mae: 0.0037 - val_loss: 2.0333e-04 - val_mae: 0.0033\n",
      "Epoch 297/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0408e-04 - mae: 0.0035 - val_loss: 2.0260e-04 - val_mae: 0.0032\n",
      "Epoch 298/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0379e-04 - mae: 0.0035 - val_loss: 2.0082e-04 - val_mae: 0.0032\n",
      "Epoch 299/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0421e-04 - mae: 0.0036 - val_loss: 2.0219e-04 - val_mae: 0.0034\n",
      "Epoch 300/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0155e-04 - mae: 0.0034 - val_loss: 2.0724e-04 - val_mae: 0.0040\n",
      "Epoch 301/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0135e-04 - mae: 0.0035 - val_loss: 2.0022e-04 - val_mae: 0.0033\n",
      "Epoch 302/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0380e-04 - mae: 0.0038 - val_loss: 1.9714e-04 - val_mae: 0.0030\n",
      "Epoch 303/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 2.0169e-04 - mae: 0.0036 - val_loss: 1.9840e-04 - val_mae: 0.0032\n",
      "Epoch 304/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.9908e-04 - mae: 0.0034 - val_loss: 1.9749e-04 - val_mae: 0.0031\n",
      "Epoch 305/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 2.0172e-04 - mae: 0.0037 - val_loss: 1.9562e-04 - val_mae: 0.0030\n",
      "Epoch 306/1000\n",
      "630/630 [==============================] - 2s 3ms/step - loss: 1.9835e-04 - mae: 0.0034 - val_loss: 1.9474e-04 - val_mae: 0.0029\n",
      "Epoch 307/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.9701e-04 - mae: 0.0034 - val_loss: 1.9726e-04 - val_mae: 0.0032\n",
      "Epoch 308/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.9897e-04 - mae: 0.0036 - val_loss: 1.9624e-04 - val_mae: 0.0032\n",
      "Epoch 309/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.9707e-04 - mae: 0.0035 - val_loss: 1.9463e-04 - val_mae: 0.0031\n",
      "Epoch 310/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.9759e-04 - mae: 0.0036 - val_loss: 1.9546e-04 - val_mae: 0.0032\n",
      "Epoch 311/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.9729e-04 - mae: 0.0036 - val_loss: 1.9624e-04 - val_mae: 0.0034\n",
      "Epoch 312/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.9716e-04 - mae: 0.0036 - val_loss: 1.9854e-04 - val_mae: 0.0037\n",
      "Epoch 313/1000\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.9607e-04 - mae: 0.0035 - val_loss: 1.9778e-04 - val_mae: 0.0036\n",
      "Epoch 314/1000\n",
      "618/630 [============================>.] - ETA: 0s - loss: 1.9595e-04 - mae: 0.0036Restoring model weights from the end of the best epoch: 309.\n",
      "630/630 [==============================] - 2s 4ms/step - loss: 1.9585e-04 - mae: 0.0035 - val_loss: 1.9715e-04 - val_mae: 0.0037\n",
      "Epoch 314: early stopping\n",
      "Durchschnittlicher Validation Loss: 0.00022871863038744777\n",
      "Durchschnittlicher Validation MAE: 0.0029764794278889896\n"
     ]
    }
   ],
   "source": [
    "#Kreuzvalidierung auf den Trainingsdaten\n",
    "# Initialisiere Listen, um Ergebnisse zu speichern\n",
    "val_loss_results = []\n",
    "val_mae_results = []\n",
    "\n",
    "# Funktion, um das Modell zu erstellen\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "            Dense(280, activation='relu', input_shape=(5,), kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(232, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(152, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(40, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(88, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(8, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(280, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.00001)),\n",
    "            \n",
    "            Dense(1 , activation = 'linear')\n",
    "\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# K-Fold Cross-Validation Konfiguration\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Leistungsüberwachung\n",
    "fold_no = 1\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train_scaled[train_index], y_train_scaled[val_index]\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "    print(f'Training für Fold {fold_no}...')\n",
    "    history = model.fit(X_train_fold, y_train_fold, batch_size=200, epochs=1000, validation_data=(X_val_fold, y_val_fold), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    # Speichere die Ergebnisse des aktuellen Folds\n",
    "    val_loss_results.append(min(history.history['val_loss']))\n",
    "    val_mae_results.append(min(history.history['val_mae']))\n",
    "\n",
    "    fold_no += 1\n",
    "\n",
    "# Berechne den Durchschnitt über alle Folds\n",
    "average_val_loss = np.mean(val_loss_results)\n",
    "average_val_mae = np.mean(val_mae_results)\n",
    "\n",
    "# Umwandeln der Listen in Pandas DataFrames\n",
    "val_loss_df = pd.DataFrame(val_loss_results, columns=['Validation Loss'])\n",
    "val_mae_df = pd.DataFrame(val_mae_results, columns=['Validation MAE'])\n",
    "\n",
    "# Speichern der DataFrames in CSV-Dateien\n",
    "val_loss_df.to_csv('val_loss_results_D4_t_21_I_F_1.csv', index=False)\n",
    "val_mae_df.to_csv('val_mae_results_D4_t_21_I_F_1.csv', index=False)\n",
    "\n",
    "# Gib die durchschnittlichen Ergebnisse aus\n",
    "print(f'Durchschnittlicher Validation Loss: {average_val_loss}')\n",
    "print(f'Durchschnittlicher Validation MAE: {average_val_mae}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T23:18:01.723162300Z",
     "start_time": "2024-04-02T22:27:02.935869400Z"
    }
   },
   "id": "929336b1a7ac475d"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703/703 - 1s - loss: 3.9304e-04 - mae: 0.0113 - 781ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.0003930425154976547, 0.01132908370345831]"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Modellevaluierung\n",
    "results = model.evaluate(X_test_scaled, y_test_scaled, verbose=2)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:50:54.797371200Z",
     "start_time": "2024-04-03T08:50:53.972652700Z"
    }
   },
   "id": "4b02697cbcecd185"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bsp. Predicted: [465.14783] Actual: [452.59] \n",
      "Durchschnittliche Abweichung (MAE): [25.12698166]\n",
      "2.1801640749309192\n"
     ]
    }
   ],
   "source": [
    "#Berechnung des MAE und MAPE\n",
    "scaled_predicted_values = model.predict(X_test_scaled, verbose = 0)\n",
    "\n",
    "# Rücktransformation der skalierten Werte \n",
    "original_predicted_values = scaler_target.inverse_transform(scaled_predicted_values)\n",
    "original_actual_values = scaler_target.inverse_transform(y_test_scaled)  # y_test sind die skalierten tatsächlichen Werte\n",
    "print(f' Bsp. Predicted: {original_predicted_values[100]} Actual: {original_actual_values[100]} ')\n",
    "\n",
    "def calculate_mae(list1, list2):\n",
    "    # Stelle sicher, dass beide Listen die gleiche Länge haben\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Listen müssen die gleiche Länge haben\")\n",
    "\n",
    "    # Berechne die absolute Differenz zwischen den Elementen der Listen\n",
    "    differences = [abs(x - y) for x, y in zip(list1, list2)]\n",
    "\n",
    "    # Berechne den Durchschnitt der absoluten Differenzen\n",
    "    mae = sum(differences) / len(differences)\n",
    "\n",
    "    return mae\n",
    "\n",
    "# Beispiel\n",
    "list1 = original_predicted_values\n",
    "list2 = original_actual_values\n",
    "\n",
    "mae = calculate_mae(list1, list2)\n",
    "print(f\"Durchschnittliche Abweichung (MAE): {mae}\")\n",
    "errors = np.abs((original_actual_values - original_predicted_values) / original_actual_values)\n",
    "mape = np.mean(errors) * 100\n",
    "print(mape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:51:12.080213400Z",
     "start_time": "2024-04-03T08:51:11.267837200Z"
    }
   },
   "id": "a402d28abbd82f60"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (22491) does not match length of index (1071)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m df_result \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEcht\u001B[39m\u001B[38;5;124m'\u001B[39m: [val[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m list1], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mVorhergesagt\u001B[39m\u001B[38;5;124m'\u001B[39m: [val[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m list2]})\n\u001B[1;32m----> 2\u001B[0m \u001B[43mdf_result\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mX-Koordinate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m X_test_scaled_2[:, \u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m      3\u001B[0m df_result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mY-Koordinate\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m X_test_scaled_2[:, \u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m      4\u001B[0m df_result[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mZeitpunkt\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m X_test_scaled_2[:, \u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4299\u001B[0m, in \u001B[0;36mDataFrame.__setitem__\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m   4296\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setitem_array([key], value)\n\u001B[0;32m   4297\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   4298\u001B[0m     \u001B[38;5;66;03m# set column\u001B[39;00m\n\u001B[1;32m-> 4299\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_set_item\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4512\u001B[0m, in \u001B[0;36mDataFrame._set_item\u001B[1;34m(self, key, value)\u001B[0m\n\u001B[0;32m   4502\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_set_item\u001B[39m(\u001B[38;5;28mself\u001B[39m, key, value) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   4503\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4504\u001B[0m \u001B[38;5;124;03m    Add series to DataFrame in specified column.\u001B[39;00m\n\u001B[0;32m   4505\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4510\u001B[0m \u001B[38;5;124;03m    ensure homogeneity.\u001B[39;00m\n\u001B[0;32m   4511\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4512\u001B[0m     value, refs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sanitize_column\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4514\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   4515\u001B[0m         key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\n\u001B[0;32m   4516\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m value\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   4517\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value\u001B[38;5;241m.\u001B[39mdtype, ExtensionDtype)\n\u001B[0;32m   4518\u001B[0m     ):\n\u001B[0;32m   4519\u001B[0m         \u001B[38;5;66;03m# broadcast across multiple columns if necessary\u001B[39;00m\n\u001B[0;32m   4520\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mis_unique \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns, MultiIndex):\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:5253\u001B[0m, in \u001B[0;36mDataFrame._sanitize_column\u001B[1;34m(self, value)\u001B[0m\n\u001B[0;32m   5250\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _reindex_for_setitem(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex)\n\u001B[0;32m   5252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_list_like(value):\n\u001B[1;32m-> 5253\u001B[0m     \u001B[43mcom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequire_length_match\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5254\u001B[0m arr \u001B[38;5;241m=\u001B[39m sanitize_array(value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, allow_2d\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   5255\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   5256\u001B[0m     \u001B[38;5;28misinstance\u001B[39m(value, Index)\n\u001B[0;32m   5257\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m value\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5260\u001B[0m     \u001B[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001B[39;00m\n\u001B[0;32m   5261\u001B[0m     \u001B[38;5;66;03m# this deprecation\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\pandas\\core\\common.py:573\u001B[0m, in \u001B[0;36mrequire_length_match\u001B[1;34m(data, index)\u001B[0m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    570\u001B[0m \u001B[38;5;124;03mCheck the length of data matches the length of the index.\u001B[39;00m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(index):\n\u001B[1;32m--> 573\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    574\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLength of values \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    575\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    576\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdoes not match length of index \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    577\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(index)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    578\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Length of values (22491) does not match length of index (1071)"
     ]
    }
   ],
   "source": [
    "df_result = pd.DataFrame({'Echt': [val[0] for val in list2], 'Vorhergesagt': [val[0] for val in list1]})\n",
    "df_result['X-Koordinate'] = X_test_scaled[:, 0]\n",
    "df_result['Y-Koordinate'] = X_test_scaled[:, 1]\n",
    "df_result['Zeitpunkt'] = X_test_scaled[:, 2]\n",
    "df_result['Strom'] = X_test_scaled[:, 3]\n",
    "df_result['Kraft'] = X_test_scaled[:, 4]\n",
    "\n",
    "df_result['Differenz'] = abs(df_result['Echt'] - df_result['Vorhergesagt'])\n",
    "df_result['Differenz'].sort_values()\n",
    "sorted_df = df_result.sort_values(by= 'Differenz')\n",
    "Anzahl_Punkte = (sorted_df['Differenz'] < 20).sum()\n",
    "print(\"Anzahl der Werte die kleiner sind:\", Anzahl_Punkte)\n",
    "\n",
    "sorted_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T08:50:39.368852200Z",
     "start_time": "2024-04-03T08:50:39.051180300Z"
    }
   },
   "id": "7ffe8ddf2200f429"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: [0.99989271]\n"
     ]
    }
   ],
   "source": [
    "#Berechnung der Auswertungsgröße R^2\n",
    "\n",
    "def calculate_r_squared(predicted, actual):\n",
    "    # Berechnung des Mittelwerts der tatsächlichen Werte\n",
    "    mean_actual = sum(actual) / len(actual)\n",
    "    \n",
    "    # Berechnung der totalen Summe der Quadrate (SST)\n",
    "    sst = sum((x - mean_actual) ** 2 for x in actual)\n",
    "    \n",
    "    # Berechnung der Summe der Quadrate der Residuen (SSE)\n",
    "    sse = sum((actual[i] - predicted[i]) ** 2 for i in range(len(actual)))\n",
    "    \n",
    "    # Berechnung des R^2-Wertes\n",
    "    r_squared = 1 - (sse / sst)\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "# Berechnung von R^2 mit den bereitgestellten Listen\n",
    "r_squared = calculate_r_squared(list1, list2)\n",
    "\n",
    "print(f\"R^2: {r_squared}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-26T13:59:02.256620900Z",
     "start_time": "2024-03-26T13:59:01.728316700Z"
    }
   },
   "id": "4c350477801f0961"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mse = history.history['loss']\n",
    "val_mse = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(mse) + 1)\n",
    "\n",
    "# MAE Diagramm\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, mse, 'r', label='Training MSE')\n",
    "plt.plot(epochs, val_mse, 'b', label='Validation MSE')\n",
    "plt.title('Training and Validation MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T12:29:07.617414Z",
     "start_time": "2024-03-25T12:29:07.401346700Z"
    }
   },
   "id": "3688dd7102e95baf"
  },
  {
   "cell_type": "markdown",
   "id": "553df6fa",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001B625802E50>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\erikm\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 47\u001B[0m\n\u001B[0;32m     45\u001B[0m grid_search \u001B[38;5;241m=\u001B[39m GridSearchCV(estimator\u001B[38;5;241m=\u001B[39mmodel, param_grid\u001B[38;5;241m=\u001B[39mparam_grid, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;66;03m# Hinweis: Stellen Sie sicher, dass Ihre Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\u001B[39;00m\n\u001B[1;32m---> 47\u001B[0m grid_result \u001B[38;5;241m=\u001B[39m \u001B[43mgrid_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_scaled\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# Beste Parameter und Score ausgeben\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBeste Parameter:\u001B[39m\u001B[38;5;124m\"\u001B[39m, grid_search\u001B[38;5;241m.\u001B[39mbest_params_)\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\base.py:1351\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1344\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1346\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1347\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1348\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1349\u001B[0m     )\n\u001B[0;32m   1350\u001B[0m ):\n\u001B[1;32m-> 1351\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    964\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    965\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    966\u001B[0m     )\n\u001B[0;32m    968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 970\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    974\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1525\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1526\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1527\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    908\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    909\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    910\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    911\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    912\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    913\u001B[0m         )\n\u001B[0;32m    914\u001B[0m     )\n\u001B[1;32m--> 916\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    920\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    922\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    924\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    930\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    931\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    932\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    934\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    935\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    936\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    937\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    938\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    939\u001B[0m     )\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\joblib\\parallel.py:1952\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1946\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   1947\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   1948\u001B[0m \u001B[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001B[39;00m\n\u001B[0;32m   1949\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   1950\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1952\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\joblib\\parallel.py:1595\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1592\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1594\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1595\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1597\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1598\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1599\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1600\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1601\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Diplomarbeit Erik Marr\\Projekt X\\venv\\Lib\\site-packages\\joblib\\parallel.py:1707\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1702\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1703\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1705\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1706\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1707\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m   1708\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1710\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1711\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1712\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def build_model(learning_rate=0.00001, activation='relu', regularization=0.00001, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(280, activation=activation, input_shape=(5,), kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(232, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(152, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(40, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(88, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(8, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(280, activation=activation, kernel_initializer='he_uniform', kernel_regularizer=l2(regularization)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "\n",
    "model = KerasRegressor(model=build_model, verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# Anpassung der Parameter im param_grid\n",
    "param_grid = {\n",
    "    'model__learning_rate': [0.00001],\n",
    "    'model__regularization': [0.00001],\n",
    "    'fit__batch_size': [16, 32, 64, 100, 200],\n",
    "    'fit__epochs': [100],\n",
    "    'model__dropout_rate' : [0.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3, verbose=2)\n",
    "# Hinweis: Sicherstellen, dass Daten (X_train_scaled, y_train_scaled) korrekt definiert sind\n",
    "grid_result = grid_search.fit(X_train_scaled, y_train_scaled)\n",
    "# Beste Parameter und Score ausgeben\n",
    "print(\"Beste Parameter:\", grid_search.best_params_)\n",
    "print(\"Beste Genauigkeit:\", grid_search.best_score_)\n",
    "\n",
    "with open(\"Gridsearch_D4_t_21_I_F_1.txt\", \"w\") as f:\n",
    "    f.write(f\"Beste Parameter: {grid_search.best_params_}\\n\")\n",
    "    f.write(f\"Beste Genauigkeit: {grid_search.best_score_}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T12:15:54.960082300Z",
     "start_time": "2024-03-18T11:56:34.729533600Z"
    }
   },
   "id": "7464a951f44a07ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Funktion zum Erstellen des Modells\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hp.Int('input_units', min_value=8, max_value=328, step=16), input_shape=(5,), activation='relu'))\n",
    "    for i in range(hp.Int('n_layers', 1, 10)):\n",
    "        model.add(Dense(hp.Int(f'units_{i}', min_value=8, max_value=328, step=16), activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Durchführung der Random Search dreimal\n",
    "for run in range(1, 4):\n",
    "    # Anpassen des Verzeichnisses und des Projektnamens für jeden Durchlauf\n",
    "    directory = 'random_search'\n",
    "    project_name = f'random_search_D4_t_21_I_F_{run}'\n",
    "\n",
    "    tuner = RandomSearch(\n",
    "        build_model,\n",
    "        objective='val_loss',\n",
    "        max_trials=100,\n",
    "        executions_per_trial=1,\n",
    "       directory=directory,\n",
    "        project_name=project_name\n",
    "    )\n",
    "\n",
    "    # Durchführung des Random Search\n",
    "    tuner.search(X_train_scaled, y_train_scaled, epochs=50, verbose =0, batch_size=200, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\n",
    "\n",
    "    # Abrufen und Speichern des besten Modells\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    model_path = os.path.join(directory, project_name, 'best_model.h5') \n",
    "    best_model.save(model_path)\n",
    "\n",
    "\n",
    "    # Optional: Abrufen und Ausgeben der besten Hyperparameter\n",
    "    best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    # Konvertieren der Hyperparameter in ein DataFrame\n",
    "    df_hyperparameters = pd.DataFrame([best_hyperparameters.values])\n",
    "    # Speichern des DataFrame als CSV\n",
    "    df_hyperparameters.to_csv(f'random_search_D4_t_21_I_F_{run}.csv', index=False)\n",
    "\n",
    "    print(f\"Beste Hyperparameter für Lauf {run}: {best_hyperparameters.values}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T10:03:46.649218900Z"
    }
   },
   "id": "d0f02feb42b652f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-13T10:03:46.650224Z"
    }
   },
   "id": "3e35d5ebef369658"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
